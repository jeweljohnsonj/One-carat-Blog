[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLearning starts in the embryo for fathead minnow fish\n\n\n\nresearch article\n\n\nscicom\n\n\n\nEmbryonic learning occurs when an organism can learn while in its embryo stage. Learn how researchers showed that fathead minnow embryos (Pimephales promelas) can detect‚Ä¶\n\n\n\nJewel Johnson\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe world‚Äôs largest bacteria is bigger than a housefly\n\n\n\nresearch article\n\n\nscicom\n\n\n\nMeet Thiomargarita magnifica, a recently discovered species of bacteria that is a whopping 1 cm in length, making it the largest bacteria ever till now.\n\n\n\nJewel Johnson\n\n\nJul 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion to keep a movie and a book list tracker\n\n\n\nNotion\n\n\nproductivity\n\n\n\nThis a tutorial on how you can use Notion to maintain reading and watch list tracker\n\n\n\nJewel Johnson\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe story behind one carat blog\n\n\n\nnews\n\n\n\nIn this article, I share my experiences on how this blog was made. Spoiler alter: it involved a lot of R package hitchhiking\n\n\n\nJewel Johnson\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion as your task manager\n\n\n\nPhD\n\n\nNotion\n\n\nproductivity\n\n\n\nIn this article I will showcase to you a template that I made, which can intelligently record and track tasks: you very own personal task manager with the help of Notion\n\n\n\nJewel Johnson\n\n\nJun 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion and Zotero to build a literature tracker\n\n\n\nPhD\n\n\nNotion\n\n\nZotero\n\n\nproductivity\n\n\n\nIn this tutorial we will learn how we can use Notion to make a literature database. With the help of the Notero plugin, our database will be synced with our Zotero‚Ä¶\n\n\n\nJewel Johnson\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion to keep a research diary\n\n\n\nPhD\n\n\nNotion\n\n\nproductivity\n\n\n\nThis a tutorial on how you can use Notion to maintain a research diary if you are a researcher or a PhD student\n\n\n\nJewel Johnson\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Notion story\n\n\n\nPhD\n\n\nNotion\n\n\nZotero\n\n\nproductivity\n\n\n\nNotion is a powerful productivity software that implements database creation in its core function. In this article, I will share my story on what led me to use Notion and‚Ä¶\n\n\n\nJewel Johnson\n\n\nJun 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe World Happiness Report 2022\n\n\n\nmaps\n\n\nleaflet\n\n\n\nPlotting an interactive map using the {leaflet} package in R\n\n\n\nJewel Johnson\n\n\nMay 20, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "one carat blog",
    "section": "",
    "text": "Hey there! My name is Jewel Johnson and I am an aspiring ecologist who is interested in behaviour, vision and in the effect of human activities on animals (especially insects). I did my master‚Äôs in Biology from the Indian Institute of Science Education and Research, Thiruvananthapuram (IISER-TVM). My master‚Äôs thesis was on understanding the Visual ecology of the giant honeybee (Apis dorsata) under the guidance of Prof.¬†Hema Somanathan at IISER-TVM.\n\nCurrently, I am looking for research labs to join for a PhD üòá\n\nI made this blog using the quarto package. My blog posts are generally focused on science communication and R programming. I also believe that teaching refines one‚Äôs ability in learning and comprehension. Therefore I document my learning experience by writing extensive tutorials on the topic I am learning. You can find them in the ‚ÄòTutorials‚Äô section in the navigation bar."
  },
  {
    "objectID": "posts/notion_intro/index.html",
    "href": "posts/notion_intro/index.html",
    "title": "My Notion story",
    "section": "",
    "text": "Notion is a productivity software that can be used to create databases, manage tasks, take notes etc.\nDownload Notion and using the links given below, ‚ÄòDuplicate‚Äô the templates to your workspace\nMade five templates using Notion which features a literature tracker, research diary, task manager, book list tracker, movie list tracker and a finance tracker\n\nThose who want to download all the templates in one go, click here"
  },
  {
    "objectID": "posts/notion_intro/index.html#what-is-notion",
    "href": "posts/notion_intro/index.html#what-is-notion",
    "title": "My Notion story",
    "section": "\n2 What is Notion?",
    "text": "2 What is Notion?\nNotion is a productivity software which can be used to create databases, manage tasks, take notes etc. Its best known for its ability to make large collaborative databases where you can invite and work together with other people. In simple words, Notion can give you a personal Wiki experience by helping you manage your everyday activities.\n\n\n\nMy Notion dashboard\n\n\n\nHere is an introduction video from Notion themselves."
  },
  {
    "objectID": "posts/notion_intro/index.html#my-notion-story",
    "href": "posts/notion_intro/index.html#my-notion-story",
    "title": "My Notion story",
    "section": "\n3 My Notion story",
    "text": "3 My Notion story\nMy journey in discovering Notion came from my quest to find software that can track the research papers that I read. Naturally for this purpose, I began to use a reference manager software, Zotero, which is free to use and is open source. Zotero even comes in with a built-in pdf reader which you can use to highlight, comment and insert notes within a research paper This was very helpful as it enabled me to write my thoughts about the paper and summarize its results, which can be helpful if you are writing a research paper for yourself where you will be citing other papers. But soon I realized a big problem. The problem is that Zotero does not provide a clean user interface to show the summary notes I made on a research paper. What I was looking for was a tabular interface which had columns titled ‚Äòtitle of the paper‚Äô, which is followed by the ‚Äòsummary‚Äô column, where I list down a summary of the paper as bullet points. The rest of the columns can contain other relevant information about the paper.\n\n\nThe table interface I was looking for\n\n\n\n\n\n\n\n\n\nTitle\nAuthors\nDate added\nYear of publication\nSummary\nURL\n\n\nGlyphosate impairs collective thermoregulation in bumblebees\n(Weidenm√ºller et al., 2022)\n22-June-2022\n2022\n1. Glyphosate sugar water reduced life expectancy of worker bees2. Glyphosate affected bees invest less time in incubating the brood\nhttps://www.science.org/doi/10.1126/science.abf7482\n\n\n\nNow Zotero is not expected to have features like this simply because it is a reference manager and not a note-taking application. So I began looking for an application which can provide this exact feature. A simple Google search lead me to people suggesting Microsoft Excel and Google docs as their note-taking apps for summarizing the research papers they read. It sure works but is extremely cumbersome. You have to manually fill in all the columns and that is unnecessary work. Also, it‚Äôs not an elegant way of categorizing the data. I wanted something highly customizable and at the same time automated to an extent. Something similar to Zotero, where you can just simply add the pdf file of a research article and it automatically populates relevant details like Title, Authors, Year etc by grabbing information from the internet. And to my surprise, I came across a tweet on Twitter which had exactly what I was looking for. Thank you @thoughtsofaphd for that tweet.\n\n\n\nLiterature Tracker 2.0: Create a FREE customizable database in @airtable for keeping track of all the papers you download (and maybe readüòÇ). Please share if you find it helpful!‚ö°Ô∏èHere‚Äôs an easy step-by-step guide: https://t.co/jBQ0XeQahgcc @AcademicChatter pic.twitter.com/jLAkIlM2wQ\n\n‚Äî thoughtsofaphd (@thoughtsofaphd) January 7, 2022\n\nIn the tweet, I learned about a new software called Airtable and by following the guide given in the tweet, I made myself a literature tracker for managing the summaries of the papers I read.\n\n\n\nMy literature tracker template in Airtable\n\n\n\nEverything was going smoothly and I was in love with the new user interface Airtable provided. But I was still annoyed by the fact that I have to manually type in the basic information like ‚Äúname of the paper‚Äù, ‚Äúauthors name‚Äù etc. and quite frankly it was becoming quite tedious as the number of papers in my collection grew in number. Humans aren‚Äôt meant to write standard titles. If there was some way to connect Zotero to Airtable that would have made my life easy. So upon searching for a solution I came to know that there is an integration feature developed by Avana Vana. But upon reading about the feature on Github I quickly realized that it was beyond my current ability to comprehend what was happening and how to implement it. So I did what any honest person would have done after that. I gave up on Airtable and went to look for an alternative. And that‚Äôs how I came to meet Notion!\nAs I began learning about Notion I quickly realized the rich potential it offered. Initially, I just wanted a tabular database where I could efficiently write down the summaries of the papers that I read. But upon seeing how others use Notion in their daily life, I couldn‚Äôt help myself to try those features, expand on them and implement them in my day-to-day activities. So after putting in some hours in learning Notion, I was able to create a beautiful template which housed a database storing all the details of the papers I read. In addition to this template, I also made a few other ones focusing on various other day-to-day activities.\n\n\n\nMy literature tracker template in Notion\n\n\n\nThis new template is a significantly improved version of the Airtable template I had before. One of the best improvements was that this one is now automated. So no more wasting time by manually typing words in boring columns. With the help of the plugin Notero by David Vanoni, you can seamlessly integrate your Zotero literature collection with your workspace in Notion. This plugin was a game-changer for me. And thus began my Notion journey starting from this literature tracker template. In addition to this literature tracker template, I also made a few other ones focusing on various other day-to-day activities. If you are interested in how to use the template in Notion and how to get them, I have written separate articles, focusing on each of these templates. You can learn more about this at the end of this article."
  },
  {
    "objectID": "posts/notion_intro/index.html#do-you-need-notion",
    "href": "posts/notion_intro/index.html#do-you-need-notion",
    "title": "My Notion story",
    "section": "\n4 Do you need Notion?",
    "text": "4 Do you need Notion?\nConsider Notion as a LEGO set that you had when you were a child. You can build almost anything with Notion and it is tailored to give you a personal Wiki experience. You are mostly only limited by your creativity and needs. The learning curve for Notion is not a hard one and it‚Äôs pretty much self-intuitive. Whatever I was able to do until now required only a week of self-learning. With that said you can also get a wide variety of templates, mostly for free which can satisfy most of your needs. Most features of Notion are free for personal use but they also have different subscription-based plans for advanced power users. You can learn more about their plans here. Also, keep in mind that you need an active internet connection to use Notion.\nNow let us answer the elephant in the room. Is it secure? The answer is yes and no. Notion uses encryption to encrypt data when it is been sent and received between the user and Notion. But created databases that are stored in Notion‚Äôs cloud service are not encrypted. This means employees of Notion can access your data. Notion has a strict data security policy and claims that they will only access user data with prior permission from the users themselves. The reason why they don‚Äôt use end-to-end encryption is because it would make certain features like ‚Äòfull-text search‚Äô impossible to implement. As for Notion‚Äôs data security commitment, they are part of the Security First Initiative which pledges to put security first by sharing their security information proactively with their customers. So in my opinion I think it‚Äôs safe to use Notion unless you create tables containing your credit card info or account passwords. If you are a researcher, I would also recommend that you do not attach any sensitive research data to Notion. With over 30 million users and with strong ideals on user security and data protection, Notion is as good as any other productive software like Slack, Evernote etc. in terms of data security. So I don‚Äôt think you will face any problem when adopting Notion for your daily activities."
  },
  {
    "objectID": "posts/notion_intro/index.html#installing-notion",
    "href": "posts/notion_intro/index.html#installing-notion",
    "title": "My Notion story",
    "section": "\n5 Installing Notion",
    "text": "5 Installing Notion\nIf you are interested to try out Notion and seeing if it works for you, then please follow these steps.\n\nGo to Notion website and sign up for a free account. You can use your Gmail account for a quick sign-in.\nNotion can be used from your browser itself. But if you want to work from your desktop, please install the desktop app.\nNow you are ready to use Notion.\n\nIt can be pretty overwhelming at first, but as soon as you get familiar, you will learn to do a lot of cool new stuff with it. There are many YouTube videos and web articles that will walk you through the basics of Notion. A good place to start is by watching this YouTube video by Notion itself where they introduce the basics of Notion.\n\n\nNow my goal is to not make you watch multiple tutorial videos and bore yourself to death. Instead, I would like you to have a goal in mind. How I came to learn Notion is that, as I mentioned before, I wanted to have a ‚Äòliterature tracker‚Äô, so having a goal sets me up to see which features in Notion would help me build it. As Notion has a lot of features it can be too much on your plate in trying to learn about all these features. So proceed to go goal by goal. Rome wasn‚Äôt built in a single day! If you are learning any new skill it‚Äôs always helpful to visualize why you are learning this skill and what you want to use it for? A sense of purpose can make the learning process more enjoyable and satisfying in the end.\nNow I am aspiring to be a researcher, so naturally, that made me interested in using Notion for what would be my plausible PhD journey in the future. For this reason, I have made a couple of templates tailored to ease up my workflow when I start my PhD. At the time of writing this article, I am using Notion as a literature tracker, research diary (I will start using it once I get a PhD position), task manager, movie and book list tracker and as a simple finance tracker. If any of these templates interest you, then you are lucky as I have made all of them free for use! Furthermore, I have also written tutorials for some of these templates which you can read and get a better idea of how to use them."
  },
  {
    "objectID": "posts/notion_intro/index.html#tutorials-on-the-templates",
    "href": "posts/notion_intro/index.html#tutorials-on-the-templates",
    "title": "My Notion story",
    "section": "\n6 Tutorials on the templates",
    "text": "6 Tutorials on the templates\n\n\nUsing Notion as a literature tracker.\n\nUsing Notion to keep a research diary.\n\nUsing Notion as a task management software.\n\nUsing Notion as a reading list tracker and a movie list tracker.\n\nUsing Notion as a simple finance ledger. (I am still in the process of developing it, you can download the current version from the link)\n\nIf you want all these templates at once, then I have made a master template which has all of them together. You can get it here.\nI hope you find the templates useful and I wish you all the best in learning Notion. You can share your feedback and thoughts about the templates in the comment section below."
  },
  {
    "objectID": "posts/notion_intro/index.html#useful-references",
    "href": "posts/notion_intro/index.html#useful-references",
    "title": "My Notion story",
    "section": "Useful References",
    "text": "Useful References\n\nBeginners introduction to Notion. Source\n\nBasics of Notion. Source\n\nQuick tutorial on setting up Notion. Source\n\nNotion official website has detailed guides for very single feature in Notion. Source"
  },
  {
    "objectID": "posts/notion_literature/index.html",
    "href": "posts/notion_literature/index.html",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "",
    "text": "Use Notion to build a literature database that can integrate with directories in Zotero with the help of the Notero plugin\nInstall Notion, Zotero and the Notero plugin\nDuplicate the literature template to your Notion database"
  },
  {
    "objectID": "posts/notion_literature/index.html#literature-tracker-using-notion-and-zotero",
    "href": "posts/notion_literature/index.html#literature-tracker-using-notion-and-zotero",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "\n2 Literature tracker using Notion and Zotero",
    "text": "2 Literature tracker using Notion and Zotero\nThis is a follow-up article from my earlier post on My Notion story. If you don‚Äôt know what Notion is or how to install it, please refer to my earlier article for the background information.\nIn this article, we will aim to build a literature tracker that looks similar to the one below. It will be automated via the Notero plugin integrating Zotero with Notion.\n\n\n\nMy literature tracker template in Notion\n\n\n\n\n2.1 Install Zotero\nThe first thing to do is to download and install Zotero, a free-to-use reference manager that you can use to categorize and manage your research article collection. You should also create an account in Zotero and log in using that account in your Zotero app.\n\n2.2 Installing Notero plugin\nNow we are going to install a plugin for Zotero, called Notero by David Vanoni, which acts as a link between Zotero and Notion. This is what syncs your Zotero library to your Notion database.\n\nGo to the Github page of Notero and download the notero-0.3.5.xpi file under the assets section. You can right-click on the file and save it by clicking on ‚Äòsave link as‚Äô\n\n\n\n\n\n\n\nVideo: Downloading the plugin (click here)\n\n\n\n\n\nVideo\n\n\n\n\nGo to the Zotero app and click on ‚ÄòTools‚Äô and then click ‚ÄòAdd-ons‚Äô. Then in the new window, click on the Settings button and then click on ‚ÄòInstall add-on from file‚Äô. Then browse to where your notero-0.3.5.xpi file is downloaded and then install it.\n\n\n\n\n\n\n\nVideo: Installing the plugin\n\n\n\n\n\nVideo\n\n\n\n\n2.3 Configure Notero plugin\n\nAfter installing Notero, you can find a new option called ‚ÄòNotero Preferences‚Äô under the ‚ÄòTools‚Äô section. To have the plugin work, you need to provide an ‚Äòintegration token‚Äô and a ‚Äòdatabase ID‚Äô.\nTo get the integration token, go to Notion integrations and click on ‚ÄòNew integration‚Äô. Give a suitable name (something like ‚ÄòNotero integration‚Äô for the ease of finding it) and then select your workspace in Notion. You can keep the rest of the options in their default setting and then click save. You will then get your integration token.\n\n\n\n\n\n\n\nVideo: Getting the integration token\n\n\n\n\n\nVideo\n\n\n\n\nGo back to your Notion app and then select the database which you want to integrate with the Notero plugin. If you are creating a database from scratch then make sure that the table that you are creating has the following named columns.\n\n\n\nImage from Notero Github page\n\n\nIf you want to use my template then click here and click on Duplicate. It contains most of the relevant columns and will save you time from creating it from scratch.\n\n\nDuplicating a template in Notion\n\n\n\nOnce you have the database that you want to integrate with Notero, click on the ‚ÄòShare‚Äô button and click on the ‚Äòcopy link‚Äô. Paste the link in a text editor and copy the first 36 characters after your workspace name. This is your ‚Äòdatabase ID‚Äô.\n\n\n\n\n\n\n\nThe format of the copied link would in this form\n\n\n\nhttps://www.notion.so/{workspace_name}/{database_id}?v={view_id}\nCopy the {database_id} part\n\n\n\n\n\n\n\n\nVideo: Getting the database ID\n\n\n\n\n\nVideo\n\n\n\n\nThen finally paste your ‚Äòintegration token‚Äô and the ‚Äòdatabase ID‚Äô into the ‚ÄòNotero Preferences‚Äô window under the ‚ÄòTools‚Äô section in Zotero. Then select the directory in Zotero that you want to integrate with your Notion database. Now you have successfully configured the Notero plugin.\n\n\n\nNotero Preferences window\n\n\nNow go to the associated database and click ‚ÄòShare‚Äô. Search for your integration and click invite. Any research article that is present in your directory associated with the Notero plugin will now automatically be synced to your associated Notion database. Notero plugin allows one-time sync between Zotero to Notion but not the other way around. So any changes that you do in the Notion database won‚Äôt be reflected in your Zotero database.\n\n\n\n\n\n\nVideo: Notero in action\n\n\n\n\n\nVideo\n\n\n\nThere is a reason why I said it‚Äôs a one-time sync, any modifications that you do for already existing files won‚Äôt be reflected in Notion. But this can be easily fixed by assigning the existing papers to a tag, you can also delete it if you dont want it to appear in the database.\n\n\n\n\n\n\nVideo: Updating existing files with a new tag\n\n\n\n\n\nVideo\n\n\n\nThere are some limitations on what the plugin is capable of doing, but it‚Äôs still better than nothing. You can learn more about the plugin from its Github repo page.\nSo there you have it, your very own literature tracker made using Notion and integrated with Zotero using the Notero plugin. I am so proud of you üëç\nThank you David Vanoni and the developers who contributed to developing the Notero plugin. You can share your feedback and thoughts about the templates in the comment section below."
  },
  {
    "objectID": "posts/notion_movie_books/index.html",
    "href": "posts/notion_movie_books/index.html",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "",
    "text": "Use Notion to track your movie and book lists\nDuplicate the movie list tracker and the book list tracker templates to your Notion database and use them."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#using-notion-to-keep-a-movie-list-tracker",
    "href": "posts/notion_movie_books/index.html#using-notion-to-keep-a-movie-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n2 Using notion to keep a movie list tracker",
    "text": "2 Using notion to keep a movie list tracker\nThis was a fun little project where I made two templates that helps in maintaining reading and a watch list tracker for books and movies. I heard from my friends who are doing PhD that they spent a hefty amount of time binge-watching movies and series and most of them also read a lot of new books. So I thought why not make a template that will help them manage this. The templates are very simple and visually pleasing.\nThis might be my shortest article ever on this blog but I thought this post can act as a source to receive feedback and comments about the templates. The links for the templates are given below.\n\nMovie list tracker\nBook list tracker"
  },
  {
    "objectID": "posts/notion_movie_books/index.html#movie-list-tracker",
    "href": "posts/notion_movie_books/index.html#movie-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n3 Movie list tracker",
    "text": "3 Movie list tracker\n\n\n\nMovie list tracker\n\n\n\nYou can categorise the list to their ‚Äòwatch status‚Äô and also indicate if it‚Äôs a movie or a series. Furthermore, if watch a lot of series, then you can also add in the extra columns indicating the season and so on. In the poster column, I have embedded an online image link showcasing the movie or series poster. This is done so that the media collection view (gallery view) can use these images and show visually pleasing cards. You can get the poster image links for most of the movies and series from here. The progress section shows the data in a ‚Äòkanban‚Äô view to quickly visualize the watch list according to their status."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#book-list-tracker",
    "href": "posts/notion_movie_books/index.html#book-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n4 Book list tracker",
    "text": "4 Book list tracker\n\n\n\nMovie list tracker\n\n\n\nThe book list tracker is almost identical to the movie list one. To get the book cover image links for the database, you can visit here. The way you add entries is also pretty self intuitive so I urge you to explore it yourself."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#conclusion",
    "href": "posts/notion_movie_books/index.html#conclusion",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion\nI hope you find these templates useful. You can give your feedback and thoughts about the template in the comment section below this article."
  },
  {
    "objectID": "posts/notion_research_diary/index.html",
    "href": "posts/notion_research_diary/index.html",
    "title": "Using Notion to keep a research diary",
    "section": "",
    "text": "Use Notion to write and maintain a research diary\nDuplicate the diary template and use it in Notion"
  },
  {
    "objectID": "posts/notion_research_diary/index.html#using-notion-to-keep-a-research-diary",
    "href": "posts/notion_research_diary/index.html#using-notion-to-keep-a-research-diary",
    "title": "Using Notion to keep a research diary",
    "section": "\n2 Using notion to keep a research diary",
    "text": "2 Using notion to keep a research diary\nIn this article, we will see how we can use Notion to keep a research diary. If you are a researcher or a PhD student, then a research diary would be a very important piece of intellectual work that will be staying with you for quite a few years. So it must be well written, well-formatted and efficiently organised. With the power of Notion, I will show you a template that I made that can be used in Notion and can act as an electronic research diary.\nYou can get the template by clicking here. Duplicate it into your Notion workspace.\n\n\n\nResearch diary database\n\n\n\nThe picture above shows the research diary database. Each row value created in the table will be each day‚Äôs entry. If you also have a literature tracker database in Notion workspace, then you can couple it with your diary entry and it will showcase any papers that you have read one that day.\nSo let us try writing an entry. First, make a blank row entry and then hover over the newly created empty row and click ‚ÄòOPEN‚Äô. This will open a new window. In that window click on ‚ÄòDiary entry template‚Äô. This will bring you a sample template. You can fill in relevant info or add in your subsection and modify the entry.\n\n\n\nResearch diary template\n\n\n\n\nThere are mainly five sections in the template. In the first section, you can outline what experiments or analyses or any other related work you did on that day. You can also attach any relevant pictures if it helps.\nIn the second section, you can outline the challenges you came across while implementing your work. It can be problems you encountered in your experiment or a bug in your code while doing data analysis etc.\nThe third section outlines your thoughts and ideas, it can either be possible solutions that you want to try out with regards to the challenges you faced or a new research question that you had while reading a paper.\nIn the fourth section of the template, I have attached my literature tracker database to the diary entry page. Now if I happened to read any research articles on that day, then by filtering the database to the diary entry date, I can have all the papers that I read on that date show up. This can be pretty convenient to track your reading goals. You can extend this idea and showcase any experiments that you have done on that day if you have an experiment database in your Notion database.\nThe fifth and last section of the template can have any other relevant work information which does not fall into the earlier section. It can be purchase reminders, suggestions by your supervisors or colleagues, deadlines for meetings or presentations etc.\n\nThis is a simple research diary template that I have created and I hope you find it useful. Suggestions and feedback to improve the diary template are most welcome. Please comment your thoughts below the article. Thanks!\n\n\n\n\n\n\nVideo: Making a diary entry (click here)\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/notion_research_diary/index.html#useful-references",
    "href": "posts/notion_research_diary/index.html#useful-references",
    "title": "Using Notion to keep a research diary",
    "section": "Useful References",
    "text": "Useful References\n\nThe research diary image in the thumbnail for this post belongs to Dr.¬†Julia Everitt (@juilaeverittdr). You can read more about research diary here."
  },
  {
    "objectID": "posts/notion_task_manager/index.html",
    "href": "posts/notion_task_manager/index.html",
    "title": "Using Notion as your task manager",
    "section": "",
    "text": "Use Notion to manage your daily tasks\nFour templates to download; To-do-list, Completed task list, Task manager and Task dashboard\n\nThose who want to download all the templates in one go, click here\n\nDuplicate and use them in Notion"
  },
  {
    "objectID": "posts/notion_task_manager/index.html#using-notion-as-a-task-management-dashboard",
    "href": "posts/notion_task_manager/index.html#using-notion-as-a-task-management-dashboard",
    "title": "Using Notion as your task manager",
    "section": "\n2 Using Notion as a task management dashboard",
    "text": "2 Using Notion as a task management dashboard\nWith the ability to make databases, Notion becomes an excellent tool to manage task lists. In this article, I am going to give you a quick tour of how you can use Notion as a task management dashboard with the templates I made.\nThere are four templates to add to your Notion database and you need all four of them to function properly. If you don‚Äôt want to individually download each of them, then I have a single-page master template which features all the templates I made in one place. You can find it here.\n\n\nThe templates of interest for this article\n\n\n\n\nTo-do-list: Acts as the master database where all tasks are stored. By default only shows incomplete tasks.\n\nCompleted task list: Same template as above, but only tasks that are completed are shown. Act as an archive for completed tasks.\n\nTask manager: Task manager interface where each task is arranged and displayed categorically. By default only shows incomplete tasks.\n\nTask dashboard: A dashboard which informs you which tasks are due and how long are they due to completion."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#to-do-list",
    "href": "posts/notion_task_manager/index.html#to-do-list",
    "title": "Using Notion as your task manager",
    "section": "\n3 To-do-list",
    "text": "3 To-do-list\nFirst, let us see the To-do-list. Specifically, we will be seeing the ‚ÄòMaster table‚Äô view which lists out all the tasks which are yet to be completed.\n\n\n\nTo-do-list\n\n\n\nLike any basic to-do list, this list has task names and due date columns, along with many other columns. I have also tagged the tasks into their respective category and their priority. The ‚ÄòDate added‚Äô and ‚ÄòDue status is automatically calculated once you input a row value in the table. The ‚ÄôDue status‚Äô column is dependent on a formula which takes in the ‚ÄòDue date‚Äô value. The possible values for the ‚ÄòDue status‚Äô column are; task finished, overdue, due today, due tomorrow, someday and later. Here ‚Äòsomeday‚Äô label means that the task has no due date mentioned and the ‚Äòlater‚Äô label means that the task has at least 7 days before the due. The ‚Äòtask finished‚Äô is only given to tasks which have the ‚ÄòDone‚Äô column check box checked. The rest of the labels are what it says.\nYou can change every single aspect of this table to suit your needs. What might be tricky to change is the ‚ÄòDue status‚Äô column as it is governed by a formula which may look complex but is very easy.\n\n\n\n\n\n\nDue status formula\n\n\n\nif(prop(\"Done\") == false, \nif(formatDate(prop(\"Due date\"),\"L\") == formatDate(now(), \"L\"), \"Due today\",\nif(prop(\"Due date\") < now(), \"Overdue\",\nif(prop(\"Due date\") <= dateAdd(now(), 1, \"days\"), \"Due tomorrow\",\nif(empty(prop(\"Due date\")), \"Someday\", \"Later\")))), \"Task finished\")\n\n\nThe formula is a series of if statements. The syntax for the if statement in Notion is as follows;\nboolean ? value : value\nif(boolean, value, value)\nFrom the syntax, you can see that the if statement is dependent on a Boolean value.\nSo let us try to understand what this formula is trying to calculate, by taking each section separately.\n\n\n\n\n\n\nif(prop(‚ÄúDone‚Äù) == false\n\n\n\nThis means that if the ‚ÄòDone‚Äô column is unchecked then run the rest of the statements, if not, then it returns the label ‚ÄòTask finished‚Äô which you can see in the last part of the formula. The checkboxes in Notion follow Boolean values. Checked means true and unchecked means false.\n\n\n\n\n\n\n\n\nif(formatDate(prop(‚ÄúDue date‚Äù),‚ÄúL‚Äù) == formatDate(now(), ‚ÄúL‚Äù), ‚ÄúDue today‚Äù\n\n\n\nThis is yet another ‚Äòif‚Äô statement but this time we are reformatting the date. The reason why we are reformatting the date is that, in Notion, any date is followed by time, so 12-12-2022 is Notion is denoted as 12-12-2022 12:00 AM. So there can be some issues where ‚ÄòDue today‚Äô won‚Äôt be properly shown. So reformatting the date to ‚Äòday‚Äô units is easier to work with. The now() function returns the current date. Therefore this formula simply returns the label ‚ÄòDue today‚Äô if the due date matches the current date.\n\n\n\n\n\n\n\n\nif(prop(‚ÄúDue date‚Äù) < now(), ‚ÄúOverdue‚Äù\n\n\n\nThis formula returns an ‚ÄòOverdue‚Äô label if the due date is past the current date.\n\n\n\n\n\n\n\n\nif(prop(‚ÄúDue date‚Äù) <= dateAdd(now(), 1, ‚Äúdays‚Äù), ‚ÄúDue tomorrow‚Äù,\n\n\n\nThe dateAdd() function takes in the first value, which is the current date returned from the now() function, and then adds 1 unit of ‚Äúdays‚Äù to it. Therefore the formula boils down to checking if the due date is less than or equal to tomorrow and returns the ‚ÄòDue tomorrow‚Äô label if it does. We give less than or equal to sign because we want the counter-statement to be strictly greater than so that, only then does the rest of the if statement runs. Also, the less than condition can never be fulfilled here as the first statement checks if the due date matches the current day.\n\n\n\n\n\n\n\n\nif(empty(prop(‚ÄúDue date‚Äù)), ‚ÄúSomeday‚Äù, ‚ÄúLater‚Äù\n\n\n\nNow sometimes we don‚Äôt want to give the due date or we forgot to give them to a task. This is notified by the ‚ÄòSomeday‚Äô label. Now the ‚ÄòLater‚Äô label is given when the ‚ÄòDue tomorrow‚Äô condition fails, which means that the due date is greater than tomorrow.\n\n\nNow the reason why I went to explain this formula so elaborately is for you to appreciate the formula feature in Notion. Using clever little formulas like this you can efficiently automate many boring tasks. If it wasn‚Äôt for this formula, we would have to manually check between the current date to the due date. So something to keep in mind while designing Notion databases.\nNow Notion also features some powerful ways of visualizing tabular data via ‚Äòviews‚Äô. You can see that there is a ‚ÄòGroup table‚Äô, ‚ÄòPriority table‚Äô and a ‚ÄòStatus table‚Äô views in the database. These are nothing but the ‚ÄòMaster table‚Äô data grouped into different categories for easy visualization.\n\n\n\n\n\n\nVideo: Different views of the to-do list (click here)\n\n\n\n\n\nVideo\n\n\n\nOkay, now we have a fair idea of what is there in this database and what each of the columns does. Now the funny part is that we won‚Äôt be adding any tasks to this table using this page. Instead, we will be exclusively using the ‚ÄòTask manager‚Äô for it."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#task-manager",
    "href": "posts/notion_task_manager/index.html#task-manager",
    "title": "Using Notion as your task manager",
    "section": "\n4 Task manager",
    "text": "4 Task manager\n\n\n\n\n\n\nVideo: Task manager\n\n\n\n\n\nVideo\n\n\n\nThe task manager page is your go-to page to list out incomplete tasks. By default, the page has categorized all tasks into their respective category as subsections. The inbox section is where you add your task to the to-do list and as soon as you add the associated category, it vanishes from the inbox and goes to its category subsection. Thus you don‚Äôt have to worry about post categorizing into the subsection parts after you have added a task. And once you check the ‚ÄòDone‚Äô column for a task, it is then archived on the Completed task list page to preserve the history of completed tasks.\n\n\n\n\n\n\nVideo: Adding a task in the task manager\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/notion_task_manager/index.html#task-dashboard",
    "href": "posts/notion_task_manager/index.html#task-dashboard",
    "title": "Using Notion as your task manager",
    "section": "\n5 Task dashboard",
    "text": "5 Task dashboard\n\n\n\nTask dashboard\n\n\n\nTreat the task dashboard page as your command centre for all the tasks. Remember all those ‚Äòdue status‚Äô labels that we had in the to-do list database. With the help of Notion‚Äôs grouping feature for databases, we can easily visualize all the tasks according to their deadlines. In the ‚Äòtask list subsection,‚Äô I have also added table views showing tasks associated with a particular priority. The purpose of this page is to easily get a glance at tasks that close the due date."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#completed-tasks",
    "href": "posts/notion_task_manager/index.html#completed-tasks",
    "title": "Using Notion as your task manager",
    "section": "\n6 Completed tasks",
    "text": "6 Completed tasks\nThe completed tasks page acts as an archive showcasing all the completed tasks. As someone used to say to me, it brings great joy to tick off all those tasks from the to-do list after spending your time and energy completing them. The primary function of this page is to preserve and record task completion history. The database is identical to that of the earlier shown to-do-list database, as I have filtered it to only show row values where the ‚ÄòDone‚Äô column is checked."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#conclusion",
    "href": "posts/notion_task_manager/index.html#conclusion",
    "title": "Using Notion as your task manager",
    "section": "\n7 Conclusion",
    "text": "7 Conclusion\nI hope you have fun using this template. I enjoyed making this template and I aimed to make it simple and effective to use. To summarise, the to-do list is your main database, actually your only database. Everything else is just a modified version of this database where the info is shown in a different form using filtering and grouping features. After duplicating this template, you can tweak each of the parameters to suit your liking. You only need to change the parameters in the to-do-list database as the changes in it will be reflected in the rest of the pages. The only other thing that you will be needing to modify after you have changed the key parameters in the database are the headings, which are pretty straightforward to change. With that, I hope you find this template useful. Any feedback and suggestions are most welcome. You can comment them below the article."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html",
    "href": "posts/research_article_biggest_bacterium/index_post.html",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "",
    "text": "Summary of the study in (1)\n\n\nResearch article that is summarised in this post: Volland, J.-M., Gonzalez-Rizzo, S., Gros, O., Tyml, T., Ivanova, N., Schulz, F., Goudeau, D., Elisabeth, N. H., Nath, N., Udwary, D., Malmstrom, R. R., Guidi-Rontani, C., Bolte-Kluge, S., Davies, K. M., Jean, M. R., Mansot, J.-L., Mouncey, N. J., Angert, E. R., Woyke, T., & Date, S. V. (2022). A centimeter-long bacterium with DNA contained in metabolically active, membrane-bound organelles. Science, 376(6600), 1453‚Äì1458. https://doi.org/10.1126/science.abb3634."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#introduction",
    "href": "posts/research_article_biggest_bacterium/index_post.html#introduction",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n2 Introduction",
    "text": "2 Introduction\nWhen you think of bacteria, the first thing that might come to your mind is a microscope, given how tiny they are to see. Most of the bacteria and archaea are about ~ 2 \\mu m in size (1). The record for the smallest living organism is shared between the pathogenic bacteria, Mycoplasma pneumoniae and the Thermodiscus sp. belonging to Archae. Both can be as small as 0.2 \\mu m in size (2). The biggest bacteria was thought to be Thiomargarita namibiensis, which can be as big as 750 \\mu m in size (average size is 180 \\mu m) and can be seen with the naked eye (3). But the recent discovery of a new bacteria species has overtaken T. namibinesis to become the biggest bacteria ever. Moreover, it is about ~50 times bigger on average, compared to T. namibinesis and is seen in lengths of more than 9000 \\mu m, which is about 1cm! To put that in perspective, a housefly, which is a complex multi-cellular organism is on average only about 0.5 \\ to\\ 0.7cm long!\nMeet Thiomargarita magnifica, your newest species of bacteria, which as we know is the biggest bacteria to date. As reported by a group of researchers in a recent paper (1) in Science, there is more than just the size that makes this bacteria special. Thiomargarita magnifica is packed with so many complex features that it is blurring the lines between what we considered to be prokaryotes and eukaryotes."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#discovering-the-biggest-bacteria",
    "href": "posts/research_article_biggest_bacterium/index_post.html#discovering-the-biggest-bacteria",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n3 Discovering the biggest bacteria",
    "text": "3 Discovering the biggest bacteria\n\n\n\nFigure¬†1: Images (A) and (B) are taken from supplementary fig.¬†S1. in (1). (A) Shows Thiomargarita magnifica (white-filaments) attached to sunken leaves of Rhizophora mangle. (B) Buds forming on the apical pole the filament\n\n\n\nResearchers first discovered Thiomargarita magnifica attached to the sunken leaves of Rhizophora mangle (as seen in Figure¬†1) present in the shallow tropical marine mangroves from Guadeloupe, Lesser Antilles, a French overseas province. The bacteria appeared as a long-white filament and was first thought to be a fungus. It was only later through various techniques that the researchers concluded that this is not a fungus, but a new species of bacteria and rather a very long one that is. Since the bacteria is not yet culturable in laboratory conditions, under the nomenclature followed in microbiology, they are called Candidatus (Ca.) Thiomargarita magnifica."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#elucidating-the-biggest-bacteria",
    "href": "posts/research_article_biggest_bacterium/index_post.html#elucidating-the-biggest-bacteria",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n4 Elucidating the biggest bacteria",
    "text": "4 Elucidating the biggest bacteria\n\n4.1 The whole filament is a single bacterial cell\n\n\nFigure¬†2: Images (A), (B) and (C) are taken from supplementary fig.¬†S4. Image (D) is taken from Fig. 1. All images are from (1). In all the images the red outline represents the plasma membrane, arrowheads represent division septa and ‚ÄúV‚Äù in the image (D) represents a large central vacuole.(A) Cell membrane of Cyanobacterium Microcoleus vaginatus showing discontinuity throughout the filament because of the presence of division septa.(B) The membrane septum separating vacuolated cells of a Beggiatoa-like filament.(C) The membrane septum separating the large vacuolated cells of the Marithrix-like bacterial filaments.(D) Continuous cell membrane of with no division septa in (Ca.) T. magnifica.\n\n\n(Ca.) T. magnifica belongs to the family of large sulphur bacteria (LSB). LSBs are known to form long filaments of lengths up to 200 \\mu m (4‚Äì6). But these filaments are built up of individual LSB cells and therefore cannot be considered as a single living organism. Thus, the white filament was initially thought to be made of individual cells. To check this, researchers used dyes to visualize the entire plasma membrane of the filament. If it was made of individual cells, then the dye will show discontinuity in the plasma membrane structure indicating septa divisions. But to their surprise, they found no discontinuity and concluded that the whole filament is a single bacterial cell (as seen in Figure¬†2). Moreover, the filaments also had small buds forming from its apical pole and some of these filaments along with the buds were up to 2cm in size.\n\n4.2 Inside the biggest bacteria: Large central vacuole, sulphur granules and pepins\n\n\n\n\nFigure¬†3: Image (E) is taken from fig.¬†1. in (1). (E) Image shows a bud forming from the mother cell, where a Large central vacuole is present and is represented as ‚ÄúV‚Äù.\n\n\nInterestingly the whole filament was not filled with the cytoplasm rather, a large central vacuole was present in the middle (as seen in Figure¬†2 and Figure¬†3). This vacuole pushed the cellular cytoplasm to its periphery. The vacuole contributed to around 70% of the total cell volume. Therefore, although the cell is large, most of its volume is metabolically inactive. Apart from the vacuole, lucent vesicles were also found, which when further analyzed revealed to be filled with sulfur granules (see Figure¬†4). As Thiomargarita spp. are sulfur-oxidizing gammaproteobacteria (7), this was not a surprise.\nAdditionally, electron-rich membrane-bound compartments were found within the cytoplasm (see Figure¬†4), which was similar to structures previously reported in other LSBs. They were hypothesized to be compartments containing ribosomes and genetic material (8). To check if this holds, researchers used a stain to check where the genetic material was localized in (Ca.) T. magnifica. It was found that almost all of its DNA was concentrated inside these compartments. Further analysis also showed evidence of structures that were 10 to 20 nm in size, inside these compartments, similar to ribosomes. Researchers then used a technique to look for ribosomal RNAs and confirmed their presence inside these structures which indicated that those structures were ribosomes. Adding to this evidence, when translational activity was checked, they were consistently seen within most of the sites where ribosomal RNA was found.\n\n\nFigure¬†4: Image (G) is taken from fig.¬†1. in (1). (E) The arrowheads show two electron-rich membrane-bound compartments called pepins, the letter ‚ÄúS‚Äù represents sulphur granules and ‚ÄúV‚Äù represents a large central vacuole.\n\n\nThese membrane-bound structures containing DNA and ribosomes, which were not seen attached to the cell membrane are equivalent to the compartmentalization seen in eukaryotes, where DNA is present inside the nucleus and ribosomes are attached to the endoplasmic reticulum. Thus, this new membrane-bound organelle was named ‚Äòpepins‚Äô by the researchers, because of how they resemble pips or seeds in a watermelon. Overall, the cytoplasm of (Ca.) T. magnifica contained sulfur granules, pepins and various membrane structures forming a complex membrane network that spanned the whole cytoplasm.\n\n4.3 Localization of ATP synthase\nPeriplasm is a gel-like matrix present between the outer membrane and the inner membrane of a gram-negative bacteria\n\n\n\n\n\n(a) Image is taken from fig.¬†1. in: (9). Electron transport chain of E. coli\n\n\n\n\n\n\n\n\n(b) Image is taken from fig.¬†1. in: (10). Electron transport chain in a mitochondrial cell\n\n\n\n\nFigure¬†5: Electron transport chains of prokaryotes and eukaryotes\n\n\nAnother defining feature between eukaryotes and prokaryotes is the localization of ATP synthase. In eukaryotes, the ATP synthase is localized in the mitochondrial membrane where the electron transport chain (ETC) protein complexes are present (10). But for bacteria and archaea, mitochondria are absent and the ATP synthase is localized to the cell membrane along with the other protein complexes present in the ETC (9). Researchers checked within (Ca.) T. magnifica, where the ATP synthase is localized. Surprisingly ATP synthase was found in the membranes of pepins and the complex membrane structure, throughout the cytoplasm (see Figure¬†6). But ATP synthase was absent in the cellular membrane, this was similar to eukaryotes, where ATP synthase is only present in the mitochondria, which is present within the cytoplasm. These results together elevated pepins as not just a housing compartment for ribosomes and DNA, but also as a potential energy-producing organelle.\n\n\n\n\nFigure¬†6: Image (F) is taken from supplementary fig.¬†S20. in: (1). (E) The red colour indicates ATP synthase and blue colour dots highlight DNA which is used here as a proxy for where the pepins are situated. As you can see, the ATP synthase is expressed throughout the complex membrane network and the pepins present in the cytoplasm\n\n\nThe surprises do not stop there. Researchers further found that (Ca.) T. magnifica also showed extreme polyploidy, with around 740,000 genome copies for a 2 cm long cell. This is the highest number of genome copies ever reported for a single cell. The genome copies were very similar in their composition. A single genome copy was found to be 11.5 and 12.2 Mb in size, comparable to the popular Saccharomyces cerevisiae genome (12.1 Mb). (Ca.) T. magnifica genome had around 11,788 genes, more than three times the median gene count of prokaryotes (3935 genes) (11). Interestingly many of the common genes present within prokaryotes which coded for proteins that are responsible for cell division were absent here. But cell elongation protein genes were found to be intact and were found to be duplicated, forming multiple copies placed beside each other in the genome, which might suggest a possible reason why (Ca.) T. magnifica grow so long.\nThe researchers also found that, unlike most bacteria which reproduce via binary fission where cell volume doubles before cell division, in (Ca.) T. magnifica, they follow a dimorphic lifecycle; a bud-like cell stage and a filament-like cell stage. When they reproduce, the apical pole of the filament mother cell constricts and buds off to become a daughter cell (see Figure¬†1 and Figure¬†2). Interestingly, the genetic material is passed on to the daughter cell via pepins which is never seen before in bacteria."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#conclusion",
    "href": "posts/research_article_biggest_bacterium/index_post.html#conclusion",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion\nOverall (Ca.) T. magnifica turned out to be a very bizarre form of bacterial species that under the current understanding of cell morphology and energetics should not exist. So how come we find this bacteria in nature? Most bacteria are unable to grow to large sizes as they are met with various constraints related to energy and space. An overview of these constraints is given below.\n\nAs bacteria lack any mode of intracellular transporters to transport materials across the cytoplasm, they rely exclusively on diffusion. Chemical diffusion is extremely slow and is only effective in micro meter distances. So if the cell is too large, it will take a tremendous amount of time to disperse proteins and other materials effectively across the cell (2).\nWhen cell size increases, it reaches a point where the number of ribosomes needed to sustain the cell exceeds the available cell volume. This is called ‚Äòribosome catastrophe‚Äô and the upper limit of the cell volume for this catastrophe is 1.39¬±0.03 √ó 10^{‚àí15}m^3. The reported cytoplasmic volume of (Ca.) T. magnifica was around 5.91 √ó 10^{‚àí12} m^3, which is several orders greater than the ribosome catastrophe limit (12).\nPlasma membrane is the only location where ATP production can happen via the electron transport chain (ETC). And for bacteria and archaea, the only available membrane that they can use for ETC is their cellular membrane. Therefore as cell size increases, the surface area increase will quickly be outpaced by the volume increase owing to their differences in orders of increase. And as volume increase is followed by increased metabolic activity, energy production will become a bottleneck (12, 13).\n\nNow even if these constraints make perfect sense in the context of cell biology, there are always outliers in nature which defy them. One such outlies is this recently discovered bacterial species: (Ca.) T. magnifica.\nHow is (Ca.) T. magnifica overcoming these constraints? First of all, they have multiple copies of their genome compartmentalised along with ribosomes inside pepins. And these pepins are present across the cytoplasm. This might solve the diffusion limit problem because now the protein and other materials are almost always accessible across the cytoplasm at any time. Efficient compartmentalisation again might be the answer to surpassing the ribosome limit. In the case of the energy production limit attained due to surface area constraints, for (Ca.) T. magnifica, ATP synthase is expressed within the cytoplasm via pepin membranes and the complex membrane structure. Therefore, the available plasma membrane for energy production is much higher compared to other prokaryotes.\nIn the end, when we look at these features carefully, it is analogous to the various features present in eukaryotes. The polyploid genome of (Ca.) T. magnifica is equivalent to polypoid mtDNA present in eukaryotes. Compartmentalisation of DNA and ribosome via pepins is equivalent to compartmentalisation seen in the nucleus and the endoplasmic reticulum. And perhaps the icing on the cake is that (Ca.) T. magnifica transfers its genetic material via a membrane-bound organelle; pepins, something that is never seen before in prokaryotes. Therefore (Ca.) T. magnifica stands as one of the extreme cases of bacterial evolution which blurs our understanding of what constitutes a eukaryote and a prokaryote.\nFor more details about the study, please find the original paper (1)."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html",
    "href": "posts/research_article_embryonic_learning/index_post.html",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "",
    "text": "Summary of the study (1)\n\n\nResearch article that is summarised in this post: Crowder, C., & Ward, J. (2022). Embryonic antipredator defenses and behavioral carryover effects in the fathead minnow (Pimephales promelas). Behavioral Ecology and Sociobiology, 76(2), 27. https://doi.org/10.1007/s00265-022-03136-2"
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#introduction",
    "href": "posts/research_article_embryonic_learning/index_post.html#introduction",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n2 Introduction",
    "text": "2 Introduction\nLearning is the process of acquiring information through experience. In nature, learning is ubiquitous in many different organisms and offers fitness advantages. Honeybees are known to learn floral features and utilize them during foraging (Review: 2), Wolf spiders (family Lycosidae) learn to avoid lesser quality prey when higher quality alternative prey is presented along with it (3), ants are known to learn and associate visual landmarks to their nest location (4). Learning is also particularly effective during different life stages. The experience acquired during the juvenile phase of an animal is known to affect their behaviour in adulthood. Baby Zebra finches (Taeniopygia guttata) are known to innately possess a ‚Äòtemplate song‚Äô which is used as a base to learn and mimic songs sung by adult conspecifics. If baby zebra finches are isolated and not allowed to mimic the song, then when they reach adulthood, they develop irregular and abnormally sound songs as compared to experienced adults (5). In the Tobacco hornworm moth, an aversive odour stimulus when learned during their larval stage persists when they metamorphose into a moth (6). In houseflies, adults develop a preference for olfactory cues that were exposed to them during their larval stage (7)."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#embryo-is-sensitive-to-external-cues",
    "href": "posts/research_article_embryonic_learning/index_post.html#embryo-is-sensitive-to-external-cues",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n3 Embryo is sensitive to external cues",
    "text": "3 Embryo is sensitive to external cues\nMost studies in the context of learning focused on fitness advantages to adults when learning was manifested during the juvenile phase. But some organisms, especially oviparous organisms undergo their embryonic stage outside of their mother‚Äôs body where the embryos are in direct contact with the surrounding environmental cues. They are also found to be highly sensitive to these cues. Perhaps the most famous evidence for this would be gender determination in reptiles, where the eggs respond to their surrounding ambient temperature. Here, gender is determined by temperature values after fertilization (Review: 8). In addition to sensing environmental cues, many studies have also reported evidence of ‚Äúembryonic learning‚Äù, where the embryos learn and form memories of surrounding environmental cues and utilize these memories in both the pre and post-embryonic period to modulate their behaviour. Some of the cases where the embryo is found to respond and/or learn external cues are given below;\n\nEggs of ringed salamanders (Ambystoma annulatum, Figure¬†1) when exposed to chemical cues from predators resulted in reduced activity and greater shelter-seeking behaviour in larvae (9).\nWood frog (Rana sylvatica, Figure¬†2) tadpoles learned to respond to chemical cues from unfamiliar predators when associated with conspecific alarm cues during the embryo stage (9).\nNaive cuttlefish (Sepia officinalis, Figure¬†3) prefer shrimps over crabs, but cuttlefish embryos which are visually conditioned with crabs preferred crabs over shrimps during the larval stage (10). Also, cuttlefish embryos that had experience with white crabs1 developed a preference for white crabs over black crabs2 during the larval stage (10). Moreover, cuttlefish larvae that received embryonic experience with white crabs exhibited prey generalisation where they prefer black crabs over shrimp (10).\nCuttlefish embryos (Sepia officinalis, Figure¬†3) are also seen to reduce their ventilation rate in the presence of predator indicative cues, possibly to remain cryptic. Furthermore, cuttlefish embryos are also found to associate non-predator cues with predator cues to invoke reduced ventilation behaviour which showcases evidence for associative learning in these embryos (11).\nRainbow trout embryos (Oncorhynchus mykiss, Figure¬†4) can learn both conspecific and heterospecific alarm cues and, rainbow trout fish with embryonic learning experiences had longer memory retention compared to the ones with no embryonic learning experience (12).\nBamboo sharks (Chiloscyllium punctatum, Figure¬†5), like all sharks, have electroreception which is normally used to locate prey. But unlike most other sharks where the embryo is developed within the mother‚Äôs body, in bamboo sharks egg sacks containing the embryo are oviposited on substrates which makes them vulnerable to predation (13). It is known that bamboo shark embryos can use their electrosensory system to innately detect electric fields indicative of predators and thereby cease their respiratory gill movements to become cryptic (14).\nPort Jackson shark (Heterodontus portusjacksoni, Figure¬†6) embryos are found to modulate their oxygen uptake rates against predator (Crested horn sharks; Heterodontus galeatus) and non-predator (Sand whiting; Sillago ciliata) olfactory cues with respect to their developmental stages. In earlier developmental stages, the embryos reduced oxygen uptake against non-predator odour cues but not against predator cues, indicating a cryptic response, possibly because of neophobia. At later developmental stages, the embryos increased oxygen uptake against predator cues and but not against non-predatory cues. This possibly indicates a fight-or-flight response where increased oxygen levels can contribute to enhanced capacity for aerobic or anaerobic activities that can aid in successful evasion by fleeing (15).\nCinnamon clownfish embryos (Amphiprion melanopus, Figure¬†7) are found to detect conspecific alarm cues and associate them with predator cues which resulted in increased heart rates (16). Increased heart rate is found to be positively correlated with anti-predator behaviours in fish.\n\nThese studies show that embryos can sense surrounding cues. The nature of surrounding cues also matters as some elicit innate behaviours and some don‚Äôt elicit any behaviour at all. For cues eliciting innate behaviours, some of the above-mentioned studies have shown that embryos can associate novel cues to these innately known cues and thereby change behaviour during the pre and/or post-embryonic stage.\n\n\n\n\n\n\nFigure¬†1: Image by Peter Paplanus from St.¬†Louis, Missouri - Ringed Salamander (Ambystoma annulatum)\n\n\n\n\n\n\nFigure¬†2: Image by Brian Gratwicke - Lithobates sylvaticus (Woodfrog). The wood frog is known as both Lithobates sylvaticus and Rana sylvatica\n\n\n\n\n\n\n\n\nFigure¬†3: Image by Diego Delso - Common cuttlefish (Sepia officinalis)\n\n\n\n\n\n\nFigure¬†4: Image by Mike Anderson - Female Rainbow trout (Oncorhynchus mykiss).\n\n\n\n\n\n\n\n\nFigure¬†5: Image by Zul M Rosle from Kuantan, Malaysia - Brownbanded bambooshark (Chiloscyllium punctatum) at the KLCC Aquaria.\n\n\n\n\n\n\nFigure¬†6: Image by Mark Norman/Museum Victoria - Port Jackson Shark (Heterodontus portusjacksoni) at Wilsons Promontory, Victoria.\n\n\n\n\n\n\n\n\nFigure¬†7: Image by Richard Ling - Red and Black Anemonefish (Amphiprion melanopus) in anemone (Entacmaea quadricolor). Steve‚Äôs Bommie, Ribbon Reefs, Great Barrier Reef 177094512."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#embryonic-response-to-predatory-cues",
    "href": "posts/research_article_embryonic_learning/index_post.html#embryonic-response-to-predatory-cues",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n4 Embryonic response to predatory cues",
    "text": "4 Embryonic response to predatory cues\nAs seen earlier, embryonic response to predators is evident and this might be because of strong predation pressures as most aquatic oviparous organisms are highly vulnerable to egg predators. While most studies have looked at the effect of embryonic response in juvenile and adult life stages, it might also have an immediate effect on the embryo itself [as seen in the Cuttlefish (11), in Bamboo shark (14), in Port Jackson shark (15) and Cinnamon clownfish (16)].\nSo meet fathead minnow fish (Pimephales promelas), who share the same fate of being embryonic prey as the above-mentioned ones. The Fathead minnow is a freshwater fish species native to North America. They live in a school of up to a hundred individuals (17) but during the onset of the breeding season. males depart from school and adopt a solitary lifestyle (18). Females of this species lay their eggs in sites guarded by a single male (19), which hatch into larvae after 5 days of post-fertilization. They are some of the well-studied model organisms which display prominent anti-predatory behaviours. They are perhaps best known for ‚ÄúSchreckstoff‚Äù (Paper is in German: 20), a chemical alarm signal, which is only produced when they suffer tissue damage, which occurs most often from predator attacks. Fathead minnow innately responds to these alarm cues and associates them with predator cues (21). With varying levels of these alarm cues, solitary individuals show a combination of anti-predatory behaviours that include, dashing, freezing, slowing and exploring (22). If chased by predators, they show an increased rate of shoaling3 and shooling4 and, also show increased shelter-seeking behaviour (23). Field studies have shown that they actively avoid areas containing heterospecific alarm cues (21). However, they are not reported to innately identify any predators (24).\nThis complex behaviour repertoire is only reported to be present in juveniles and adults (well no surprises there!). But fathead minnow starts their life cycle as eggs and faces high rates of predation (25). The embryonic stage is by far the worst period to get predated on as the embryos are heavily handicapped due to their immobility. Nevertheless, they are known to alter hatching times in response to predator cues (26). Moreover, when predator cues in combination with cues indicating embryo damage were introduced to embryos, the developed larvae from these embryos were small in size, indicating loss of developmental maturity (26). But in the presence of conspecific alarm cues, hatching times remain unchanged (27). Therefore these results suggest that fathead minnow embryos can sense environmental cues but clear-cut evidence was lacking. This is exactly what was explored by graduate student Christopher Crowder and Dr.¬†Jessica Ward at Ball State University, Indiana, USA. in their recent paper (1). The researchers asked whether fathead embryos can innately detect alarm cues just like their adult counterparts and if they do, then does that embryonic experience affect pre and/or post-embryonic behaviours?"
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#methods-and-results",
    "href": "posts/research_article_embryonic_learning/index_post.html#methods-and-results",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n5 Methods and results",
    "text": "5 Methods and results\nTo initiate embryonic experience, researchers grew the embryos in four different environments imparting ‚Äúpredator-risk‚Äù and ‚Äúnon-predator-risk‚Äù experiences.\n\n\nTable¬†1: Embryonic environment\n\n\n\n\n\nEnvironment\nCondition\n\n\n\nEnvironment 1\nEmbryos reared in a control condition (just water with no other cues)\n\n\nEnvironment 2\nEmbryos reared with predator cues\n\n\nEnvironment 3\nEmbryos reared with conspecific alarm cues\n\n\nEnvironment 4\nEmbryos reared with predator + conspecific alarm cues\n\n\n\n\nExcept for environments 1 and 2, all other environments indicated predator risk. Since the fatheaded minnow shows no innate behaviour against any predators, environment 2 (predator cue) should be similar to environment 1 (control). If associative learning occurs in fathead minnow embryos then the larvae which came from environment 4 should be able to associate the predator cue to the alarm cue, which can later be tested through behavioural assays. As adult fathead minnows are known to innately respond to conspecific alarm cues, environments 3 and 4 can be indicative of predation pressure and embryos should be able to detect them.\nAs fathead minnows show embryonic locomotor activity, it might make them conspicuous to egg predators. Interestingly, when locomotor activity was checked 5 days post fertilization, embryos from predator-risk environments (env. 3 and 4) showed lower motor activity, possibly indicating a cryptic response. This suggests that embryos can detect alarm cues like their adult counterparts.\nAfter testing for locomotor activity, the embryos were then grouped according to the environments they were raised in and were placed in separate control tanks devoid of alarm or predator cues. The embryos were allowed to hatch, and at 22 days post fertilization, the larvae were tested for predator avoidance behaviour and predator evasion behaviour. The idea was that larvae from predator-risk environments should show enhanced predator avoidance and evasion behaviours.\nTherefore, researchers tested the larvae individually on a test arena where swimming activity was checked before and after applying a stimulus. Four different stimuli were used which were synonymous with the environments. Here swimming activity was measured, which acted as a proxy to the ‚Äòfreezing‚Äô behaviour fathead minnow showed, which is a predator avoidance behaviour. Lower swimming activity corresponds to enhanced predator avoidance behaviour, as lower activity might make them more cryptic to predators.\nThe four stimuli that were used are;\n\n\nTable¬†2: Stimuli administered\n\nStimulus\nCondition\n\n\n\nStimulus 1\nControl (absence of predator or alarm cues)\n\n\nStimulus 2\nPredator cue\n\n\nStimulus 3\nConspecific alarm cue\n\n\nStimulus 4\nPredator + conspecific alarm cues\n\n\n\n\nLarvae from the predator risk environments (env. 3 and 4) should show enhanced freezing behaviour when predator risk stimuli (stimuli 3 and 4) are presented, compared to the control stimulus. Stimulus 2 should also elicit enhanced freezing behaviour if the embryo was able to associate the predator cue with the alarm cue (i.e.¬†associative learning occurred in embryos indicating evidence for embryonic learning)\nResearchers found that before stimulus use, larvae from predator-risk environments (env. 3 and 4) had decreased swimming activity. This is in conjunction with earlier mentioned studies, where it was shown that embryos which experience predator indicative cues showed an overall decrease in activity in post-embryonic stages. Upon stimulus usage, the same trend followed, larvae from predator-risk environments had an overall decreased swimming activity for all stimuli administration instances. Both these results indicate that the larvae from predator risk environments (env. 3 and 4) showed enhanced predator avoidance behaviour compared to larvae from non-predator risk environments (env. 1 and 2). In the case of larvae from environment 4 (predator + alarm cues), when the predator cue was administered, it showed reduced swimming activity compared to the control. But the difference was not statistically significant. The level of significance for this study was chosen to be \\alpha = 0.05 and the p-value for the difference between swimming activity when predator cue stimulus was used compared to control stimulus usage was 0.05 (same as the \\alpha value), as reported in the paper. But that does not rule out its biological significance and the researchers assert that associative learning has occurred. For larvae from environment 1 (control), except for the control stimulus, all other stimuli administration resulted in reduced swimming activity. This was a strange finding and the researchers suggested that it might be because of neophobia.\nThe next anti-predatory behaviour researchers looked at was predator evasion behaviour. As mentioned before, researchers hypothesized that larvae from predator-risk environments (env. 3 and 4) should show enhanced evasion behaviours. To quantify evasion behaviour, two parameters were measured; maximal body curvature and latency to respond to simulated predator attack. When a fish tries to escape from a predator, it bends its body into a ‚ÄúC‚Äù shape to generate momentum drag. The maximal body curvature is the angle formed when the ‚ÄúC‚Äù shape is formed in a fish. A shorter angle means that the body bend is greater indicating an enhanced escape behaviour. Overall, it was found that predator risk environments (env. 3 and 4) have resulted in reduced body curvature angle, indicating enhanced evasion behaviour in the larvae. But the evidence for associative learning was not present in this case, as for larvae from environment 4 (predator + alarm cues), upon predator cue administration, the maximal body curvature angle was not significantly different from the control. Latency to respond was found to be independent of embryonic environment and stimulus administration, as no change was observed between the groups. This indicates that post-embryonic behaviour can have varying levels of susceptibility to modification via embryonic learning."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#conclusion",
    "href": "posts/research_article_embryonic_learning/index_post.html#conclusion",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n6 Conclusion",
    "text": "6 Conclusion\nThe researchers showed, for the first time in fathead minnow embryos (Pimephales promelas), that they can detect conspecific alarm cues, moreover, they also showed that;\n\nIn addition to detecting conspecific alarm cues, the embryos, in response to its detection also modulated their embryonic locomotor activity to become cryptic, indicating an immediate adaptive response within the embryos\nEmbryonic experience overall enhanced predator avoidance and evasion behaviours\nEmbryonic associative learning only modulated predator avoidance and not evasion behaviour in the larvae, indicating a varying level of susceptibility to modification via embryonic learning\n\nThe evidence for associative learning in this study did not achieve statistical significance and was in a borderline area (p = 0.05, same as \\alpha = 0.05) but that does not mean there is no biological significance. As sample sizes were adequate, we can explore some possible future directions.\n\nThe predator cue used in this study came from a known egg predator of fathead minnow, the blue gill sunfish (Lepomis macrochirus). Using another predator cue in association with the conspecific alarm cue can mitigate any possible intrinsic effect the previous predator cue had in learning.\nAs fathead minnows were trained to associate predator cues to conspecific alarm cues throughout 5 days post fertilization as embryos and were only tested 22 days post fertilization as larvae. There was a significant time gap between training and testing which might have corroded its memory. But fathead minnows are known to form long-lasting associations between odour cues and other cues. In an earlier study (28), fathead minnow was shown to distinguish between familiar shoal mates and unfamiliar conspecifics, and they were able to maintain this discriminability even after 2 months of separation suggesting evidence for a long-lasting memory. I did not find any other studies which have exclusively looked at learning and memory in fathead minnow. An interesting experiment to follow up would be, training fathead minnow embryos to associate a predator cue to the conspecific alarm and testing this association at different ages after hatching. This can again check if embryonic associative learning occurred? if it occurred then how does the memory fare with increasing time interval between last association and larval age.\n\nIn conclusion, this study (1) showcased a whole new aspect of learning in fathead minnow (Pimephales promelas) which opens up new avenues of research questions given their peculiar behaviour and life history."
  },
  {
    "objectID": "posts/welcome_to_blog/index.html",
    "href": "posts/welcome_to_blog/index.html",
    "title": "The story behind one carat blog",
    "section": "",
    "text": "Cover photo of my old blog made using distill\n\n\n\n\n\n\nCover photo of my new blog made using quarto"
  },
  {
    "objectID": "posts/welcome_to_blog/index.html#making-of-one-carat-blog",
    "href": "posts/welcome_to_blog/index.html#making-of-one-carat-blog",
    "title": "The story behind one carat blog",
    "section": "\n2 Making of one carat blog",
    "text": "2 Making of one carat blog\nMy name is Jewel Johnson and I am the creator of this blog. The story about this blog started with my wish to have a writing space, to showcase the things that I learned. I find that having a blog to write, instills self-discipline in learning new things. So I initially began looking for ways to build a blog. I read many online articles which talked about using WordPress, Blogger, Weebly, Wix and so on. But almost all of these services were paid and I was not able to afford them. I began looking for free alternatives and I came to know about Medium. Now Medium is not precisely a blog but a publishing platform and it restricts readers behind a paywall if they want to read your posts. But you do get paid if anyone reads your articles. So I tried writing a few articles on R programming. Medium supports code blocks but it was extremely difficult to use them and it was never tailored for writing R codes. As I was interested in sharing some of the things I learned about R, I wanted to see what all articles were written about R programming on Medium. So I read some of them in Towards data science, which is part of Medium but specialized in articles related to data science. There were some good articles but to my surprise, most articles were downright copy-pasting what was there in the package tutorials or their related source webpages. And the articles were made with very low effort. The only thing I found nice was the stock images. So I ditched Medium for these reasons and went on to see if I can make a blog using purely R. And that‚Äôs when I came to know about the {Rmarkdown} package in R and Github pages. So long story short, I learned how to use this package and how to host a webpage for free on Github. And this led to the creation of my first blog. It was very basic and visually simple, but I was proud of it. So I began writing tutorials on data visualization using R and wrote my first article about the {ggplot2} package in R. I shared it with my friends and received good support from them. Then I wanted to improve my blog and I began searching for new ways to make the blog better, which led me to learn about the {distill} package in R.\nI was flabbergasted when I saw the blogs and websites made using the distill package. They were so much better than the ones made from {Rmarkdown}. I first learned about {distill} from Dr.¬†Ella Kaye‚Äôs blog post and Dr.¬†Lisa Lendway‚Äôs blog post. And after reading Prof.¬†Andreas Handel blog post on how to build a website using distill I was convinced to rebuild my blog. The YouTube video by Dr.¬†Lisa Lendway‚Äôs and the official tutorials given on the distill website also helped me very much. And the end product was this beautiful blog that I created. I even named my blog; 'one carat blog', a name resembling me.\nAnd all was well and I began concentrating on my blog. I went on to write a series of articles encompassing a complete tutorial on learning data visualization and data manipulation using R. It was well received and I got a lot of helpful comments about it. One thing that kind of annoyed me about the {distill} package was that the table of contents won‚Äôt float for long posts. This made the ‚Äòtable of contents‚Äô obsolete. Nevertheless, the package fulfilled most of what I wanted to have in a blog. This period was also the first time I began learning about HTML and CSS languages. Using that knowledge I began tweaking my blog and customizing it. I also wrote an article highlighting a few quality of life modifications one can implement in their distill blog and to my surprise, the post was adopted to the official distill website, which showcased useful tutorials related to distill blogging.\n\n\n\nGreat to see you‚Äôve added your blog to the distillery. You should also consider adding your excellent ‚ÄúQuality of life modifications for your distill websites‚Äù post to the Tips & Tricks section of the site https://t.co/SMOtCU0eM1\n\n‚Äî Ella Kaye (@ellamkaye) December 20, 2021\n\nI also received back from Dr.¬†Ella Kaye‚Äôs in Twitter and I was added to the distill club! I was very happy at that time.\n\n\n\nThis is fab! Welcome to #DistillClub pic.twitter.com/4jpeBe7Kis\n\n‚Äî Ella Kaye (@ellamkaye) December 20, 2021\n\nThis was also the time when I changed the About me page. Everyone who was having a distill blog was using a default template provided by the {postcards} package in R. But I wanted to have a unique ‚Äòabout me‚Äô page, which acted like my CV. So I used the modern resume theme by James Grant and made this cool looking CV.\nThen for about four months, I had the worst time in my life. I was feeling depressed and sick. Many things had happened and I could not do any blogging or anything for that matter. And then finally after sorting out my affairs, I went back to blogging. I opened Rstudio one day and I got a blank window. This was the first time I was seeing this bug, so when I searched about it, I learned that a new package for R called {quarto} was causing some issues with Rstudio. To my knowledge, it only affected Linux users. Then I searched about what is {quarto} package is about and I struck a gold mine.\nIt was everything {distill} had, but better. The best thing was that it natively supported a floating table of contents, something which was missing in {distill}. My first introduction to quarto came from Dr.¬†Alison Hill‚Äôs blog post where she gave a brief intro on what the quarto package is about. After reading the post I was in a dilemma as to whether I should migrate my blog to quarto or not. Then I found that most of the blogs featured on the distill official website had migrated to quarto. It was the new trend in town, and everyone was adapting it. So I finally made up my mind and decided to migrate, again, for the second time and it led to a few sleepless nights that I am not very proud of. The post from Dr.¬†Danielle Navarro was also very helpful and made the transition process much easier.\nSo with help of all these articles and many glasses of homemade wine, I finally migrated from {distill} to {quarto} and made this blog which you are browsing now. I had to convert all my .Rmd files to the new .qmd file that quarto supports. The YAML headers were a pain in the beep to change. But in the end, it was worth it. I changed the previous visitor counter to a new flag counter for this blog. I also designed a new icon and a new cover image. And as of writing this blog post, I also wrote a series of articles focusing on beginner-level statistics in my blog. And recently I have been working on a software called Notion and have made some cool productivity templates focusing on PhD students. I also have written some blog posts showcasing those templates, so make sure to check them out.\nTo conclude I hitchhiked from Rmarkdown to distill and now to quarto, the end product is this beautifully made blog that you are browsing. I hope you enjoy this blog and find the posts and tutorials helpful. If you find them useful, make sure to share them. If you have any suggestions or feedback for this blog, then please comment them in the comment section at the end of the blog. Thanks for visiting my blog and I hope you have a great time."
  },
  {
    "objectID": "posts/welcome_to_blog/index.html#references",
    "href": "posts/welcome_to_blog/index.html#references",
    "title": "The story behind one carat blog",
    "section": "References",
    "text": "References\nUseful Distill articles\n\nKaye (2021, May 8). ELLA KAYE: Welcome to my {distill} website!. Retrieved from https://ellakaye.rbind.io/posts/2021-05-08-welcome-to-my-distill-website/\nLendway (2020, Dec.¬†18). Lisa Lendway: Building a {distill} website. Retrieved from https://lisalendway.netlify.app/posts/2020-12-09-buildingdistill/\nCreate a GitHub website with distill in less than 30 minutes by Prof.¬†Andreas Handel.\nOfficial guide on creating a website using distill.\nBuilding a website using R {distill}. Youtube video by Dr.¬†Lisa Lendway.\n\nUseful Quarto articles\n\nWe don‚Äôt talk about Quarto by Dr.¬†Alison Hill.\nDanielle Navarro. 2022. ‚ÄúPorting a Distill Blog to Quarto.‚Äù April 20, 2022. https://blog.djnavarro.net/posts/2022-04-20_porting-to-quarto.\nCreating a blog with Quarto in 10 steps by Beatriz Milz\nCool quarto tips and tricks. Tweet by Albert Rapp\nOfficial guide by developers of Quarto"
  },
  {
    "objectID": "posts/word_happy_2022/index.html",
    "href": "posts/word_happy_2022/index.html",
    "title": "The World Happiness Report 2022",
    "section": "",
    "text": "World Happiness Report 2022 shows which are the happiest countries for the year 2022. By statistically analysing six key parameters, each country is given a score (which is called happiness score within the dataset). The higher the score, the happier the country is and vice versa. The six key parameters which are taken into analysis for determining the score are;\n\nGross domestic product per capita\nSocial support\nHealthy life expectancy\nFreedom to make your own life choices\nGenerosity of the general population\nPerceptions of internal and external corruption levels.\n\nFinland is ranked first among 149 countries with an overall score of 7.82. Despite COVID 19 wrecking havoc around the world, citizens of Finland have persevered through it and they have been maintaining first rank since 2016. Afghanistan is at the lowest rank with a score of 2.40. With complications from COVID 19 pandemic and the Taliban take over, Afghanistan is going through one of the worst humanitarian crisis in human history and the ranking reflects it."
  },
  {
    "objectID": "posts/word_happy_2022/index.html#getting-the-data",
    "href": "posts/word_happy_2022/index.html#getting-the-data",
    "title": "The World Happiness Report 2022",
    "section": "\n2 Getting the data",
    "text": "2 Getting the data\nIn this post we will do some exploratory data visualizations using data from The World Happiness Report 2022. You can download the .csv file from here."
  },
  {
    "objectID": "posts/word_happy_2022/index.html#plotting-a-world-map",
    "href": "posts/word_happy_2022/index.html#plotting-a-world-map",
    "title": "The World Happiness Report 2022",
    "section": "\n3 Plotting a world map",
    "text": "3 Plotting a world map\nWe will plot a world map with a scalable colour palette based on the ladder score where greater scores indicated happier countries and vice versa.\nIn short what we we will be doing is, we are going to join the World Happiness Report 2021 dataset with the map data and plot it using the ggplot2 package. The map_data() function helps us easily turn data from the {maps} package in to a data frame suitable for plotting with ggplot2.\n\n# required packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(viridis)\n\n# load the dataset which you have downloaded\n# please change the location to where your downloaded file is kept\nhap_pre <- read.csv(\"datasets/2022.csv\")\n\n# renaming column names of ease of use\ncolnames(hap_pre)[2] <- \"country\"\ncolnames(hap_pre)[3] <- \"score\"\nhap_pre <- hap_pre[-147,]\n\n# the score values are separated by commas\n# let us change that to dots\nhap_pre$score <- scan(text=hap_pre$score, dec=\",\", sep=\".\")\n\n# selecting country and score columns\nhap <- hap_pre %>% select(country,score)\n\n# removing special characters in df\nhap$country <- gsub(\"[[:punct:]]\",\"\",as.character(hap$country))\n\n# loading map\nmap_world <- map_data('world')\n# remove Antarctica\nmap_world <- map_world[!map_world$region ==\"Antarctica\",]\n\n# checking which country names are a mismatch between map data and the downloaded dataset\nanti_join(hap, map_world,  by = c(\"country\" = \"region\"))\n\n\n\n  \n\n\n\nYou can see that some country names are a mismatch with the map data. So we will attempt to fix it.\n\n# display all country names in the dataset\n# useful to locate correct country names\n#map_world %>% group_by(region) %>% summarise() %>% print(n = Inf)\n\n# correcting country names\n# here we are matching the country names of downloaded dataset with the map data\ncorrect_names <- c(\"United States\" = \"USA\",\n                   \"United Kingdom\" = \"UK\",\n                   \"Czechia\" = \"Czech Republic\",\n                   \"Taiwan Province of China\"  = \"Taiwan\",\n                   \"North Cyprus\"= \"Cyprus\",\n                   \"Congo\"= \"Republic of Congo\",\n                   \"Palestinian Territories\" = \"Palestine\",\n                   \"Eswatini Kingdom of\" = \"Swaziland\")\n\n# recoding country names \nhap2 <- hap %>% mutate(country = recode(country, !!!correct_names))\n\n# joining map and the data\nworld_hap <- left_join(map_world, hap2, by = c(\"region\" = \"country\"))\n\n# creating a function to add line in text, for the caption\naddline_format <- function(x,...){\n  gsub(',','\\n',x)}\n\n# plotting the world map\nggplot(world_hap, aes(long, lat)) + geom_polygon(aes(fill = score, group = group)) +\n  scale_fill_viridis(option = \"viridis\") + theme_void() +\n  theme(plot.background = element_rect(fill = \"aliceblue\"),\n        legend.position=\"bottom\") + \n  labs(title = \"Happiness scores of countries in 2022\",\n       subtitle = addline_format(\"Higher scores indicate happier countries and vice versa,Grey colour represents countries with no data\"),\n       fill = \"Score\",\n       caption = addline_format(\"Source: World Happiness Report 2022,Visualization by Jewel Johnson\"))"
  },
  {
    "objectID": "posts/word_happy_2022/index.html#plotting-an-interactive-world-map",
    "href": "posts/word_happy_2022/index.html#plotting-an-interactive-world-map",
    "title": "The World Happiness Report 2022",
    "section": "\n4 Plotting an interactive world map",
    "text": "4 Plotting an interactive world map\nLets plot an interactive world map with happiness score as a variable, where greater scores indicates happier countries and vice versa. We will be using the leaflet package in R for plotting the world map.\nWe have to download the world map data which comes as a .shp file.\nRun the codes given below to download the .shp file and load the .csv file required to plot the map. For plotting the interactive map, we will be using the sf package for reading the .shp file and the leaflet package for plotting the map.\n\n# loading the .csv file which was downloaded\n# please change the location to where your .csv file is kept\nhap_pre <- read.csv(\"datasets/2022.csv\")\n\n# renaming column names of ease of use\ncolnames(hap_pre)[1] <- \"rank\"\ncolnames(hap_pre)[2] <- \"country\"\ncolnames(hap_pre)[3] <- \"score\"\nhap_pre <- hap_pre[-147,]\n\n# the score values are separated by commas\n# let us change that to dots\nhap_pre$score <- scan(text=hap_pre$score, dec=\",\", sep=\".\")\n\n# selecting country and score columns\nhap <- hap_pre %>% select(rank,country,score)\n\n# removing special characters in df\nhap$country <- gsub(\"[[:punct:]]\",\"\",as.character(hap$country))\n\nNow that we have the dataset ready, let us download the .shp file which contains the world map.\n\n# downloading and loading the .shp file\n# please change the 'destfile' location to where your zip file is located\ndownload.file(\"http://thematicmapping.org/downloads/TM_WORLD_BORDERS_SIMPL-0.3.zip\",\n              destfile=\"shp/world_shape_file.zip\")\n\n# unzip the file into a directory. You can do it with R (as below).\n# the directory of my choice was folder named 'shp'\nunzip(\"shp/world_shape_file.zip\", exdir = \"shp/\")\n\nNow let us load the .shp file and plot the map.\n\n# Read the shape file with 'sf'\n#install.packages(\"sf\")\nlibrary(sf)\n\nworld_spdf <- st_read(paste0(getwd(),\"/shp/TM_WORLD_BORDERS_SIMPL-0.3.shp\"), stringsAsFactors = FALSE)\n\nReading layer `TM_WORLD_BORDERS_SIMPL-0.3' from data source \n  `C:\\Work\\Github\\one-carat-blog\\posts\\word_happy_2022\\shp\\TM_WORLD_BORDERS_SIMPL-0.3.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 246 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.57027\nGeodetic CRS:  WGS 84\n\n# checking which country names are a mismatch between map data and the downloaded dataset\n# this is an important check as we have to join the happiness dataset and .shp file with country names\nanti_join(hap, world_spdf,  by = c(\"country\" = \"NAME\"))\n\n\n\n  \n\n\n# correcting country names, note that some countries are not available in the .shp file\n\ncorrect_names <- c(\"Czechia\" = \"Czech Republic\",\n                   \"Taiwan Province of China\" = \"Taiwan\",\n                   \"South Korea\" = \"Korea, Republic of\",\n                   \"Moldova\"  = \"Republic of Moldova\",\n                   \"Belarus*\" = \"Belarus\",\n                   \"Vietnam\" = \"Viet Nam\",\n                   \"Hong Kong SAR of China\"= \"Hong Kong\",\n                   \"Libya\" = \"Libyan Arab Jamahiriya\",\n                   \"Ivory Coast\" = \"Cote d'Ivoire\",\n                   \"North Macedonia\" = \"The former Yugoslav Republic of Macedonia\",\n                   \"Laos\" = \"Lao People's Democratic Republic\",\n                   \"Iran\" = \"Iran (Islamic Republic of)\",\n                   \"Palestinian Territories*\" = \"Palestine\",\n                   \"Eswatini, Kingdom of*\" = \"Swaziland\",\n                   \"Myanmar\" = \"Burma\",\n                   \"Tanzania\" = \"United Republic of Tanzania\")\n\n# recoding country names \nhap2 <- hap %>% mutate(country = recode(country, !!!correct_names))\n\n# joining .shp file and the happiness data\nworld_hap <-  left_join(world_spdf, hap2, by = c(\"NAME\" = \"country\"))\n\n#install.packages(\"leaflet\")\nlibrary(leaflet)\nlibrary(viridis)\n\n# making colour palette for filling\nfill_col <- colorNumeric(palette=\"magma\", domain=world_hap$score, na.color=\"transparent\")\n\n# Prepare the text for tooltips:\ntext <- paste(\n  \"Country: \", world_hap$NAME,\"<br/>\", \n  \"Score: \", world_hap$score, \"<br/>\", \n  \"Rank: \", world_hap$rank, \n  sep=\"\") %>%\n  lapply(htmltools::HTML)\n\n# plotting interactive map\nleaflet(world_hap) %>% \n  addTiles()  %>% \n  setView( lat=10, lng=0 , zoom=2) %>%\n  addPolygons( \n    fillColor = ~fill_col(score), \n    stroke=TRUE, \n    fillOpacity = 0.9, \n    color= \"grey\", \n    weight=0.3,\n    label = text,\n    labelOptions = labelOptions( \n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"), \n      textsize = \"13px\", \n      direction = \"auto\"\n    )\n  ) %>%\n  addLegend( pal=fill_col, values=~score, opacity=0.7, title = \"Score\", position = \"bottomleft\" )"
  },
  {
    "objectID": "posts/word_happy_2022/index.html#summary",
    "href": "posts/word_happy_2022/index.html#summary",
    "title": "The World Happiness Report 2022",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope this post was helpful to you in understanding how to plot world maps in R. In short using ggplot2 we have first plot a static world map using the data from The World Happiness Report 2022, then similarly using the leaflet package we plotted an interactive world map."
  },
  {
    "objectID": "posts/word_happy_2022/index.html#references",
    "href": "posts/word_happy_2022/index.html#references",
    "title": "The World Happiness Report 2022",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nJoe Cheng, Bhaskar Karambelkar and Yihui Xie (2021). leaflet: Create Interactive Web Maps with the JavaScript ‚ÄòLeaflet‚Äô Library. R package version 2.0.4.1. https://CRAN.R-project.org/package=leaflet\nPebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009\nTutorial on plotting interactive maps in R.\nThe World Happiness Report\nSource for .csv file of World Happiness Score of countries 2022. Compiled by Mathurin Ach√© in Kaggle.com"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html",
    "href": "tutorials/data_man/dplyr_1.html",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "",
    "text": "The dplyr package is a grammar of data manipulation just like how ggplot2 is the grammar of data visualization. It helps us to apply a wide variety of functions such as;\n\nSummarising the dataset\nApplying selections and orderings as a function of a variable\nCreating new variables as a function of existing variables\n\nWe will see in-depth how to manipulate our data like a boss!"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#the-pipe-operator",
    "href": "tutorials/data_man/dplyr_1.html#the-pipe-operator",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n2 The pipe operator %>%",
    "text": "2 The pipe operator %>%\nPerhaps the most amazing thing in making codes short and efficient is the pipe operator which is originally from the magrittr package which is made available for the dplyr package. The pipe operator helps you skip the intermediate steps of saving an object before you can use them in command. It does so by ‚Äòpiping‚Äô together results from the first object to the function ahead of the pipe operator. The command x %>% y %>% z can be read as ‚Äòtake the result of x and use it with function y and take that result and use it with function z‚Äô. This is the gist of what the pipe operator does. Allow me to demonstrate.\n\nlibrary(ggplot2)\n\n# dummy data\na <- c(sample(1:100, size = 50))\nb <- c(sample(1:100, size = 50))\ndata <- as.data.frame(cbind(a,b))\n\n# without %>%\ndata <- mutate(data, ab = a*b, twice_a = 2*a)\ndata_new <- filter(data, ab < 300, twice_a < 200)\nggplot(data_new, aes(ab, twice_a)) + geom_point()\n\n# with %>%\ndata %>% mutate(ab = a*b, twice_a = 2*a) %>% \n  filter(ab < 300, twice_a < 200) %>%\n  ggplot(aes(ab, twice_a)) + geom_point()\n\nAs you can see, with pipe operator %>%, we did not have to save any objects in the intermediate steps and also it improved the overall clarity of the code. I have used a few commands from the dplyr package in the example given above. So without further ado let us delve into the dplyr package. For this chapter, I will be using the penguin dataset from the popular {palmerpenguin} package as an example.\n\n# install palmerpenguins package\ninstall.packages(\"palmerpenguins\")\nlibrary(dplyr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#grouping-the-data",
    "href": "tutorials/data_man/dplyr_1.html#grouping-the-data",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n3 Grouping the data",
    "text": "3 Grouping the data\n\n3.1 group_by()\nThe command group_by() allows us to group the data via existing variables. It allows for a ‚Äòsplit-apply-combine‚Äô way of getting output. First, it will split the data or group the data with the levels in the variable, then apply the function of our choice and then finally combine the results to give us a tabular output. On its own the command doesn‚Äôt do anything, we use it in conjunction with other commands to get results based on the grouping we specify. The command ungroup() is used to ungroup the data."
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#summarising-the-data",
    "href": "tutorials/data_man/dplyr_1.html#summarising-the-data",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n4 Summarising the data",
    "text": "4 Summarising the data\n\n4.1 summarise()\nThe summarise() command allows you to get the summary statistics of a variable or a column in the dataset. The result is given as tabular data. Many types of summary statistics can be obtained using the summarise() function. Some of them are given below. To calculate average values it is necessary to drop NA values from the dataset. Use drop_na() command from the tidyr package. The comments denote what each summary statistic is.\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nsummary_data <- penguins %>% drop_na() %>%\n  group_by(species) %>% # we are grouping/splitting the data according to species\n  summarise(avg_mass = mean(body_mass_g), # mean mass\n            median_mass = median(body_mass_g), # median mass\n            max_mass = max(body_mass_g), # max value of mass, can also use min()\n            standard_deviation_bill_length = sd(bill_length_mm), # standard deviation of bill_length\n            sum_mass = sum(flipper_length_mm), # sum\n            distinct_years = n_distinct(year), # distinct values in column year\n            no_of_non_NAs = sum(!is.na(year)), # gives no of non NAs, \n            length_rows = n(), # length of the rows\n            iqr_mass = IQR(body_mass_g), # inter quartile range of mass\n            median_absolute_deviation_mass = mad(body_mass_g), # median absolute deviation of mass\n            variance_mass = var(body_mass_g)) # variance\n# viewing summary as a table\nsummary_data\n\n\n\n  \n\n\n\nThe number of non NA values will be the same as that of n() result as we have used drop_na()command in the beginning.\nThe base function summary() in R also gives the whole summary statistics of a dataset.\n\nlibrary(palmerpenguins)\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\n4.2 when to use group_by()\nIt can be confusing to decide when to use the group_by() function. In short, you should use it whenever you want any function to act separately on different groups present in the dataset. Here is a graphical representation of how the summarise() function is used to calculate the mean values of a dataset. When used with group_by() it calculates mean values for the respective groups in the data, but when group_by() is not used, it will calculate the mean value of the entire dataset irrespective of the different groups present and outputs a single column.\n\n\n\n\n\n\n4.3 count()\nThe count() command is used to count the number of rows of a variable. Has the same function as that of n()\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\ncount <- penguins %>% group_by(species) %>%\n  count(island)\n\n# viewing count as a table\ncount"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#manipulating-cases-or-observations",
    "href": "tutorials/data_man/dplyr_1.html#manipulating-cases-or-observations",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n5 Manipulating cases or observations",
    "text": "5 Manipulating cases or observations\nThe following functions affect rows to give a subset of rows in a new table as output.\n\n5.1 filter()\nUse filter() to filter rows corresponding to a given logical criteria\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% filter(body_mass_g < 3000)\n\n\n\n  \n\n\n\n\n5.2 distinct()\nUse distinct() to remove rows with duplicate or same values.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% group_by(species) %>% distinct(body_mass_g)\n\n\n\n  \n\n\n\n\n5.3 slice()\nUse slice() to select rows by position.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# slice from first row to fifth row\npenguins %>% slice(1:5) \n\n\n\n  \n\n\n\n\n5.4 slice_sample()\nUse slice_sample() to randomly select rows from the dataset. Instead of (n = ) you can also provide the proportion value (between 0 and 1) using (prop = ). For e.g.¬†for a dataset with 10 rows, giving (prop = 0.5) will randomly sample 5 rows. Other related functions include;\n\n\npreserve : Values include TRUE to preserve grouping in a grouped dataset and FALSE to not preserve grouping while sampling.\n\nweight_by : Gives priority to a particular variable during sampling. An example is given below.\n\nreplace : Values include TRUE if you want sampling with replacement which can result in duplicate values, FALSE if you want sampling without replacement.\n\n\n\n(n = 4)\nweight_by\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# samples 4 rows randomly\npenguins %>% slice_sample(n = 4)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# sampling will favour rows with higher values of 'body_mass_g'\npenguins %>% drop_na() %>% slice_sample(n = 4, weight_by = body_mass_g)\n\n\n\n  \n\n\n\n\n\n\n\n5.5 slice_min() and slice_max()\nUse slice_min() to extract rows containing least values and use slice_max() to extract rows with greatest values. The function with_ties = FALSE is included to avoid tie values.\n\n\nslice_min()\nslice_max()\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_min(body_mass_g, n = 4, with_ties = FALSE)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_max(body_mass_g, n = 4,with_ties = FALSE)\n\n\n\n  \n\n\n\n\n\n\n\n5.6 slice_head() and slice_tail()\nUse slice_head() to extract first set of rows and use slice_tail() to extract last set of rows.\n\n\nslice_head()\nslice_tail()\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_head(n = 4)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_tail(n = 4)\n\n\n\n  \n\n\n\n\n\n\n\n5.7 arrange()\nUse arrange() to arrange rows in a particular order.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# arranging rows in descending order of bill length\n# by default it arranges data by ascending order when no specifications are given\npenguins %>% arrange(desc(bill_length_mm))\n\n\n\n  \n\n\n\n\n5.8 add_row()\nUse add_row() to add rows to the dataset.\n\nlibrary(dplyr)\n\nName <- c(\"a\", \"b\")\nAge <- c(12,13)\ndata.frame(Name, Age) %>% add_row(Name = \"c\", Age = 15)"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#manipulating-variables-or-columns",
    "href": "tutorials/data_man/dplyr_1.html#manipulating-variables-or-columns",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n6 Manipulating variables or columns",
    "text": "6 Manipulating variables or columns\nThe following functions affect columns to give a subset of columns in a new table as output.\n\n6.1 pull()\nUse pull() to extract columns as a vector, by name or index. Only the first 10 results are shown for easy viewing.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% pull(body_mass_g)\n\n\n6.2 select()\nUse select() to extract columns as tables, by name or index.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% select(species, body_mass_g)\n\n\n\n  \n\n\n\n\n6.3 relocate()\nUse relocate() to move columns to new position. Results are not shown as these are trivial results.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# relocates 'species' column to last position\npenguins %>% relocate(species, .after = last_col())\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# relocates 'species' column before column 'year' and renames the column as 'penguins'\npenguins %>% relocate(penguins = species, .before = year)\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# you can also relocate columns based on their class\n# relocates all columns with 'character' class to last position\npenguins %>% relocate(where(is.character), .after = last_col())\n\n\n6.4 rename()\nUse rename() function to rename column names in the dataset.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# renames the column sex to gender\npenguins %>% rename(gender = sex)\n\n\n6.5 mutate()\nUse mutate() function to create new columns or variables.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% drop_na() %>% \n  group_by(species) %>%\n  mutate(mean_mass = mean(body_mass_g))\n\n\n\n  \n\n\n\n\n6.6 transmute()\nDoes the same function as mutate() but in the process will drop any other columns and give you a table with only the newly created columns.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% drop_na() %>% \n  group_by(species) %>%\n  transmute(mean_mass = mean(body_mass_g))\n\n\n\n  \n\n\n\n\n6.7 across()\nUse across() to summarise or mutate columns in the same way. First example shows across() used with summarise() function.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# summarise across columns body mass, bill length and bill depth\n# and calculate the mean values\n# since we are calculating mean values,\n# NAs are dropped using 'drop_na() function from 'tidyr' package\n\npenguins %>% drop_na() %>%\n  group_by(species) %>%\n  summarise(across(c(body_mass_g, bill_length_mm, bill_depth_mm), mean))\n\n\n\n  \n\n\n\nSecond example showing across() used with mutate() function. We can efficiently create new columns using mutate() and across() together. Suppose we want to multiply all numerical values in a dataset with 2 and create new columns of those values. This can be done using the code below.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# define the function\ntwo_times <- function(x) {\n  2*x\n} \n\n# .name will rename the new columns with 'twice` prefix combined with existing col names\npenguins %>% group_by(species) %>%\n  mutate(across(where(is.numeric), two_times, .names = \"two_times_{col}\"))\n\n\n\n  \n\n\n\nThe same code when used just with mutate() function will look like this\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# define the function\ntwo_times <- function(x) {\n  2*x\n}\n\n# using only 'mutate()' function\npenguins %>% group_by(species) %>%\n  mutate(twice_bill_lenght = two_times(bill_length_mm),\n         twice_body_mass = two_times(body_mass_g),\n         .....)\n\nSo in this code, I will have to manually type all the col names and apply the operation individually which is too much of a hassle. Now we can better appreciate how efficient it is in using mutate() and across() functions together.\n\n6.8 c_across()\nThe function c_across() is similar to the earlier mentioned across() function. But instead of doing a column-wise function, it applies function across columns in a row-wise manner. Now, most functions in R by default computes across columns, so to specify row-wise computation, we have to explicitly use the function rowwise() in conjunction with other functions. In the example below we will sum both bill and flipper lengths of the penguins in the penguins dataset and create a new column called ‚Äòsum_of_lengths‚Äô.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% drop_na() %>%\n  group_by(species) %>%\n  rowwise() %>%\n  transmute(sum_of_length = sum(c_across(c(bill_length_mm,flipper_length_mm))))"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#summary",
    "href": "tutorials/data_man/dplyr_1.html#summary",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n7 Summary",
    "text": "7 Summary\nThe dplyr package is the grammar of the data manipulation in R. It features well-made functions to help us summarise the data, group data by variables and manipulate columns and rows in the dataset. In this chapter, we learned in detail the different functions that help us manipulate data efficiently and have seen case examples also. In the next chapter, we will see the remaining set of functions in the dplyr package."
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#references",
    "href": "tutorials/data_man/dplyr_1.html#references",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n8 References",
    "text": "8 References\n\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr Here is the link to the cheat sheet explaining each function in dplyr.\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html",
    "href": "tutorials/data_man/dplyr_2.html",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "",
    "text": "In the previous chapter we have seen quite a lot of functions from the dplyr package. In this chapter, we will see the rest of the functions where we learn how to handle row names, how to join columns and rows and different set operations in the dplyr package.\n\n# loading necessary packages\nlibrary(dplyr)\n\n\nTidy data does not use row names. So use rownames_to_column() command to convert row names to a new column to the data. The function column_to_rownames() does the exact opposite of rownames_to_column() as it converts a column into rownames but make sure that the column you are converting into rownames does not contain NA values.\n\n# mtcars dataset contains rownames\n# creates new column called car_names which contains row names\nmtcars %>% rownames_to_column(var = \"car_names\")\n\n# returns the original mtcars dataset\nmtcars %>% rownames_to_column(var = \"car_names\") %>%\n  column_to_rownames(var = \"car_names\")"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#combine-tablescolumns",
    "href": "tutorials/data_man/dplyr_2.html#combine-tablescolumns",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n2 Combine tables/columns",
    "text": "2 Combine tables/columns\n\n2.1 bind_cols()\nJoins columns with other columns. Similar function as that of cbind() from base R.\n\ndf1 <- tidytable::data.table(x = letters[1:5], y = c(1:5))\ndf2 <- tidytable::data.table(x = letters[3:7], y = c(6:10))\nbind_cols(df1,df2)\n\n#similar functionality\ncbind(df1,df2)\n\n\n2.2 bind_rows()\nJoins rows with other rows. Similar function as that of rbind() from base R.\n\ndf1 <- tidytable::data.table(x = letters[1:5], y = c(1:5))\ndf2 <- tidytable::data.table(x = letters[3:7], y = c(6:10))\nbind_rows(df1,df2)\n\n#similar functionality\nrbind(df1,df2)\n\nThe functions that are described below have the same functionality as that of bind_cols() but give you control over how the columns are joined."
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#mutating-joins-and-filtering-joins",
    "href": "tutorials/data_man/dplyr_2.html#mutating-joins-and-filtering-joins",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n3 Mutating joins and filtering joins",
    "text": "3 Mutating joins and filtering joins\nMutating joins include left_join(), right_join(), inner_join() and full_join() and filtering joins include semi_join() and anti_join().\n\n\nleft_join()\nright_join()\ninner_join()\nfull_join()\nanti_join()\nsemi_join()\n\n\n\nIn the code below, matching variables of df2 are joined with df1. In the final data, you can see that only kevin and sam from df2 are matched with df1, and only those row values are joined with df1. For those variables which didn‚Äôt get a match, the row values for those are filled with NA. You can interpret the variables with NA values as; both john and chris are not present in df2.\nIf you are familiar with set theory in mathematics, what we are doing essentially is similar to (df1 \\cap df2) \\cup df1.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% left_join(df2)\n\n\n\n  \n\n\n\n\n\nSimilar to left_join() but here, you will be joining matching values from df1 to df2, the opposite of what we did earlier. As you can see only kevin and sam from the df1 is matched with df2, and only those row values are joined with df2. For the variables which didn‚Äôt get a match, the row values for those are filled with NA. You can interpret the variables with NA values as; bob is not present in df1.\nThis function, in the manner used here, is similar to (df1 \\cap df2) \\cup df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% right_join(df2)\n\n\n\n  \n\n\n\n\n\nThe function inner_join() compares both df1 and df2 variables and only joins rows with the same variables. Here only kevin and sam are common in both the dataframes so the row values of only those columns are joined and others are omitted.\nThis function is similar to df1 \\cap df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% inner_join(df2)\n\n\n\n  \n\n\n\n\n\nThe function full_join() compares both df1 and df2 variables and joins all possible matches while retaining both mistakes in df1 and df2 with NA values.\nThis function is similar to df1 \\cup df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% full_join(df2)\n\n\n\n  \n\n\n\n\n\nThis is an example of filtering join. The function anti_join() compares df1 variables to and df2 variables and only outputs those variables of df1 which didn‚Äôt get a match with df2.\nThis function, in the manner used here, is similar to df1 \\cap df2^c.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% anti_join(df2)\n\n\n\n  \n\n\n\n\n\nThis is an example of filtering join. The function semi_join() is similar to inner_join() but it only gives variables of df1 which has a match with df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\ndf1 %>% semi_join(df2)\n\n\n\n  \n\n\n\n\n\n\nHere is a nice graphical representation of the functions we just described now. Image source.\n\n\n\n\n\n(a) Mutating joins\n\n\n\n\n\n\n(b) Filtering joins\n\n\n\n\nFigure¬†1: Graphical abstract for joins. Image source: RPubs.com"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#additional-commands-for-joins",
    "href": "tutorials/data_man/dplyr_2.html#additional-commands-for-joins",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n4 Additional commands for joins",
    "text": "4 Additional commands for joins\nAdditionally, you can specify which common columns to match.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\n# match with column 'x'\ndf1 %>% left_join(df2, by = \"x\")\n\n\n\n  \n\n\ndf3 <- tidytable::data.table(a = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf4 <- tidytable::data.table(b = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\n# matching with column having different names, a and b in this case\ndf3 %>% left_join(df4, by = c(\"a\" = \"b\"))"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#set-operations",
    "href": "tutorials/data_man/dplyr_2.html#set-operations",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n5 Set operations",
    "text": "5 Set operations\nSimilar to the mutating join functions that we had seen, there are different functions related to set theory operations.\n\n5.1 intersect()\nOutputs common rows in the dataset.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nintersect(df1, df2)\n\n\n\n  \n\n\n\n\n5.2 setdiff()\nOutputs rows in first data frame but not in second data frame.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nsetdiff(df1, df2)\n\n\n\n  \n\n\n\n\n5.3 union()\nOutputs all the rows in both dataframes\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nunion(df1, df2)\n\n\n\n  \n\n\n\n\n5.4 setequal()\nChecks whether two datasets have same number of rows.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nsetequal(df1, df2)\n\n[1] FALSE"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#summary",
    "href": "tutorials/data_man/dplyr_2.html#summary",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n6 Summary",
    "text": "6 Summary\nIn this chapter, we have seen;\n\n\nHow to handle row names\nHow to combine columns and rows\nWhat are mutating and filtering joins and various set operations\n\n\nThus to conclude this chapter, we have now learned almost all functions in the dplyr package and have seen how to manipulate data efficiently. With the knowledge of the pipe operator that we have seen in chapter 1, we are now equipped to write codes compactly and more clearly. I hope this chapter was useful for you and I will see you next time."
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#references",
    "href": "tutorials/data_man/dplyr_2.html#references",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n7 References",
    "text": "7 References\n\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr. Here is the link to the cheat sheet explaining each and every function in dplyr."
  },
  {
    "objectID": "tutorials/data_man/tidyr.html",
    "href": "tutorials/data_man/tidyr.html",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "",
    "text": "Raw data might not be always in a usable form for any form of analysis or visualization process. The tidyr package aims to help you in reshaping your data in a usable form. In short, it helps you to ‚Äòtidy‚Äô up your data using various tools. In this chapter, we will see how you can use the tidyr package to make your data tidy."
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#what-is-tidy-data",
    "href": "tutorials/data_man/tidyr.html#what-is-tidy-data",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n2 What is tidy data?",
    "text": "2 What is tidy data?\nFirst, we need to understand what tidy data looks like. For that let us imagine a scenario where you are a doctor who is trying to find the best treatment for a disease. Now your colleagues have short-listed five different treatment methods and have reported their efficacy values when tested with five different patients. Now you are tasked with finding which of the five treatments is the best against the disease. You open your computer and you find the following data of the experiment.\n\n\n\n\n  \n\n\n\nThis is how often data is stored because it is easy to write it this way. In the first column, you can see the different treatments from one to five. And in the second column, you have the efficacy values of the treatments for patient 1 and it goes on for the other patients. Now, this is a good example of how a dataset should not look like! Surprised? Let us see what makes this dataset ‚Äòdirty‚Äô.\nYou can quickly notice that there is no mentioning of what these numerical values mean. Of course, we know that they are efficacy values for the different treatments. But for someone who only has this data as a reference, that person would not have a clue as to what these numbers mean. Also, note that each of the rows contains multiple observation values which is not a feature of tidy data. This kind of data format is called ‚Äòwide data‚Äô which we will talk more about later.\nWith that being said, tidy data will have;\n\nEach of its variables represented in its own column\nEach observation or a case in its own row.\nEach of the rows will contain only a single value.\n\nSo let us see how the ‚Äòtidier‚Äô version of this data would look like.\n\n\n\n\n  \n\n\n\nYou can see each of the columns represent only one type of variable. In the first column, you have the types of treatments, followed by patient IDs and their efficacy values for each treatment. Also, note that each row represents only one observation. So this kind of data format is what we strive to achieve by using the tidyr package and they are called as ‚Äòlong data‚Äô. So let us begin!"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#reshaping-dataset",
    "href": "tutorials/data_man/tidyr.html#reshaping-dataset",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n3 Reshaping dataset",
    "text": "3 Reshaping dataset\nThere are different sets of commands which you can utilize to reshape your data and make it tidy. Let us each of these commands in action. But first, make sure you have the tidyr package loaded.\n\nCode# load tidyr package\nlibrary(tidyr)\n\n\n\n3.1 pivot_longer()\nThe pivot_longer() command converts a ‚Äòwide data‚Äô to a ‚Äòlong data‚Äô. It does so by converting row names to a new column under a new variable name with its corresponding values moved into another column with another variable name. So let us see how it goes. We will take the earlier mentioned example and will see how to make it tidy. Now you don‚Äôt have to be concerned with the codes I have used to make the dummy data. Just have your focus on the pivot_longer() syntax.\n\nCodelibrary(tidyr)\n# making a dummy data\n# using sample function to pick random numbers in a sequence\npatient1 <- c(seq(1,5,1))\npatient2 <- c(seq(6,10,1))\npatient3 <- c(seq(11,15,1))\npatient4 <- c(seq(16,20,1))\npatient5 <- c(seq(21,25,1))\n\n# cbind simple combines the columns of same size\ntreatment_data <- cbind(patient1,patient2,patient3,patient4,patient5) \n\ntrt <- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\n\ntrt_data <- cbind(trt, treatment_data)\ntrt_data <- as.data.frame(trt_data) # making it a data frame\n\ntrt_data_tidy <- pivot_longer(trt_data,\n                              c(patient1,patient2,patient3,patient4,patient5), \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\ntrt_data_tidy\n\n\n\n  \n\n\n\nFurthermore, you don‚Äôt have to manually type in the column names as you can use colnames() to call the column names of the dataset. Another way of doing the same is by excluding the first column from the process. By doing so the command will automatically pivot all columns except the excluded ones, so in this way, we don‚Äôt need to manually specify the column names. The codes given below will give you the same result as before.\n\nShow the codelibrary(tidyr)\npatient1 <- c(seq(1,5,1))\npatient2 <- c(seq(6,10,1))\npatient3 <- c(seq(11,15,1))\npatient4 <- c(seq(16,20,1))\npatient5 <- c(seq(21,25,1))\ntreatment_data <- cbind(patient1,patient2,patient3,patient4,patient5) \ntreatment <- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\ntrt_data <- cbind(treatment, treatment_data)\ntrt_data <- as.data.frame(trt_data)\n# using colnames, [-1] is included to exclude the name of first column from the process\ntrt_data_tidy1 <- pivot_longer(trt_data,\n                              colnames(trt_data)[-1], \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\n\n# the same can be done by manually specifying which columns to exclude\n# this can be done by denoting the column name ('treatment' in this case) with '-' sign\ntrt_data_tidy2 <- pivot_longer(trt_data, names_to = \"patient_ID\",\n                               values_to = \"efficacy\", -treatment)\n# checking if both the tidy datasets are one and the same\ntrt_data_tidy1 == trt_data_tidy2\n\n\nThe syntax for pivot_longer() is given below with description\n\nCodepivot_longer(\"data\", c(\"colname1, colname2,.....\"), \n  names_to = \"name of the column where your row names are present\",\n  values_to = \"name of the column where your corresponding row values are present\")\n\n\nHere is a graphical representation\n\n\n3.2 pivot_wider()\nThe pivot_wider() does the exact opposite of what pivot_longer() does, which is to convert long data into wide data. We will use the previously given dummy data.\n\nCodelibrary(tidyr)\n# making a dummy data\n# using sample function to pick random numbers in a sequence\npatient1 <- c(seq(1,5,1))\npatient2 <- c(seq(6,10,1))\npatient3 <- c(seq(11,15,1))\npatient4 <- c(seq(16,20,1))\npatient5 <- c(seq(21,25,1))\n\n# cbind simple combines the columns of same size\ntreatment_data <- cbind(patient1,patient2,patient3,patient4,patient5) \n\ntrt <- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\n\ntrt_data <- cbind(trt, treatment_data)\ntrt_data <- as.data.frame(trt_data) # making it a data frame\n\ntrt_data_tidy <- pivot_longer(trt_data,\n                              c(patient1,patient2,patient3,patient4,patient5), \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\n\n# making the data wide\ntrt_data_wider <- pivot_wider(trt_data_tidy, names_from = \"patient_ID\",\n                              values_from = \"efficacy\")\n\n# paged_Table() for viewing the dataset as a table, \n# you can see that the dataset is same as before\ntrt_data_wider\n\n\n\n  \n\n\n\nThe syntax for pivot_wider() is given below with description\n\nCodepivot_longer(\"data\", \n  names_from = \"name of the column which contains your wide data columns\",\n  values_from = \"name of the column where your corresponding wide data column values are\")\n\n\nHere is a graphical representation"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#splitting-and-uniting-cells",
    "href": "tutorials/data_man/tidyr.html#splitting-and-uniting-cells",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n4 Splitting and uniting cells",
    "text": "4 Splitting and uniting cells\nThere can be an instance where you want to split or untie cells within your dataset. Let us look at some examples.\n\n4.1 unite()\nIn the data given below, let say we want to unite the century column and the year column together. This can be done using the unite() command. You can view the before and after instances in the tabs below.\n\n\nBefore\nAfter\n\n\n\n\nShow the codeevent <- c(letters[1:4])\ncentury <- c(rep(19:20, each = 2))\nyear <- c(seq(10,16,2))\ndata <- as.data.frame(cbind(event,century,year))\n\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nevent <- c(letters[1:4])\ncentury <- c(rep(19:20, each = 2))\nyear <- c(seq(10,16,2))\ndata <- as.data.frame(cbind(event,century,year))\n\n# uniting columns century and year\ndata_new <- unite(data, century, year, col = \"event_year\", sep = \"\")\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of unite() is as follows.\n\nCodeunite(\"dataset name\",\n      \"name of first column to unite, name of second column to unite,.......\",\n      col = \"name of the new column to which all the other column will unite together\",\n      sep = \"input any element as a separator between the joining column values\")\n# in this case we are not putting a sep value\n\n\n\n4.2 separate()\nIn the data given below, let say we want to split the ‚Äòarea_perimeter‚Äô column into two separate columns. This can be done using the separate() command. You can view the before and after instances in the tabs below. As always I will be making dummy data to work with.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n# viewing data as a table\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n\n# separating column values into two separate columns named area and perimeter respectively\ndata_new <- separate(data, area_perimetre, sep = \"_\",\n                     into = c(\"area\", \"perimetre\"))\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of separate() is as follows.\n\nCodeseparate(\"data name\",\n         \"column to separate into\",\n         sep = \"the separator element\",\n         into = c(\"col1\", \"col2\", \"........\")) # column names for the separated values\n\n\n\n4.3 separate_rows()\nSimilar to the above case, you can also separate column values into several rows.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n# viewing data as a table\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n\n# separating column values into two several rows\ndata_new <- separate_rows(data, area_perimetre, sep = \"_\")\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of separate_rows() is as follows.\n\nCodeseparate_rows(\"data name\",\n         \"column to separate\",\n         sep = \"the separator element\")"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#expanding-and-completing-dataset",
    "href": "tutorials/data_man/tidyr.html#expanding-and-completing-dataset",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n5 Expanding and completing dataset",
    "text": "5 Expanding and completing dataset\nYou can expand your data to include all possible combinations of values of variables listed or complete the dataset with NA values for all possible combinations.\n\n5.1 expand()\nUsing the expand() command we can expand our data with missing combinations for the variables we specify.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# viewing data as a table\ndress_data\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# expanding dataset with brand and dress as variables\ndress_data_expand <- expand(dress_data, brand, dress)\n\n# viewing data as a table\ndress_data_expand\n\n\n\n  \n\n\n\n\n\n\nThe syntax of expand() is as follows.\n\nCodeexpand(\"data name\", \"column names which you want to expand separated by commas\")\n\n\n\n5.2 complete()\nThe complete() command functions similar to the expand() command, but it also fills in NA values for columns which we didn‚Äôt specify, The main reason to use this command would be to convert implicit NA values hidden in the dataset to explicit NA values which are expressed in the dataset. Given below is a comparison between the complete() and expand() commands.\n\n\nexpand()\ncomplete()\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# expanding dataset with brand and dress as variables\ndress_data_expand <- expand(dress_data, brand, dress)\n\n# viewing data as a table\ndress_data_expand\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# completing dataset with brand and dress as variables\n# the variable 'size' will be filled with NAs as we did not specify it\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# viewing data as a table\ndress_data_complete\n\n\n\n  \n\n\n\n\n\n\nThe syntax of complete() is as follows.\n\nCodecomplete(\"data name\", \"column names which you want to complete separated by commas\")"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#handling-nas-or-missing-values",
    "href": "tutorials/data_man/tidyr.html#handling-nas-or-missing-values",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n6 Handling NAs or missing values",
    "text": "6 Handling NAs or missing values\nMost data collection would often result in possible NA values. The tidyr package allows us to drop or convert NA values. We will reuse the earlier example. Below tabs show before and removing NA values.\n\n6.1 drop_na()\nUse drop_na() to remove NA value containing rows from the dataset.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# viewing data as a table\ndress_data_complete\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# dropping NA values\n\ndress_data_noNA <- drop_na(dress_data_complete)\n\n# viewing data as a table\ndress_data_noNA\n\n\n\n  \n\n\n\n\n\n\n\n6.2 fill()\nUse fill() to replace NA values by taking values from nearby cells. By default the NA values as replaced by whatever value that is above the cell containing the NA value. This can be changed by specifying the .direction value within fill()\n\nCodelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# direction 'downup' simultaneously fill both upwards and downwards NA containing cells\ndress_data_fill <- fill(dress_data_complete, size, .direction = \"downup\")\n\n# viewing data as a table\ndress_data_fill\n\n\n\n  \n\n\n\n\n6.3 replace_na()\nUse replace_na() command to replace NA values to whatever value specified.\n\nCodelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# replace NA to unknown\n# specify the column which have NA inside the list()\n# then equate the value which would replace NAs\ndress_data_zero <- replace_na(dress_data_complete, list(size = \"unknown\"))\n\n# viewing data as a table\ndress_data_zero"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#summary",
    "href": "tutorials/data_man/tidyr.html#summary",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n7 Summary",
    "text": "7 Summary\nSo in this chapter, we learned what is tidy data and how we can make our data into tidy data. Making our data tidy is very important as it helps us to analyse and visualise the data in a very efficient manner. We also learned how to reshape our data, how to split or unite cells, how to complete and expand data and how to handle NA values. Hope this chapter was fruitful for you!"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#references",
    "href": "tutorials/data_man/tidyr.html#references",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n8 References",
    "text": "8 References\n\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html",
    "href": "tutorials/data_viz/ggplot_1.html",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "",
    "text": "This series of chapters are focused on people who have a basic understanding of R programming. You are expected to know how the basic syntaxes of R language work like how to assign values, how to load your data, the different operations of R, and so on. If you just started out on your R journey then this chapter and the rest will become a roadblock for you. But fear not, there are some amazing websites that teach you the very basics of R, and that too for free! For an interactive way of learning, I recommend DataCamp. It is an online platform for learning programming languages. They have both paid and free classes. Luckily for us, they are providing the introductory classes on R programming for free. This would be a great way to start your R journey. After you completed the course on data camp you can come back to this blog and you will find it very easy to comprehend and learn the various chapters that are available here. Coming from my own experience, it would be really helpful if you have your own data to work with. Rather than religiously following the steps in these chapters, I would recommend you have a goal in your mind before diving into the chapters. The goal should be to try incorporating the things you learned here into your own data. That would be the best way to learn anything in R. Hope you have a great time learning!"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#introduction-to-ggplot2-package",
    "href": "tutorials/data_viz/ggplot_1.html#introduction-to-ggplot2-package",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n2 Introduction to ggplot2 package",
    "text": "2 Introduction to ggplot2 package\nIn this chapter we will be plotting different types of graphs using a package called ggplot2 in R. The ggplot2 package is based on ‚Äògrammar of graphics plot‚Äô which provides a systematic way of doing data visualizations in R. With a few lines of code you can plot a simple graph and by adding more layers onto it you can create complex yet elegant data visualizations.\nA ggplot2 graph is made up of three components.\n\n\nData: Data of your choice that you want to visually summarise.\n\nGeometry or geoms: Geometry dictates the type of graph that you want to plot and this information is conveyed to ggplot2 through the geom() command code. For e.g.¬†using the geom_boxplot() command, you can plot a box plot with your data. Likewise, there are many types of geometry that you can plot using the ggplot2 package.\n\nAesthetic mappings: Aesthetics define the different kinds of information that you want to include in the plot. One fo the most important aesthetic is in choosing which data values to plot on the x-axis and the y-axis. Another example is changing the colour of the data points, which can be used to differentiate two different categories in the data. The use of aesthetics depends on the geometry that you are using. We use the command aes() for adding different types of aesthetics to the plot. We will learn more about aes() in Chapter 2. For now, we will only see what kind of plots can be made using the ggplot2 package. We will learn how to tweak them in Chapter 2.\n\nThis tutorial is primarily focused on students who are beginners in R programming and wants to quickly plot their data without much of a hassle. So without further ado let‚Äôs plot some graphs!"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#setting-up-the-prerequisites",
    "href": "tutorials/data_viz/ggplot_1.html#setting-up-the-prerequisites",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n3 Setting up the prerequisites",
    "text": "3 Setting up the prerequisites\nFirst, we need to install the ggplot2 package in R as it does not come in the standard distribution of R.\n\nTo install packages in R we use the command install.packages() and to load packages we use the command library(). Therefore to install and load ggplot2 package we use the following lines of command.\n\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nAll right we have the ggplot2 package loaded, now we just need some data to plot. Most R programming tutorials use the iris dataset as an example. But this tutorial won‚Äôt be like most tutorials. So let me introduce you to some lovely penguins from Palmer Station in Antarctica!\nFor this tutorial, we will be installing the palmerpenguins package which showcases body measurements taken from three different species of penguins from Antarctica. This package was made possible by the efforts of Dr.¬†Allison Horst. The penguin data was collected and made available by Dr.¬†Kristen Gorman and the Palmer Station, Antarctica LTER.\n\nInstall the palmerpenguins package and load it in R.\n\n\ninstall.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\nNow there are two datasets in this package. We will be using the penguins dataset which is a simplified version of the raw data present in the package.\n\nUse the command head() to display the first few values of penguins dataset to see how it looks like\n\n\nlibrary(palmerpenguins)\nhead(penguins)\n\n\n\n  \n\n\n\nWe can see that are 8 columns in the dataset representing different values. Now let us try plotting some graphs with this data.\n\n3.1 Bar graph\nSo we will try to plot a simple bar graph first. Bar graphs are used to represent categorical data where the height of the rectangular bar represents the value for that category. We will plot a bargraph representing frequency data for all three species of penguins.\n\nWe will be using the geom_bar() command to plot the bar graph. Let us also use the command theme_bw() for a nice looking theme.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, fill = species)) + \n  xlab(\"Species\") + ylab(\"Frequency\") + \n  ggtitle(\"Frequency of individuals for each species\") + \n  geom_bar() + theme_bw()\n\n\n\n\n\n3.2 Histogram\nHistograms are similar to bar graphs visually and are used to represent continuous data. Histograms splits data into ‚Äòbins‚Äô and tells us the number of observations for each of the bin.\n\nWe can plot a histogram using the command geom_histogram() and change the bin width using the binwidth = argument.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, fill = species)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Frequency\") + \n  ggtitle(\"Frequency of individuals for respective body mass\") + \n  geom_histogram(binwidth = 1000) + theme_bw()\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nThe warning message indicates that for two rows in the dataset, they have NA values or that they did not have any values present. This is true for real-life cases, as during data collection sometimes you will be unable to collect data due to various reasons. So this is perfectly fine.\n\n3.3 Line graph\nLine graph simply joins together data points to show overall distribution.\n\nUse the command geom_line() for plotting a line graph.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = bill_length_mm, \n                            y = bill_depth_mm, colour = species)) + \n  xlab(\"Bill length (mm)\") + ylab(\"Bill depth (mm)\") + \n  ggtitle(\"Bill length vs Bill depth\") + geom_line() + theme_bw()\n\n\n\n\n\n3.4 Scatter plot\nThe scatter plot simply denotes the data points in the dataset.\n\nUse the command geom_point() to plot a scatter plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm, \n                            shape = species, colour = species)) + \n  xlab(\"Body mass (g)\") + ylab(\"Flipper length (mm)\") + \n  ggtitle(\"Body mass vs Filpper length\") + geom_point(size = 2) + theme_bw()\n\n\n\n\n\n3.5 Density Plot\nDensity plots are similar to histograms but show it shows the overall distribution of the data in a finer way. This way we will get a bell-shaped curve if our data follows a normal distribution.\n\nUse the command geom_density() to a density plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, fill = species)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density() + theme_bw()\n\n\n\n\nSince we plotted for all three species the graph looks clustered. Let us try plotting the same graph for only gentoo penguins. We will use the dplyr package to filter() data for gentoo penguins alone. The dplyr package comes in-built with R so just load the dplyr package using the command library().\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins_gentoo <- penguins %>% filter(species == \"Gentoo\")\n\nggplot(data = penguins_gentoo, aes(x = body_mass_g)) + \n  xlab(\"Body Mass of Gentoo penguins (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Gentoo penguins\") + \n  geom_density(fill = \"red\") + theme_bw()\n\n\n\n\n\n3.6 Dot-plot\nDot-plot is similar to a density plot but it shows discretely each data point in the distribution.\n\nUse the command geom_dotplot() to plot a dot-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", binwidth = 100) + theme_bw()\n\n\n\n\n\n3.7 Rug-plot\nRug-plot is a simple way to visualize the distribution of data along the axis lines. It is often used in conjunction with other graphical representations.\n\nUse the command geom_rug() to plot a rug-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins_gentoo <- penguins %>% filter(species == \"Gentoo\")\n\nggplot(data = penguins_gentoo, aes(x = body_mass_g, y = flipper_length_mm)) + \n  xlab(\"Body Mass of Gentoo penguins (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Gentoo penguins\") + \n  geom_point(colour = \"darkred\") + geom_rug() + theme_bw()\n\n\n\n\n\n3.8 Box plot\nBox-plot is one of the better ways of showing data via quartiles. You can learn more about box plots here.\n\nUse the command geom_boxplot() to plot a box-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, colour = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + geom_boxplot() + \n  theme_bw()\n\n\n\n\n\n3.9 Violin plot\nViolin plot can be considered as the best of both a box-plot and a density plot. It shows the quartile values, like in a box-plot and also shows the distribution of the data, like in a density plot.\n\nUse the command geom_violin() in conjunction with geom_boxplot() to plot a violin plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_violin(aes(colour = species), trim = TRUE) + geom_boxplot(width = 0.2) +\n  theme_bw()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#saving-your-ggplot2-graphs",
    "href": "tutorials/data_viz/ggplot_1.html#saving-your-ggplot2-graphs",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n4 Saving your ggplot2 graphs",
    "text": "4 Saving your ggplot2 graphs\n\nUse the command ggsave() to save the graph locally. In the code below, ‚Äòmy_graph‚Äô is the ggplot element containing your graph. The plot will be saved in your working directory.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nmy_graph <- ggplot(data = penguins, aes(x = species, y = body_mass_g,\n                                    fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_violin(aes(colour = species), trim = TRUE) + \n  geom_boxplot(width = 0.2) +\n  theme_bw()\n\n#to save the plot\nggsave(my_graph, filename = \"your_graph_name.png\", width = 20, height = 20,\n       units = \"cm\")"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#summary",
    "href": "tutorials/data_viz/ggplot_1.html#summary",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope this tutorial helped you to get familiarize with the ggplot2 commands. The best way to learn R is by actually doing it yourself. So try to recreate the examples given in this tutorial and then try to apply what you learned using the different datasets available in R. In chapter 2, we will learn how to customize the plots by tweaking the aesthetic mappings."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#references",
    "href": "tutorials/data_viz/ggplot_1.html#references",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html",
    "href": "tutorials/data_viz/ggplot_2.html",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "",
    "text": "After going through Chapter 1 you would be now familiar with the different types of graphs that you can plot using ggplot2. So for this tutorial, we will be learning how to customize those ggplot graphs to our liking. We will learn how to tweak the aesthetics, how to change labels and how to modify and change the axes in a graph.\nSo let us plot a graph from scratch and learn how to use different aesthetics available."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#setting-up-the-prerequisites",
    "href": "tutorials/data_viz/ggplot_2.html#setting-up-the-prerequisites",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n1 Setting up the prerequisites",
    "text": "1 Setting up the prerequisites\nFirst, we need to install the ggplot2 package in R as it does not come in the standard distribution of R. For the dataset, we will first download the Stat2Data package which houses a lot of cool datasets. For this tutorial let us use the Hawks dataset which showcases body measurements from three different species of Hawks. This data was collected by students and faculty at Cornell College in Mount Vernon and the dataset was made available by late Prof.¬†Bob Black at Cornell College.\n\nTo install packages in R we use the command install.packages() and to load packages we use the command library(). Therefore to install and load ggplot2 and {Stats2Data} packages we use the following lines of command. Call the Hawks data using the data() command.\n\n\n# Installng packages\ninstall.packages(\"ggplot2\")\ndevtools::install_github(\"statmanrobin/Stat2Data\")\n\n# Loading required packages\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\n# Loading the Hawks dataset\ndata(\"Hawks\")\n\n\nLet us look at how the dataset is structured. Use str() command\n\n\nlibrary(Stat2Data)\ndata(\"Hawks\")\n\nstr(Hawks)\n\n'data.frame':   908 obs. of  19 variables:\n $ Month       : int  9 9 9 9 9 9 9 9 9 9 ...\n $ Day         : int  19 22 23 23 27 28 28 29 29 30 ...\n $ Year        : int  1992 1992 1992 1992 1992 1992 1992 1992 1992 1992 ...\n $ CaptureTime : Factor w/ 308 levels \" \",\"1:15\",\"1:31\",..: 181 25 138 42 62 71 181 88 261 192 ...\n $ ReleaseTime : Factor w/ 60 levels \"\",\" \",\"10:20\",..: 1 2 2 2 2 2 2 2 2 2 ...\n $ BandNumber  : Factor w/ 907 levels \" \",\"1142-09240\",..: 856 857 858 809 437 280 859 860 861 281 ...\n $ Species     : Factor w/ 3 levels \"CH\",\"RT\",\"SS\": 2 2 2 1 3 2 2 2 2 2 ...\n $ Age         : Factor w/ 2 levels \"A\",\"I\": 2 2 2 2 2 2 2 1 1 2 ...\n $ Sex         : Factor w/ 3 levels \"\",\"F\",\"M\": 1 1 1 2 2 1 1 1 1 1 ...\n $ Wing        : num  385 376 381 265 205 412 370 375 412 405 ...\n $ Weight      : int  920 930 990 470 170 1090 960 855 1210 1120 ...\n $ Culmen      : num  25.7 NA 26.7 18.7 12.5 28.5 25.3 27.2 29.3 26 ...\n $ Hallux      : num  30.1 NA 31.3 23.5 14.3 32.2 30.1 30 31.3 30.2 ...\n $ Tail        : int  219 221 235 220 157 230 212 243 210 238 ...\n $ StandardTail: int  NA NA NA NA NA NA NA NA NA NA ...\n $ Tarsus      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ WingPitFat  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ KeelFat     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Crop        : num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nSo there is a lot of information in the dataset which we can use for plotting. So let us try plotting them."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#building-a-plot",
    "href": "tutorials/data_viz/ggplot_2.html#building-a-plot",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n2 Building a plot",
    "text": "2 Building a plot\nOne thing to remember here is that how ggplot2 builds a graph is by adding layers. Let us start by plotting the basic layer first where the x-axis shows ‚Äòweight of the hawks‚Äô and the y-axis shows ‚Äòwingspan of the hawks‚Äô.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n\nggplot(data = Hawks, aes(x = Weight, y = Wing))\n\n\n\n\nWait a sec! Where are my data points? So right now if we look at the syntax of the ggplot code we can see that we have not told ggplot2 which geometry we want. Do we want a scatter plot or a histogram or any other type of graph? So let us plot a scatter plot first. Use geom_point() command. By adding geom_point() to the ggplot() command is equivalent to adding an extra layer to the already existing layer that we got previously. Let us also use theme_bw() for a nice looking theme.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + geom_point() + \n  theme_bw()\n\n\n\n\nWe got the graph! but we also got a warning message. The warning message tells us that the dataset which we had used to plot the graph had 11 rows of NA values and which could not be plotted into the graph. In real-life cases, we can have datasets with NA values due to various reasons, so this is fine.\nNow, this graph even though shows us data points we are not sure which point belongs to which species, as this dataset contains data for three species of Hawks. So let us try giving different colours to the points concerning the different species so that we are able to differentiate them.\n\n2.1 Changing colour\n\nTo change colour of the ‚Äòelement‚Äô as a function species, we have to add colour = Species within the aes() of the ggplot command. I use the general term ‚Äòelement‚Äô here to emphasize that the same change in aesthetics will work for most of other types of geometries in ggplot2 (something which you have seen extensively in Chapter 1. Like for a line graph, the ‚Äòelement‚Äô would be lines. Here we have a scatter plot, so the ‚Äòelement‚Äô would be points.\n\nAlso note that, in addition to colour, R also recognizes color and col wordings and they function the same as colour.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw()\n\n\n\n\nThe species abbreviations are the following: CH=Cooper‚Äôs, RT=Red-tailed, SS=Sharp-Shinned.\nNow, this graph is way better than the previous one.\n\n2.2 Changing point shape.\n\nNow instead of the colour let us change the shape of the point. Use shape() command in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, shape = Species)) + # instead of colour use shape.\n  geom_point() + theme_bw() \n\n\n\n\nNow we did change the shape of points but it is still hard to make out the difference. Let us try specifying colour along with the shape\n\nAdding both colour and shape in aesthetics\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, \n  colour = Species, shape = Species)) + geom_point() + theme_bw()\n\n\n\n\nThis plot is much better than the previous one.\nNow let us try specifying colour within the aes() of the geom()\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, shape = Species)) + \n  geom_point(aes(colour = Species)) + theme_bw()\n\n\n\n\nWe got the same graph as before! So what is the difference in specifying colour within aes() of ggplot() compared to the same but within geom_point(). Here Let us look at another example.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = \"red\")) + \n  geom_point() + theme_bw()\n\n\n\n\nI manually changed the colour of the points to red colour. Please not that you can also use hex codes to specify the colour attribute. Now let try specifying colour to the aes() within the geom()\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = \"red\")) + \n  geom_point(aes(colour = Species)) + theme_bw()\n\n\n\n\nYou can see that the red colour is overridden by other colours. So the aes() mapping (in this case colour) within geom_point() will override any aes() mapping within ggplot(). And whatever aes() mapping we give within ggplot() will be inherited by all other geom layers that are specified.\nLet us see another case.\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_point(colour = \"darkred\") + theme_bw()\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_point(aes(colour = \"darkred\")) + theme_bw()\n\n\n\n\nIf you compare both the codes, the only difference is that the colour = \"darkred\" command was outside aes() in the first code and inside aes() in the second code. So why didn‚Äôt the second graph have the same dark-red coloured points as the first one? The reason is that in the first code we are explicitly told to have all data points to be coloured dark-red but that is not the case with the second code. In the second code, since we have specified it inside aes(), ggplot is trying to look for a variable called ‚Äúdarkred‚Äù inside the dataset and colour it accordingly. This is why the legend that appears in the second graph has listed ‚Äúdarkred‚Äù as a category. And ggplot fails to find the variable called ‚Äúdarkred‚Äù but it still recognizes the colour command line and colour all the points in red. So the bottom line is that R has a pre-determined way of reading a code, so we users should well-understand what each line is expected to do and should not expect R to just fill it in accordingly to what we write.\nNow let us try a few other examples;\n\n2.3 Changing size\n\nUse size() in aes(). The shape aesthetic works best if the input variable is categorical.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Species, y = Hallux, size = Culmen)) + \n  geom_point() + theme_bw()\n\n\n\n\n\n2.4 Changing colour, shape and size manually\n\nUse scale_shape_manual() for changing shape, similarly scale_color_manual() for changing colour and scale_size_manual() for changing size of the element.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Hallux, colour = Species,\n                         shape = Species, size = Species)) + \n  geom_point() +\n  scale_shape_manual(values=c(1, 2, 3)) +\n  scale_color_manual(values=c('red','blue', 'green')) +\n  scale_size_manual(values=c(1,5,10)) + theme_bw()\n\n\n\n\n\n\n\n\n\n\n2.5 Changing the opcaity of the elements\n\nUse alpha() within the geom() with a numeric value to change the opacity of the elements. This is useful for visualizing large datasets such as this.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point(alpha = 1/5) + theme_bw()\n\n\n\n\nThe same commands also work for most of the other types of geom(). Now let us see a few other aesthetics in other types of geoms.\n\n2.6 Changing fill colour\n\nUse fill() in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, fill = Species)) +\n  geom_histogram(bins = 25) + theme_bw()\n\n\n\n\n\nUse scale_fill_manual() to manually change the colours.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, fill = Species)) +\n  geom_histogram(bins = 25) + theme_bw() + \n  scale_fill_manual(values = c(\"darkred\", \"darkblue\", \"darkgreen\"))\n\n\n\n\n\n2.7 Changing line type\n\nUse linetype in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, linetype = Species)) + \n  geom_line() + theme_bw()\n\n\n\n\n\nYou can manually change line types using scale_linetype_manual()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, linetype = Species)) + \n  geom_line() + \n  scale_linetype_manual(values= c(\"twodash\", \"longdash\", \"dotdash\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNow let us also see how to change the labels in a graph.\n\n2.8 Viewing datapoints as labels\n\nYou can plot data points as their values or as their labels using the geom_text() function.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Plotting the values\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_text(aes(label = Wing)) + theme_bw()\n\n\n\n# Plotting the values according to species text label\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_text(aes(label = Species)) + theme_bw()\n\n\n\n\n\n2.9 Changing labels in the axes\n\nUse xlab() to change x-axis title, ylab() to change y-axis title, ggtitle() with label and subtitle to add title and subtitle respectively.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + xlab(\"Weight (gm)\") + ylab(\"Wing (mm)\") +\n  ggtitle(label = \"Weight vs Wing span in three different species of Hawks\", \n          subtitle = \"CH=Cooper's, RT=Red-tailed, SS=Sharp-Shinned\")\n\n\n\n\n\nThe same result can be obtained by using labs() to specify each label in the graph. For renaming the legend title, the command will depend on what is there within the aes() or in other words what is the legend based on.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + labs(x = \"Weight (gm)\", y = \"Wing (mm)\", \n  title= \"Weight vs Wing span in three different species of Hawks\", \n  subtitle = \"CH=Cooper's, RT=Red-tailed, SS=Sharp-Shinned\",\n  caption = \"Source: Hawk dataset from Stat2Data r-package\", #caption for the graph\n  colour = \"Hawk Species\", # rename legend title\n  tag = \"A\") #figure tag\n\n\n\n\n\n2.10 Tweaking the axes\n\nUse xlim() and ylim() for limiting x and y axes respectively.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + xlim(c(0,1000)) + ylim(c(200,350))\n\n\n\n\n\nUse coord_cartesian() to zoom in on a particular area in the graph\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + coord_cartesian(xlim = c(0,1000),\n                                                   ylim = c(200,350))\n\n\n\n\n\nUse coord_flip() to flip the x and y axes.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + coord_flip()\n\n\n\n\n\nUse scale_x_continuous() for tweaking the x-axis. The same command work for the y-axis also. You can include label() inside the command to manually label the breaks of the axes.\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + scale_x_continuous(breaks = c(0,1000,2000))\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + \n  scale_x_continuous(breaks = c(0,1000,2000),label = c(\"low\", \"medium\", \"high\"))\n\n\n\n\n\nUse scale_y_reverse() to display the y values in the descending order. Same command applies to x-axis also.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + scale_y_reverse()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#typical-aesthetic-mappings",
    "href": "tutorials/data_viz/ggplot_2.html#typical-aesthetic-mappings",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n3 Typical aesthetic mappings",
    "text": "3 Typical aesthetic mappings\n\nTable of aesthetic mappings\n\nAesthetic\nDescription\n\n\n\nx\nX axis position\n\n\ny\nY axis position\n\n\nfill\nFill colour\n\n\ncolor\nColour points, outlines of other geoms\n\n\nsize\nArea or radius of points, thickness of the lines\n\n\nalpha\nTransparency\n\n\nlinetype\nLine dash pattern\n\n\nlabels\nText on a plot or axes\n\n\nshape\nShape\n\n\n\nWe are now familiar with all these different aesthetic mappings."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#summary",
    "href": "tutorials/data_viz/ggplot_2.html#summary",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n4 Summary",
    "text": "4 Summary\nIn this tutorial, we learned how to modify aesthetic present for different geoms in ggplot2 Then we learned how to modify labels in a graph and finally, we learned how to modify and change the axes elements. This tutorial is in no way exhaustive of the different ways you can modify a graph as there many more methods which are not discussed here. Instead of trying to include everything, this tutorial tries to be a stepping stone to help students of R to learn the basics of tweaking a graph. Try to practice what is covered here using other datasets available in the r-package Stat2Data."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#references",
    "href": "tutorials/data_viz/ggplot_2.html#references",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n5 References",
    "text": "5 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nAnn Cannon, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Allan Rossman and Jeffrey Witmer (2019). Stat2Data: Datasets for Stat2. R package version 2.0.0. https://CRAN.R-project.org/package=Stat2Data"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html",
    "href": "tutorials/data_viz/ggplot_3.html",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "",
    "text": "In this chapter, we will learn how to change the theme settings of a graph in ggplot2 The theme of a graph consists of non-data components present in your graph. This includes the different labels of the graph, fonts used, colour of axes, the background of the graph etc. By changing the theme we would not be changing or transforming how the data will look in the graph. Instead, we would only change the visual appearances in the graph and by doing so we can make it more aesthetically pleasing. Furthermore, we will see a few popular packages featuring ready to use themes. We will also learn about colour palettes and will see different packages associated with them."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#complete-themes",
    "href": "tutorials/data_viz/ggplot_3.html#complete-themes",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n1 Complete themes",
    "text": "1 Complete themes\nThe ggplot2 package features ready to use themes called ‚Äòcomplete themes‚Äô. So before we begin customizing themes, let us plot a few graphs and see how these themes look like. For the plots, I have used the BirdNest dataset from the Stat2Data package in R. The BirdNest dataset contains nest and species characteristics of North American passerines. The data was collected by Amy R. Moore, as a student at Grinnell College in 1999.\n\nGetting the BirdNest dataset and viewing how the dataset is structured.\n\n\ninstall.packages(\"Stat2Data\") # for installing Stat2Data package\ninstall.packages(\"ggplot2\") # for installing ggplot2 package\n\n# load the packages\nlibrary(Stat2Data)\nlibrary(ggplot2)\n\ndata(\"BirdNest\") # loading the BirdNest dataset\nstr(BirdNest) # for viewing structure of the dataset\n\nSo we have plenty of variables to play with. The tabs shown below are named according to the theme used in the plots.\n\n\ntheme_gray()\ntheme_bw()\ntheme_linedraw()\ntheme_light()\ntheme_dark()\ntheme_minimal()\ntheme_classic()\ntheme_void()\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_gray()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_light()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_dark()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_minimal()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_classic()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_void()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#themes-from-ggthemes-package",
    "href": "tutorials/data_viz/ggplot_3.html#themes-from-ggthemes-package",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n2 Themes from ggthemes package",
    "text": "2 Themes from ggthemes package\nIf you want even more pre-built themes then you can try the ggthemes package. This package was developed by Dr.¬†Jeffrey B. Arnold.\n\n# install and load ggthemes package\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\nThe tabs shown below are named after the themes present in ggthemes package.\n\n\ntheme_base()\ntheme_calc()\ntheme_clean()\ntheme_economist()\ntheme_excel()\ntheme_excel_new()\ntheme_few()\ntheme_fivethirtyeight()\ntheme_foundation()\ntheme_gdocs()\ntheme_hc()\ntheme_igray()\ntheme_map()\ntheme_pander()\ntheme_par()\ntheme_solarized()\ntheme_solid()\ntheme_stata()\ntheme_tufte()\ntheme_wsj()\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_base()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_calc()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_clean()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_economist()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_excel()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_excel_new()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_few()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_foundation()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_gdocs()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_hc()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_igray()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_map()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_pander()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_par()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_solarized()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_solid()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_stata()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_tufte()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_wsj()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#changing-colour-palettes-in-ggplot2",
    "href": "tutorials/data_viz/ggplot_3.html#changing-colour-palettes-in-ggplot2",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n3 Changing colour palettes in ggplot2",
    "text": "3 Changing colour palettes in ggplot2\nApart from ready to use themes, there are also ready to use colour palettes which we can use. A colour palette contains a set of pre-defined colours which will be applied to the different geometries present in a graph.\nChoosing a good colour palette is important as it helps us to represent data in a better way and at the same time, it also makes the graph easier to read for people with colour blindness. Let us see a few popular colour palette packages used in R.\n\n3.1 viridis package\nviridis package is a popularly used colour palette in R. It is aesthetically pleasing and well designed to improve readability for colour blind people. The {virids} package was developed by Bob Rudis, Noam Ross and Simon Garnier. There are eight different colour scales present in this package. The name of the tab denotes the colour scale present in this package.\n\nlibrary(viridis)\n\n\n\nviridis\nmagma\nplasma\ninferno\ncividis\nmako\nrocket\nturbo\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"viridis\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"magma\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"plasma\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"inferno\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"cividis\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"mako\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"rocket\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"turbo\")\n\n\n\n\n\n\n\n\n3.2 wesanderson package\nIf you like your colours distinctive and narrative, just like how American film-maker Mr.¬†Wes Anderson would like it, then try the wesanderson package. Relive the The Grand Budapest Hotel moments through your graphs. The wesanderson package was developed by Karthik Ram. There are a total of 19 colour palettes present in this package. We will see a subset of them. All colour scales in this package are available here. The name of the tab denotes the colour scale used. The data used in this plot is the penguin dataset present in the package palmerpenguins.\n\n#install and load wesanderson and palmerpenguins package\ninstall.packages(\"wesanderson\")\ninstall.packages(\"palmerpenguins\")\nlibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\") #load the penguins dataset\n\n\n\nGrandBudapest1\nBottleRocket2\nRushmore1\nRoyal1\nZissou1\nDarjeeling2\nIsleofDogs1\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"GrandBudapest1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"BottleRocket2\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Rushmore1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Royal1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Zissou1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Darjeeling2\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"IsleofDogs1\", n = 3))\n\n\n\n\n\n\n\n\n3.3 ggsci package\nIf you want high-quality colour palettes reflecting scientific journal styles then you can try the ggsci package. The ggsci package was developed by Dr.¬†Nan Xiao and Dr.¬†Miaozhu Li. All colour scales in this package are available in package webpage. The name of the tab denotes the colour scale used.\n\n# load ggsci package\ninstall.packages(\"ggsci\")\nlibrary(ggsci)\n\n\n\nnpg\naaas\nnejm\nlancet\njama\njco\nucscgb\nd3\nlocuszoom\nigv\nuchicago\nstartrek\ntron\nfuturama\nrickandmorty\nsimpsons\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_npg()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_aaas()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_nejm()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_lancet()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_jama()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_jco()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_ucscgb()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_d3(palette = \"category10\")\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_locuszoom()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_igv()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_uchicago()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_startrek()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_tron()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_futurama()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_rickandmorty()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_simpsons()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#customizing-the-theme",
    "href": "tutorials/data_viz/ggplot_3.html#customizing-the-theme",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n4 Customizing the theme()",
    "text": "4 Customizing the theme()\nA ggplot theme is made up of different elements and it‚Äôs functions. For e.g.¬†plot.title() element allows you to modify the title of the graph using the element function element_text(). In this way, we can change the font size, font family, text colour etc. of the title of the plot. So let us begin customising our graph. We will be reusing the BirdNest dataset for the graphs.\n\n4.1 Customizing text elements using element_text()\nAll text elements can be customized using the element function element_text(). The syntax for element_text() is as follows\n\nelement_text(\n  family = NULL, #insert family font name, e.g. \"Times\"\n  face = NULL,  #font face (\"plain\", \"italic\", \"bold\", \"bold.italic\")\n  colour = NULL, #either from colours() or hex code inside \"\"\n  size = NULL, #text size (in pts)\n  hjust = NULL, #horizontal justification values 0 or 1\n  vjust = NULL, #vertical justification values 0 or 1\n  angle = NULL, #angle in degrees\n  lineheight = NULL, #distance between text and axis line\n  color = NULL, #same function as colour\n  margin = NULL,\n  debug = NULL,\n  inherit.blank = FALSE\n)\n\nWe can modify each of these parameters to improve our plots as shown below in ‚Äòbefore‚Äô and ‚Äòafter‚Äô the changes are made.\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(No.eggs, Totcare, colour = Nesttype)) + geom_point() +\n    labs(x= \"Number of eggs\", y= \"Total care time (days)\",\n       title= \"Relationship between number of eggs and total care time\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\",\n       colour = \"Nest type\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(No.eggs, Totcare, colour = Nesttype)) + geom_point() +\n    labs(x= \"Number of eggs\", y= \"Total care time (days)\",\n       title= \"Relationship between number of eggs and total care time\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\",\n       colour = \"Nest type\")\n\n#customizing text elements\np + theme(plot.title=element_text(size = 15,family = \"Comic Sans MS\",colour = \"darkred\",face = \"bold\"),\n          plot.subtitle=element_text(size = 10,family = \"Courier\",colour= \"blue\",face= \"italic\"),\n          plot.caption = element_text(size = 8,family = \"Times\",colour= \"green\",face=\"bold.italic\", hjust=0),\n          axis.text.x= element_text(size = 6,colour = \"magenta\", angle=20),\n          axis.text.y= element_text(size = 6,colour = \"darkblue\", angle=30),\n          axis.title.x = element_text(colour = \"orchid\"),\n          axis.title.y = element_text(colour = \"sienna\"),\n          legend.text = element_text(size = 8,colour = \"darkgreen\"),\n          legend.title = element_text(size = 10,colour = \"lightblue\",face = \"bold\"))\n\n\n\n\n\n\n\n\n4.2 Customizing line elements using element_line()\nLine elements include axes, grid lines, borders of the graph etc. All line elements can be customized using the element function element_line(). The syntax for element_line() is as follows\n\nelement_line(\n  colour = NULL, #either from colours() or hex code inside \"\"\n  size = NULL, #line size in mm units\n  linetype = NULL, # eg: dashed, dotted etc \n  lineend = NULL, #line end style (round, butt, square)\n  color = NULL, #same function as colour\n  arrow = NULL, #arrow specification\n  inherit.blank = FALSE\n)\n\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\n\n#customizing line elements\np + theme(panel.grid.major = element_line(colour = \"red\", size = 0.8, linetype = \"dashed\"),\n          panel.grid.minor = element_line(colour = \"blue\",linetype = \"twodash\"),\n          axis.line.x = element_line(colour = \"darkred\", arrow = arrow()),\n          axis.line.y = element_line(colour = \"darkblue\"),\n          axis.ticks = element_line(size = 5, colour = \"yellow\"),\n          axis.ticks.length.y=unit(0.5, \"cm\")) #ticks positioned 0.5cm away from y axis\n\n\n\n\n\n\n\n\n4.3 Customizing background elements using element_rect()\nBackground elements include plot, panel and legend backgrounds and their margins. All background elements can be customized using the element function element_rect(). The syntax for element_rect() is as follows\n\nelement_rect(\n  fill = NULL, #fills colour\n  colour = NULL, #colours the border\n  size = NULL, #changes border size in mm units\n  linetype = NULL, #changes border linetype\n  color = NULL,\n  inherit.blank = FALSE\n)\n\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\n\n#customizing line elements\np + theme(plot.background = element_rect(size = 5, colour = \"red\", fill = \"lightblue\"),\n          panel.background = element_rect(size = 3, colour = \"blue\", fill = \"lightyellow\", linetype = \"dotted\"),\n          legend.key = element_rect(fill = \"lightgreen\"),\n          legend.background = element_rect(fill = \"grey\"),\n          legend.key.size = unit(0.75, \"cm\"))"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#summary",
    "href": "tutorials/data_viz/ggplot_3.html#summary",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope you are now able to customize the theme of a graph with ease. In this chapter, we learned about different theme elements and how to customize them. We also saw different packages in R which featured ready to use themes. We learned about colour palettes and got introduced to the popular colour packages available R. With that being said, always make sure that your graphs are colour-blind friendly."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#references",
    "href": "tutorials/data_viz/ggplot_3.html#references",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nAnn Cannon, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Allan Rossman and Jeffrey Witmer (2019). Stat2Data: Datasets for Stat2. R package version 2.0.0. https://CRAN.R-project.org/package=Stat2Data\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nJeffrey B. Arnold (2021). ggthemes: Extra Themes, Scales and Geoms for ‚Äòggplot2‚Äô. R package version 4.2.4. https://CRAN.R-project.org/package=ggthemes\nSimon Garnier, Noam Ross, Robert Rudis, Ant√¥nio P. Camargo, Marco Sciaini, and C√©dric Scherer (2021). Rvision - Colorblind-Friendly Color Maps for R. R package version 0.6.2. You can read more here.\nKarthik Ram and Hadley Wickham (2018). wesanderson: A Wes Anderson Palette Generator. R package version 0.3.6. https://CRAN.R-project.org/package=wesanderson\nNan Xiao (2018). ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for ‚Äòggplot2‚Äô. R package version 2.9. https://CRAN.R-project.org/package=ggsci"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html",
    "href": "tutorials/data_viz/ggpubr.html",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "",
    "text": "If you are a researcher who wants to have publication-ready plots but does not want to get hassled by the ggplot2 package, then let me introduce you to the ggpubr package. Using this package you can make publication grade plots without spending too much time modifying things. Even if you are a beginner in R programming and does not know how to use theggplot2 package, you will still be able to plot graphs using the ggpubr package because of how easy the syntax is. But having prior knowledge of the ggplot2 package will surely make things easier, and an experienced person will know that any plot which can be plotted using ggpubr can also be plotted using ggplot2. So let us start!\nFirst things first, install the ggpubr package and load it in the library.\n\ninstall.packages(\"ggpubr\")\nlibrary(ggpubr)\n\nLet us see what all plots can be plotted."
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#plots-in-ggpubr-package",
    "href": "tutorials/data_viz/ggpubr.html#plots-in-ggpubr-package",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n2 Plots in ggpubr package",
    "text": "2 Plots in ggpubr package\n\n2.1 Balloon plot\nThe balloon plot is similar to bar plots as it is used to represent a large categorical dataset. The size and colour of the dot can be attributed to different values in the dataset.\n\nlibrary(viridis)\nlibrary(ggpubr)\n\nggballoonplot(mtcars, fill = \"value\") + \n  scale_fill_viridis(option = \"turbo\")\n\n\n\n\n\n2.2 Bar plot\nA simple bar graph which is used for representing categorical data. By using the add function inside the main plot function, you can easily display summary statistics like mean, median etc. and various types of errors like standard error, standard deviation and various others. You can view the whole list of features here.\n\n# install.packages(\"palmerpenguins\")\nlibrary(ggpubr)\nlibrary(palmerpenguins)\n\nggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\n\n\n\n\n2.3 Box plot\nStandard box plot graph. Like in the previous graph you can specify colour palettes from the scientific journal palettes featured in the ggsci R package.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\")\n\n\n\n\n\n2.4 Violin plot\nA simple violin plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggviolin(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          fill = \"species\",\n          palette = \"npg\",\n          add = \"boxplot\",\n          shape = \"species\")\n\n\n\n\n\n2.5 Density plot\nStandard density plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggdensity(penguins,\n          x = \"body_mass_g\",\n          color = \"species\",\n          rug = TRUE,\n          fill = \"species\",\n          add = \"mean\",\n          title = \"Mean body mass of penguins\",\n          xlab = \"Body mass (g)\",\n          palette = \"lancet\")\n\n\n\n\n\n2.6 Donut chart\nSimilar to a pie diagram. Also please note that you don‚Äôt have to explicitly mention x and y parameters in the command. You can simply just type the column names, the first column name will be shown on the x-axis and the second on the y axis.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(tidyr)\n\npenguins_freq <- penguins %>% drop_na() %>%\n  group_by(species) %>%\n  summarise(frequency = length(species))\n\nlabs <- paste0(penguins_freq$species, \" (\", round((penguins_freq$frequency/sum(penguins_freq$frequency))*100, digits = 0), \"%)\")\n\nggdonutchart(penguins_freq,\n             \"frequency\",\n             label = labs,\n             fill = \"species\",\n             palette = \"ucscgb\",\n             lab.pos = \"in\",\n             title = \"Frequency of penguins\")\n\n\n\n\n\n2.7 Pie chart\nSimple pie chart.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(tidyr)\n\npenguins_freq <- penguins %>% drop_na() %>%\n  group_by(species) %>%\n  summarise(frequency = length(species))\n\nlabs <- paste0(penguins_freq$species, \" (\", round((penguins_freq$frequency/sum(penguins_freq$frequency))*100, digits = 0), \"%)\")\n\nggpie(penguins_freq,\n             \"frequency\",\n             label = labs,\n             fill = \"species\",\n             palette = \"futurama\",\n             lab.pos = \"in\",\n             title = \"Frequency of penguins\")\n\n\n\n\n\n2.8 Dot chart\nThis is an upgrade from bar charts where the data is displayed with minimum clutter in the form of dots. This allows the readers to not get bothered about things like the slope of a line in case of line plots, or width of bars in case of bar charts or any other confusing aesthetics of a plot. You can read more about this graph here. It is also called ‚ÄúCleveland dot plots‚Äù named after the founder of this plot.\n\nlibrary(tibble)\nlibrary(ggpubr)\nlibrary(tidyr)\n\nmtcars %>% rownames_to_column(var = \"car_names\") %>% \n  mutate(cyl = as.factor(cyl)) %>%\n  ggdotchart(\"car_names\",\n             \"mpg\",\n             color = \"cyl\",\n             palette = \"aaas\",\n             sorting = \"ascending\",\n             rotate = TRUE,\n             y.text.col = TRUE,\n             dot.size = 2,\n             ylab = \"Miles per gallon of fuel\",\n             title = \"Mileage of different cars\",\n             ggtheme = theme_pubr()) + theme_cleveland() \n\n\n\n\n\n2.9 Dot plot\nSimple dot plot. Similar to a box plot. You can also overlay a box plot or a violin plot over the dot plot using the add function inside the main function.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggdotplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          fill = \"species\",\n          add = \"mean_sd\",\n          palette = \"locuszoom\")\n\n\n\n\n\n2.10 Histogram plot\nThe same function as that of a density plot but the data is represented in bars.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ngghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\n\n\n\n\n2.11 Line plot\nA simple line plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggline(penguins,\n      x = \"body_mass_g\",\n      y = \"bill_depth_mm\",\n      linetype = \"species\",\n      shape = \"species\",\n      color = \"species\",\n      title = \"Body mass vs Bill depth\",\n      xlab = \"Body mass (g)\",\n      ylab = \"Bill depth (mm)\",\n      palette = \"startrek\")\n\n\n\n\n\n2.12 Plotting paired data\nThis is essentially a box plot but for paired data. Widely used to represent treatment groups showing before and after results of the same sample. We will be using the Anorexia dataset from the {PairedData} package in R. It features weights of girls before and after treatment for Anorexia.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(ggpubr)\ndata(\"Anorexia\")\n\nAnorexia %>% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\")\n\n\n\n\n\n2.13 Quantile-Quantile plot\nQuantile-Quantile plot or QQ plot is useful in assessing the distribution of a data. A data having normal distribution will be shown as a straight line of the formula ‚Äòy=x‚Äô in the QQ plot. Points outside the confidence interval are outliers in the data.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% ggqqplot(\"body_mass_g\",\n                      color = \"species\",\n                      palette = \"aaas\",\n                      title = \"Quantile-Quantile plot\")\n\n\n\n\n\n2.14 Scatter plot\nA simple scatter plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% filter(species == \"Chinstrap\") %>%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            add = \"reg.line\",\n            add.params = list(color = \"darkred\", fill = \"yellow\"),\n            cor.coef = TRUE,\n            cor.method = \"pearson\",\n            conf.int = TRUE,\n            title = \"Body mass distribution of Chinstrap penguins\",\n            subtitle = \"Correlation method used was Pearson\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill lenght (mm)\")\n\n\n\n\nYou can also use scatter plot for data having different categories. Using ellipse=TRUE you can group data to its category.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5,\n            palette = \"d3\",\n            ellipse = TRUE, #adds an ellipse to group data of different category\n            title = \"Body mass vs Bill length\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\nYou can also label points in the scatter plot using the label function.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nmtcars %>% rownames_to_column(var = \"car_names\") %>% \n  mutate(cyl = as.factor(cyl)) %>%\n  ggscatter(\"wt\",\n             \"mpg\",\n             color = \"cyl\",\n             palette = \"nejm\",\n             xlab = \"Weight (1000 lbs)\",\n             ylab = \"Miles per gallon of fuel\",\n             title = \"Mileage vs Weight of different cars\",\n             label = \"car_names\",\n             repel = TRUE,\n             ggtheme = theme_pubr()) + theme_cleveland() \n\n\n\n\n\n2.15 Scatter plot with marginal histograms\nThis is plot is a combination of scatter plot and histograms.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.params = list(fill = \"species\", color = \"black\", size = 0.2),\n            title = \"Body mass distribution of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\nYou can also choose to show box plots.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_depth_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.plot = \"boxplot\",\n            title = \"Body mass vs Bill depth\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill depth (mm)\",\n            ggtheme = theme_bw())"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#other-functions-in-ggpubr-package",
    "href": "tutorials/data_viz/ggpubr.html#other-functions-in-ggpubr-package",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n3 Other functions in ggpubr package",
    "text": "3 Other functions in ggpubr package\n\n3.1 Statistical tests\nYou can do various statistical tests using the functions in the ggpubr package. We will be using the Anorexia dataset in the {PairedData} package in R. In the code given below, we are doing a Wilcoxon test to compare the mean weights of girls before treatment to the mean weights of girls post-treatment. Since the data is paired we will indicate it by the paired = TRUE function. A word of caution! Before starting to do statistical tests please ensure whether you can fulfil conditions for using parametric tests or not using or data. You can check whether your data is normally distributed using a QQ plot or by using any normality tests.\nPS: I use knitr::kable() just for illustrative purpose only. You can run the command inside the kable() argument and you will be fine.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndata(\"Anorexia\")\n\n# tidying the data\nAnorexia_new <- Anorexia %>% \n  pivot_longer(c(Prior, Post), names_to = \"condition\", values_to = \"weight\")\nknitr::kable(compare_means(weight ~ condition, Anorexia_new, paired = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\np\np.adj\np.format\np.signif\nmethod\n\n\nweight\nPrior\nPost\n0.0008392\n0.00084\n0.00084\n***\nWilcoxon\n\n\n\n\nYou can also do parametric tests like ANOVA and its non-parametric version; the Kruskal-Wallis test, which can be followed by multiple pairwise comparisons.\n\nknitr::kable(compare_means(body_mass_g ~ species, penguins, method = \"anova\"))\n\n\n\n.y.\np\np.adj\np.format\np.signif\nmethod\n\n\nbody_mass_g\n0\n0\n<2e-16\n****\nAnova\n\n\n\nknitr::kable(compare_means(body_mass_g ~ species, penguins, method = \"kruskal.test\"))\n\n\n\n.y.\np\np.adj\np.format\np.signif\nmethod\n\n\nbody_mass_g\n0\n0\n<2e-16\n****\nKruskal-Wallis\n\n\n\n\nNow doing pairwise comparisons\n\n# multiple pairwise comparisons\n# when there is more than two levels, the function automatically does pairwise comparisons\nknitr::kable(compare_means(body_mass_g ~ species, penguins))\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\np\np.adj\np.format\np.signif\nmethod\n\n\n\nbody_mass_g\nAdelie\nChinstrap\n0.4854773\n0.49\n0.49\nns\nWilcoxon\n\n\nbody_mass_g\nAdelie\nGentoo\n0.0000000\n0.00\n<2e-16\n****\nWilcoxon\n\n\nbody_mass_g\nChinstrap\nGentoo\n0.0000000\n0.00\n<2e-16\n****\nWilcoxon\n\n\n\n\n\n\n3.2 Descriptive statistics by groups\nUsing the function desc_statby() we can get the summary statistics of a dataset in the form of a data frame. Similar to the summary() function in base R.\n\nknitr::kable(desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nlength\nmin\nmax\nmedian\nmean\niqr\nmad\nsd\nse\nci\nrange\ncv\nvar\n\n\n\nAdelie\n151\n2850\n4775\n3700\n3700.662\n650.0\n444.780\n458.5661\n37.31758\n73.73601\n1925\n0.1239146\n210282.9\n\n\nChinstrap\n68\n2700\n4800\n3700\n3733.088\n462.5\n370.650\n384.3351\n46.60747\n93.02891\n2100\n0.1029537\n147713.5\n\n\nGentoo\n123\n3950\n6300\n5000\n5076.016\n800.0\n555.975\n504.1162\n45.45463\n89.98198\n2350\n0.0993134\n254133.2\n\n\n\n\n\nYou can also show the data as a table using the ggtexttable() function.\n\nsummary <- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short <- summary %>% dplyr::select(species, mean, median, se, sd)\nsummary_tbl <- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\nsummary_tbl\n\n\n\n\n\n3.3 Showing p-values and statistical results within plots\nUsing various functions you can show statistical outputs within the plots.\nUsing Wilcoxon test for paired data.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(ggpubr)\n\ndata(\"Anorexia\")\nAnorexia %>% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\") + stat_compare_means(paired = TRUE)\n\n\n\n\nUsing t-test\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% filter(species == \"Adelie\" & island == c(\"Biscoe\",\"Torgersen\")) %>%\nggboxplot(x = \"island\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Island\",\n          ylab = \"Bill depth (mm)\",\n          color = \"island\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"island\") + stat_compare_means(method = \"t.test\")\n\n\n\n\nUsing ANOVA test and t-test as post hoc test. For pairwise comparison, we have to manually list out the pairwise comparisons that we want.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\n# listing out pairwise comparisons \ncompare <- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\n\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(method = \"anova\", label.y = 25) + #anova test\n  stat_compare_means(comparisons = compare, method = \"t.test\") # post hoc test using t-test\n\n\n\n\nUsing Kruskal-Wallis test and Wilcoxon test as post hoc test.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ncompare <- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(label.y = 25) + #anova test\n  stat_compare_means(comparisons = compare) # post hoc test using t-test\n\n\n\n\nYou can also choose to show only asterisks as significance levels\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ncompare <- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(label.y = 25) +\n  stat_compare_means(comparisons = compare, label = \"p.signif\")\n\n\n\n\nFor illustrative purposes, I have used box plots for showing p-values and statistical test results, but you can do the same with most of the other types of graphs shown in this chapter.\n\n3.4 Faceting plots into grids\nYou can also facet different plots into grids using the function facet.by.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% drop_na() %>%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5,\n            palette = \"d3\",\n            facet.by = c(\"island\", \"sex\"), # faceting graphs via island and sex categories\n            title = \"Body mass vs Bill length\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\n\n3.5 Adding paragraph\nYou can also add a paragraph beneath the plot of your interest using the ggparagraph() and ggarrange() functions.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\ndata(\"Anorexia\")\nlibrary(ggpubr)\n\ntext <- paste(\"The above dataset shows the weight (in lbs) of 17 girls before\",\n              \"and after they got treatment for anorexia.\", sep = \" \")\n\ntext_plot <- ggparagraph(text, face = \"bold\", size = 12)\n\nplot <- Anorexia %>% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\")\n\nggarrange(plot, text_plot,\n         ncol = 1, nrow = 2,\n         heights = c(1, 0.3))\n\n\n\n\n\n3.6 Having plots placed adjacent to each other\nYou can use the ggarrange() function to place different plots together.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nbar_plot <- ggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\nhistogram <- gghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\nsummary <- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short <- summary %>% dplyr::select(species, mean, median, se, sd)\nsummary_tbl <- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\n\n# arranging plots together\nggarrange(bar_plot, histogram,\n         ncol = 2, nrow = 2, labels = c(\"A\", \"B\"),\n         heights = c(1, 0.3))\n\n\n\n\nIf you are arranging three graphs it is better to use the grid.arrange() function from the {gridExtra} package in R.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nbar_plot <- ggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\nhistogram <- gghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\nsummary <- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short <- summary %>% dplyr::select(species, mean, median, se, sd)\nsummary_tbl <- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\n\n# arranging three plots together\nlayout_matrix <- matrix(c(1, 1, 2, 2, 4, 3, 3, 4), nrow = 2, byrow = TRUE)\nlibrary(gridExtra)\ngrid.arrange(bar_plot, histogram, summary_tbl, layout_matrix = layout_matrix)"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#saving-your-plot",
    "href": "tutorials/data_viz/ggpubr.html#saving-your-plot",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n4 Saving your plot",
    "text": "4 Saving your plot\nUsing the function ggexport() you can save your plot. Tweak width and height accordingly and also change the resolution to fit your needs.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.params = list(fill = \"species\", color = \"black\", size = 0.2),\n            title = \"Body mass distribution of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\") %>%\n  ggexport(filename = \"my_plot.png\", width = 800, height = 600, res = 150)"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#summary",
    "href": "tutorials/data_viz/ggpubr.html#summary",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n5 Summary",
    "text": "5 Summary\nIn this chapter we learned how to plot publication standard graphs using the ggpubr package in R. Even with little to no experience in using the ggplot2 package in R, one can plot graphs with ease using the ggpubr package. To quickly recap, from this chapter we saw;\n\nHow to plot around 15 different types of graphs\nHow to facet plots\nHow to do basic statistical tests and visualize them within graphs\nHow to add paragraph text under the graphs\nHow to group different graphs into one single file\n\nI hope this chapter was useful to you. Check out the other chapter for more beginner content."
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#references",
    "href": "tutorials/data_viz/ggpubr.html#references",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n6 References",
    "text": "6 References\n\nAlboukadel Kassambara (2020). ggpubr: ‚Äòggplot2‚Äô Based Publication Ready Plots. R package version 0.4.0. https://CRAN.R-project.org/package=ggpubr\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nStephane Champely (2018). PairedData: Paired Data Analysis. R package version 1.1.1. https://CRAN.R-project.org/package=PairedData\nWilliam S. Cleveland & Robert McGill (1984) Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods, Journal of the American Statistical Association, 79:387, 531-554, DOI: 10.1080/01621459.1984.10478080\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nSimon Garnier, Noam Ross, Robert Rudis, Ant√¥nio P. Camargo, Marco Sciaini, and C√©dric Scherer (2021). Rvision - Colorblind-Friendly Color Maps for R. R package version 0.6.2."
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html",
    "href": "tutorials/stat_basic/intro_stat.html",
    "title": "Introductory statistics with R",
    "section": "",
    "text": "TL;DR\n\n\n\nIn this article you will learn;\n\nDescriptive and Inferential statistics\nMeasures of centre: mean, median and mode\nMeasures of spread: variance, standard deviation, mean absolute deviation and interquartile range\nDistributions: binomial, normal, standard normal, Poisson, exponential, student‚Äôs t and log-normal\nCentral limit theorem\nData transformation: Skewness, Q-Q plot and data transformation functions"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#introduction",
    "href": "tutorials/stat_basic/intro_stat.html#introduction",
    "title": "Introductory statistics with R",
    "section": "\n1 Introduction",
    "text": "1 Introduction\nThis tutorial will cover the basics of statistics and help you to utilize those concepts in R. Most of the topics that I will be discussing here is from what I learned from my undergraduate course: Biological Data Analysis, which was offered in 2019 at IISER-TVM. Then by reading online and by taking courses at DataCamp I further continued my journey to master statistics on my own. If you find any mistakes in any of the concepts that I have discussed in this tutorial, kindly raise it as a GitHub issue or comment them down or you can even mail me (the mail address is given in the footnote). Without further ado let us start!"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#why-do-we-need-statistics",
    "href": "tutorials/stat_basic/intro_stat.html#why-do-we-need-statistics",
    "title": "Introductory statistics with R",
    "section": "\n2 Why do we need statistics?",
    "text": "2 Why do we need statistics?\nIf you are a student and are currently pursuing an undergrad course in college or a university, then chances are that you would come across scientific publications in your respective fields. And while you are reading them, for most of the articles, there will be a separate section highlighting the statistical methods used in the published study. So what was the purpose of it? One of the most important purpose statistical tests fulfil is that it gives evidence to test if the results given in the study occurred due to pure chance alone or due to the experiment that the authors of the paper did."
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#descriptive-and-inferential-statistics",
    "href": "tutorials/stat_basic/intro_stat.html#descriptive-and-inferential-statistics",
    "title": "Introductory statistics with R",
    "section": "\n3 Descriptive and Inferential statistics",
    "text": "3 Descriptive and Inferential statistics\nStatistics is broadly categorized into two: Descriptive statistics and Inferential statistics\nDescriptive statistics: Descriptive statistics are used to describe and summarise data. Suppose we have data on the food preferences of a bird in a small area of an island. We found that out of the 100 birds we observed, 20 of them prefer fruits (20%), 30 of them prefer insects (30%) and the remaining 50 prefer reptiles (50%). So with the available data, this is what we know about the birds we have observed.\nInferential statistics: Inferential statistics, as suggested by the name is used to make inferences about the population using the sample collected from that population. Using the above data, we can make inferences about the food preferences of all the birds on the island. Thus, we can find what is the percentage of birds on that island that would be preferring reptiles to eat."
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#a-note-on-data",
    "href": "tutorials/stat_basic/intro_stat.html#a-note-on-data",
    "title": "Introductory statistics with R",
    "section": "\n4 A note on data",
    "text": "4 A note on data\nData can be broadly categorised into two: Quantitative and Qualitative. Quantitative data are numerical values and can be continuous or discrete. They can also be arranged in a defined order (as they are numbers) and are thus called ordinal data. Qualitative data are often categorical data which can be either ordinal or nominal.\n\nExamples of quantitative and qualitative data\n\n\n\n\n\nExamples of Quantitative data\nExamples of Qualitative data\n\n\n\nSpeed of train\nSurvival data: Alive or Dead\n\n\nAge of a person\nOutcomes: Win or Lose\n\n\nProportion of visits to a shop\nChoices: Kannur or Pune\n\n\nChange in prices of an item\nMarital status: Married or Unmarried\n\n\nGrowth of bacteria\nOrdered choices: Agree, Somewhat agree, Disagree"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#measures-of-centre",
    "href": "tutorials/stat_basic/intro_stat.html#measures-of-centre",
    "title": "Introductory statistics with R",
    "section": "\n5 Measures of centre",
    "text": "5 Measures of centre\nA good way to summarise data is by looking at their measure of the centre which can be mean, median and mode. I am sure that you all know how to find these measures.\n\nMean is best used to describe data that are normally distributed or don‚Äôt have any outliers.\nMedian is best for data with non-normal distribution as it is not affected much by outliers in the data. For a median value of 10, what it means is that 50% of our data is above the value of 10 and 50% of the remaining data is below the value of 10.\nMode is best used if our data have a lot of repeated values and is best used for categorical data as for calculating mode, the data does not need to be in ordinal scale.\n\nLet us visualize the measure of centres.\nWe will use the penguins dataset from the palmerpenguins package in R. Let‚Äôs plot the distribution curve for the ‚Äúbody mass‚Äù of the ‚ÄúChinstrap‚Äù species of penguins. In R, function for calculating mean is mean() and for median is median(). There is no base function to calculate the mode, so we will write a function in R which can calculate the mode value.\n\nCodeif (!require(palmerpenguins)) install.packages('palmerpenguins')\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Creating a function to calculate the mode value\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\n# Calculating the mean, median and mode\npen_avg <- penguins %>% filter(species == \"Chinstrap\") %>% summarise(mean = mean(body_mass_g),\n                                                          median = median(body_mass_g),\n                                                          mode = getmode(body_mass_g))\n\n# Plotting the data\npenguins %>% filter(species == \"Chinstrap\") %>% \n  ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density(fill = \"grey\") + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Chinstrap\") %>% nrow())) +\n  geom_vline(aes(xintercept = pen_avg$mean, colour = \"red\")) +\n  geom_text(aes(x=pen_avg$mean, label=\"mean\", y=4e-04), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = pen_avg$median, colour = \"blue\")) +\n  geom_text(aes(x=pen_avg$median, label=\"median\", y=4e-04), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = pen_avg$mode, colour = \"green\")) +\n  geom_text(aes(x=pen_avg$mode, label=\"mode\", y=4e-04), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\nFor a normal distribution, which is what we have in this case, the mean, median and mode are all the same value and are in the middle of the curve. For a skewed dataset or a dataset with outliers, all the measures of the centre will be different and they depend on the skewness of the data.\nConsider the plot of the diamonds dataset from the ggplot2 package in R. The data is skewed in nature.\n\nCodelibrary(tidyverse)\n\n# Create function to get mode\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\n# Plotting the data\ndiamonds %>% filter(cut == \"Fair\") %>%\n  ggplot(aes(x = carat)) + \n  xlab(\"carats\") + ylab(\"Density\") + ggtitle(\"Diamond carat distribution\") + \n  geom_density(fill = \"gold\") + \n  labs(subtitle = paste0(\"N=\",diamonds %>% filter(cut == \"Fair\") %>% nrow())) +\n  geom_vline(aes(xintercept = mean(diamonds$carat), colour = \"red\")) +\n  geom_text(aes(x=mean(diamonds$carat), label=\"mean\", y=1), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = median(diamonds$carat), colour = \"blue\")) +\n  geom_text(aes(x=median(diamonds$carat), label=\"median\", y=1), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = getmode(diamonds$carat), colour = \"green\")) +\n  geom_text(aes(x=getmode(diamonds$carat), label=\"mode\", y=1), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\nHere the curve is right-skewed, as the mean is skewed towards the right side of the mode value. If it was the other way around then the curve will be called left-skewed.\n\n\nMeasures of centre in relation to skeweness"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#measures-of-spread",
    "href": "tutorials/stat_basic/intro_stat.html#measures-of-spread",
    "title": "Introductory statistics with R",
    "section": "\n6 Measures of spread",
    "text": "6 Measures of spread\nLike the measures of the centre, we can also summarise the data by measuring the spread of the data. Spread tell us how close or how far each of the data points is distributed in the dataset. There are many methods to measure the spread of the data, let us look at each of them one by one.\n\n6.1 Variance\nThe variance of data is defined as the average squared difference from the mean of the data. It tells us how far each of our data points is from the mean value.\nThe formula for finding the variance is as follows;\ns = \\sigma^2 = \\frac{\\sum (x_{i} - \\bar{x})^{2}}{n - 1}\nWhere s is sample variance, x_{i} is your data point, \\bar{x} is mean, n is the sample size and n-1 is called as the degrees of freedom. Here \\sigma is the standard deviation which is explained below.\nThe function to calculate variance in R is var()\n\nlibrary(palmerpenguins)\n\n# Calculating the variance of bill length\nvar(penguins$bill_length_mm, na.rm = T)\n\n[1] 29.80705\n\n\n\n6.2 Standard deviation\nStandard deviation like the variance tells us how much each of our data points is spread around the mean and is easier to understand as they are not being squared like in the case with variance.\nThe formula to calculate the standard deviation of the mean is;\n\\sigma = \\sqrt{\\frac{\\sum (x_{i} - \\bar{x})^{2}}{n - 1}}\nWhere \\sigma is the standard deviation, x_{i} is your data point, \\bar{x} is the sample mean, n is the sample size and n-1 is called the degrees of freedom. Here \\sigma^2 gives the variance of the data.\nThe function to calculate variance in R is sd()\n\nlibrary(palmerpenguins)\n\n# Calculating the variance of bill length\nsd(penguins$bill_length_mm, na.rm = T)\n\n[1] 5.459584\n\n# Calculating the variance\n(sd(penguins$bill_length_mm, na.rm = T))^2\n\n[1] 29.80705\n\n\n\n6.3 Mean absolute deviation\nMean absolute deviation takes the absolute value of the distances to the mean and then takes the mean of those differences. While this is similar to standard deviation, it‚Äôs not the same. Standard deviation squares distances, so longer distances are penalized more than shorter ones, while mean absolute deviation penalizes each distance equally.\nThe formula to calculate mean absolute deviation is;\nMAD = (\\frac{1}{n})\\sum_{i=1}^{n}\\left | x_{i} - \\bar{x} \\right |\nWhere n is the sample size, x_{i} is your data point, \\bar{x} is the sample mean, n is the sample size and n-1 is called the degrees of freedom. Here \\sigma^2 gives the variance of the data.\nTo calculate absolute deviation in R, there is no base function to do so, so we have to manually calculate it.\n\nlibrary(palmerpenguins)\n\n# Calculating the distances\ndistances <- penguins$bill_length_mm - mean(penguins$bill_length_mm, na.rm = T)\n\n# Calculating the mean absolute deviation\nmean(abs(distances), na.rm = T)\n\n[1] 4.706797\n\n\n\n6.4 Interquartile range (IQR)\nQuantiles of data can be calculated using the quantile() function in R. By default, the data is split into four equal parts which is why it‚Äôs called a ‚Äòquartile‚Äô.\n\nlibrary(palmerpenguins)\n\n# Calculating the quartile\nquantile(penguins$bill_length_mm, na.rm = T)\n\n    0%    25%    50%    75%   100% \n32.100 39.225 44.450 48.500 59.600 \n\n\nHere the data is split into four equal parts which are called the quartiles of the data. In the output, we can see that 25% of the data points are between 32.1 and 39.225, and another 25% of the data points are between 39.225 and 44.450 and so on. Here the 50% quartile is 44.450 which is the median value.\nWe can manually specify the splitting. Below given code splits the data into five parts and hence is called a quantile. The splitting is specified by the argument probs inside the quantile() function. You can either manually put the proportions of the split or use the seq() function to provide the proportions.\n\nlibrary(palmerpenguins)\n\n# Calculating the quantile\nquantile(penguins$bill_length_mm, probs =  seq(0, 1, 0.2), na.rm = T)\n\n   0%   20%   40%   60%   80%  100% \n32.10 38.34 42.00 46.00 49.38 59.60 \n\n\n\nIQR is the distance between the second quartile and the third quartile of the data or the height of the box plot.\n\nThe interquartile range can be calculated using the IQR() function.\n\n# Finding the IQR\nIQR(penguins$bill_length_mm, na.rm = T)\n\n[1] 9.275\n\n\nThe interquartile range is overall the best way to summarise the spread of the data and forms the crux of a boxplot design.\n\nlibrary(palmerpenguins)\n\n# Plotting a boxplot\nggplot(penguins, aes(species, body_mass_g, fill = species)) + \n  geom_boxplot() +\n  labs(title = \"Body masses of three different species of penguins\",\n       x = \"Penguin species\",\n       y = \"Body mass (g)\") +\n  theme_bw()\n\n\n\n\nIn the box plot, the middle dark line is the median or the 50% quartile or the second quartile, the bottom of the box is the first quartile (25%) and the top of the box is the third quartile (75%). You can see two data points outside the box for ‚ÄòChinstrap‚Äô species, these are outliers. What makes a data point an outlier? That‚Äôs where the whiskers of the box come in. Typically the outliers are calculated in the following format;\n\nOutliers: x_i < Q_1 - 1.5 * IQR or x_i > Q_3 + 1.5 * IQR\n\n\nHere Q_1 is the first quartile, Q_3 is the third quartile and IQR is the interquartile distance.\n\n\nCharacteristics of a boxplot"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#distributions",
    "href": "tutorials/stat_basic/intro_stat.html#distributions",
    "title": "Introductory statistics with R",
    "section": "\n7 Distributions",
    "text": "7 Distributions\nWe will come across different types of distributions while analysing data. Let us look at each of them.\n\n7.1 Binomial distribution\nThe binomial distribution describes the probability of the number of successes in a sequence of independent trials. You might have seen this type of distribution in your introductory probability classes. It is easy to visualize this using a coin toss event. Imagine we have a fair coin, we are tossing it to see the number of times we get heads. So here getting a head is a success and getting a tail is a failure.\nIn R we can simulate this using the rbinom() function.\nWe want to see the results when we are tossing a coin once. Since the functions take random values, to be concise I will set a seed so that you can repeat the codes that I have given to get the same results.\n\n# Setting seed\nset.seed(123)\n\n# Tossing a coin one time\nrbinom(1,1,0.5)\n\n[1] 0\n\n\nWe got a value of zero, which corresponds to the event of getting a tail. Here, in the rbinom() function, we have the following syntax;\nrbinom(no. of flips, no. of coins, the probability of getting a head)\nTo see the number of heads we get by flipping a single coin three times would be with equal chances of getting a head and a tail is;\n\n# Setting seed\nset.seed(123)\n\n# Tossing a one coins three times\nrbinom(3,1,0.5)\n\n[1] 0 1 0\n\n\nIn the first flip we got zero heads, in the second we got one head and in the third, we got zero head.\nChecking to see how total number of heads we get by flipping three coins one time;\n\n# Setting seed\nset.seed(258)\n\n# Tossing a three coins one time\nrbinom(1,3,0.5)\n\n[1] 2\n\n\nSo we get a total of 2 heads when we flipped three coins one time.\nChecking to see the total number of heads we get by flipping four coins three times.\n\n# Setting seed\nset.seed(258)\n\n# Tossing a four coins three times\nrbinom(3,4,0.5)\n\n[1] 3 3 2\n\n\nIn the first flip of three coins, we got three heads, in the second we got again three heads and lastly, we got 2 heads.\nWe can also calculate the results if our coin is biased.\nChecking to see the total number of heads we get by flipping four coins three times, the coin only has a 25% probability of falling on heads.\n\n# Setting seed\nset.seed(258)\n\n# Tossing a four coins three times, but coins is unfair\nrbinom(3,4,0.25)\n\n[1] 2 2 1\n\n\nHence the rbinom() function is used to sample from a binomial distribution.\nWhat about finding discrete probabilities like are chance of getting 7 heads if we flipped a coin 10 times? To find that we use the function dbinom().\nChecking the probability of getting exactly 7 heads while tossing a coin 10 times;\n\n# Setting seed\nset.seed(258)\n\n# Probability of getting 7 heads in 10 coin flips\n# dbinom(no. of heads, no. of flips, chance of getting a head)\ndbinom(7,10,0.5)\n\n[1] 0.1171875\n\n\nLikewise, we can also find the probability of getting 7 or fewer heads while tossing the 10 times using the pbinom() function.\n\n# Setting seed\nset.seed(258)\n\n# Probability of getting 7 heads in 10 coin flips\n# pbinom(no. of heads, no. of flips, chance of getting a head)\npbinom(7,10,0.5)\n\n[1] 0.9453125\n\n\nTo find the probability of getting more than 7 heads in 10 trials would be;\n\n# Setting seed\nset.seed(258)\n\n# Probability of getting more than 7 heads in 10 coin flips\n# Set lower.tail = FALSE\npbinom(7,10,0.5, lower.tail = F)\n\n[1] 0.0546875\n\n# Alternatively you find it in the following also\n1 - pbinom(7,10,0.5)\n\n[1] 0.0546875\n\n\nThe expected value of an event would be equal to; E = n*p\nWhere n is the number of trails and p is the probability of successes.\nNow let us visualize the binomial distribution using the function we have covered till now;\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a binomial distribution\ndata.frame(heads = 0:10, \n           pdf = dbinom(0:10, 10, prob = 0.5)) %>%\nggplot(aes(x = factor(heads), y = pdf)) +\n  geom_col() +\n  geom_text(\n    aes(label = round(pdf,2), y = pdf + 0.01),\n    position = position_dodge(0.9),\n    size = 3,\n    vjust = 0\n  ) +\n  labs(title = \"Probability of X = x successes.\",\n       subtitle = \"Binomial distribution (n = 10, p = 0.5)\",\n       x = \"No of heads (successes)\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\nThis is a binomial distribution showing the probability of getting heads when a single coin is flipped 10 times. For a fair coin, we get the expected probability that most times we will get 5 heads when the coin is flipped 10 times.\n\n7.2 Normal distribution\nWe saw an example of the normal distribution when we were discussing the measures of the centre. The normal distribution or also known as the Gaussian distribution is one of the most important distributions in statistics as it is one of the requirements the data has to fulfil for numerous statistical analyses.\nLet us look at the penguins dataset from the palmerpenguins package in R. We will plot the distribution curve for the ‚Äúbody mass‚Äù of the ‚ÄúChinstrap‚Äù species of penguins.\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Plotting a normal distribution\npenguins %>% filter(species == \"Chinstrap\") %>% ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Chinstrap penguins\") + \n  geom_density(fill = \"darkred\") + \n  labs(subtitle = paste0(\"N=\", penguins %>%\n                           filter(species == \"Chinstrap\") %>% nrow())) +\n  theme_bw()\n\n\n\n\nAs you can see, the data distribution closely resembles a ‚Äúbell-shaped‚Äù curve. On closer look, you can also see that the area under the curve is almost symmetrical to both sides and there is only a single peak present. We can also visualize the variance of the data by looking at the width of the curve. A wider curve means variance is higher and a narrower curve means variance in the data is smaller. Also for a normal distribution, there are no outliers present. We saw earlier that the mean, median and mode for a normal distribution are the same.\nAs seen earlier, we can use the rnorm() function to sample from the normal distribution. The syntax for the function is as follows;\nrnorm(number of samples, mean, sd)\nMany real-life datasets closely resemble a normal distribution, like the one which is plotted above. Since they approximate a normal distribution, we can use different base functions in R to answer some interesting questions.\n\nWhat percentage of Chinstrap penguins are below 3500g? To get this answer we approximate the body mass distribution of Chinstrap penguins to a normal distribution and use the pnrom() function.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins below 3500g\npnorm(3500, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n\n[1] 0.2721009\n\n\nSo there are about 27% of penguins who have their body mass below 3500g\n\nWhat percentage of Chinstrap penguins are above 4000g? To get this answer we again use the pnrom() function but use the lower.tail = F argument.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins above 4000g\npnorm(4000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass,\n      lower.tail = F)\n\n[1] 0.2436916\n\n\nThere are about 24% of penguins in the dataset have a body mass above 4000g.\n\nWhat percentage of Chinstrap penguins have their body masses between 3000g and 4000g? To find this answer, we first find the percentage of penguins who have masses below 3000g and then below 4000g. Then we subtract them to get our answer.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins between 3000g and 4000g\nbelow_3000 <- pnorm(3000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\nbelow_4000 <- pnorm(4000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n\nbelow_4000 - below_3000\n\n[1] 0.7280752\n\n\nAbout 73% of penguins have body masses between 3000g and 4000g.\n\nAt what body mass is 60% of the penguins weigh lower than? We can get the answer using the qnorm() function.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Body mass at which 60% of the penguins weigh lower than\nqnorm(0.6, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n\n[1] 3830.458\n\n\nWe find that 60% of the penguins weigh lower than 3830g.\n\nAt what body mass is 30% of the penguins weigh greater than? We can get the answer using the qnorm() function.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Body mass at which 30% of the penguins weigh greater than\nqnorm(0.3, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass, lower.tail = F)\n\n[1] 3934.634\n\n\nWe find that 30% of the penguins weigh greater than 3935g.\n\n7.3 Standard normal distribution\nA normal distribution with mean (\\bar{x}) = 0 and variance (\\sigma^{2}) = 1 is called standard normal distribution or a z-distribution. With the help of standard deviation, we can split the area under the curve of standard normal distribution into three parts. This partitioning of the area is known as the 68‚Äì95‚Äì99.7 rule.\nWhat it means is that;\n\n68% of the data will lie within ¬±\\sigma from \\bar{x} or they lie within 1 standard deviation from the mean\n95% of the data will lie within ¬±2\\sigma from \\bar{x} or they lie within 2 standard deviations from the mean\n99.7% of the data will lie within ¬±3\\sigma from \\bar{x} or they lie within 3 standard deviations from the mean\n\n\nCodelibrary(ggplot2)\n\n# Plotting a standard normal distribution\nggplot(data.frame(x = c(-4,4)), aes(x)) +\n  # Plot the pdf\n  stat_function(\n    fun = dnorm,\n    n = 101, args = list(mean = 0, sd = 1),\n    geom = \"area\", color = \"grey75\", fill = \"grey75\", alpha = 0.4) +\n  # Shade below -2\n  stat_function(\n    fun = function(x) ifelse(x <= -2, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade below -1\n  stat_function(\n    fun = function(x) ifelse(x <= -1, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade above 2\n  stat_function(\n    fun = function(x) ifelse(x >= 2, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade above 1\n  stat_function(\n    fun = function(x) ifelse(x >= 1, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  ggtitle(\"The standard normal distribution\") +\n  xlab(expression(italic(z))) +\n  ylab(expression(paste(\"Density, \", italic(f(z))))) +\n  geom_text(x = 0.5, y = 0.25, size = 3.5, fontface = \"bold\",\n            label = \"34.1%\") +\n  geom_text(x = -0.5, y = 0.25, size = 3.5, fontface = \"bold\",\n            label = \"34.1%\") +\n  geom_text(x = 1.5, y = 0.05, size = 3.5, fontface = \"bold\",\n            label = \"13.6%\") +\n  geom_text(x = -1.5, y = 0.05, size = 3.5, fontface = \"bold\",\n            label = \"13.6%\") +\n  geom_text(x = 2.3, y = 0.01, size = 3.5, fontface = \"bold\",\n            label = \"2.1%\") +\n  geom_text(x = -2.3, y = 0.01, size = 3.5, fontface = \"bold\",\n            label = \"2.1%\") +\n  geom_vline(xintercept=0, col = \"red\") +\n  annotate(\"text\", x=-0.25, y=0.15, label=\"Mean(xÃÖ) = 0\", angle=90) +\n  theme_bw()\n\n\n\n\n\nThe lightest grey area contains 68% (~ 34.1 + 34.1) of the data and it corresponds to \\bar{x}¬±\\sigma.\nThe second lightest grey area contains 95% (~ 34.1 + 34.1 + 13.6 + 13.6) and it corresponds to \\bar{x}¬±2\\sigma.\nThe darkest grey area contains 99.7% (~ 34.1 + 34.1 + 13.6 + 13.6 + 2.1 + 2.1) and it corresponds to \\bar{x}¬±3\\sigma.\nFor a standard normal distribution; the area under the curve is 1, the mean is 0 and the variance is 1.\n\n7.4 Poisson distribution\nPoisson distribution describe poisson processes. A Poisson process is when events happen at a certain rate but are completely random. For example; the number of people visiting the hospital, the number of candies sold in a shop, and the number of meteors falling on earth in a year. Thus the Poisson distribution describes the probability of some number of events happening over a fixed time. Thus the Poisson distribution is only applicable for datasets containing 0 and positive integers. It won‚Äôt work with negative or decimal values.\nThe Poisson distribution has the same value for its mean and variance and is denoted by \\lambda.\nAs seen in earlier distributions, the function to sample from a Poisson distribution is rpois(). Given below is an example of a Poisson distribution for \\lambda = 3, where the number of events ranges from 0 to 10.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a poisson distribution\ndata.frame(events = 0:10, \n           pdf = dpois(x = 0:10, lambda = 3)) %>%\nggplot(aes(x = factor(events), y = pdf)) +\n  geom_col() +\n  geom_text(\n    aes(label = round(pdf,2), y = pdf + 0.01),\n    position = position_dodge(0.9),\n    size = 3,\n    vjust = 0\n  ) +\n  labs(title = \"Probability of X = x Events\",\n       subtitle = \"Poisson distribution (Œª = 3)\",\n       x = \"Events (x)\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\nLet us take a scenario where the number of people visiting the hospital per week is 25. This is a Poisson process and we can model this scenario using a Poisson distribution. Here \\lambda = 25. As seen earlier, we can use different base functions in R to answer different questions concerning the Poisson distribution.\n\nWhat is the probability that 20 people will visit the hospital in a week given that 25 people on average visit the hospital in a week?\n\n\n# Calculating the probability that 20 people will visit the hospital in a week\ndpois(20, lambda = 25)\n\n[1] 0.05191747\n\n\nWe get a 5.2% chance that 20 people will visit the hospital in a week.\n\nWhat is the probability that 15 or fewer people will visit the hospital given that 25 people on average visit the hospital in a week? To get the answer to this question, we use the ppois() function.\n\n\n# Calculating the probability that 15 or fewer people will visit the hospital in a week\nppois(15, lambda = 25)\n\n[1] 0.02229302\n\n\nThere is a 2.2% chance that 15 or fewer people will visit the hospital.\n\nWhat is the probability that more than 5 people will visit the hospital given that 25 people on average visit the hospital in a week? To get the answer to this question, we use the ppois() function but with the lower.tail = F argument.\n\n\n# Calculating the probability that more than 5 people will visit the hospital in a week\nppois(5, lambda = 25, lower.tail = F)\n\n[1] 0.9999986\n\n\nWe get a 100% chance that more than 5 people will visit the hospital.\n\n7.5 Exponential distribution\nThe exponential distribution describes the probability distribution of time between events in a Poisson process. Exponential distribution can be used to predict the probability of waiting 10 minutes between two visitors in a hospital, the probability of elapsing 5 minutes between the sale of two candies, probability of having 6 months between two meteor showers on earth.\nSome real-life examples which exhibit exponential distribution are bacterial growth rate, oil production, call duration, and my parent‚Äôs patience (quickly decaying curve).\nThe exponential distribution is also described by the same parameter \\lambda as that of the Poisson distribution but it‚Äôs measured as a ‚Äòrate‚Äô.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a exponential distribution\nx <- seq(0, 20, length.out=1000)\ndat <- data.frame(time =x, px=dexp(x, rate=0.6))\n\nggplot(dat, aes(x=x, y=px)) +\n  geom_line() +\n  labs(title = \"Probability of X = x Time\",\n       subtitle = \"Exponential distribution (Œª = 0.6 or rate = 0.6)\",\n       x = \"Time\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\nYou can use the function rexp() to sample across an exponential distribution, similarly use dexp() and pexp() to find the probability of events like shown for other distributions.\n\n7.6 Student‚Äôs t distribution\nThe student‚Äôs t distribution or simply called the t distribution is a probability distribution similar to a normal distribution but is estimated with a low sample size collected from a population whose standard deviation is unknown. The parameter in estimating the t-distribution is called the degrees of freedom.\n\nCode# Plotting the t distribution and comparing to the standard normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = function(x) dt(x, df = 2),\n                aes(color = \"t\")) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1),\n                aes(color = \"normal\")) +\n  labs(title = \"Student's t-distribution vs Standard normal distribution\",\n       subtitle = \"t-distribution (degrees of freedom = 2)\",\n       x = \"t or z\",\n       y = \"Probability\") +\n  scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\")) +\n  theme_bw()\n\n\n\n\nIn the graph shown above, the t-distribution with degrees of freedom = 2 is plotted alongside the standard normal distribution. You can see that both the tail ends of the t-distribution are thicker as compared to the normal distribution and hence for the t-distribution the values are more away from the mean. But as the degrees of freedom increase, the t-distribution tends to become similar to that of a normal distribution.\n\nCode# Plotting the t distribution and comparing to the standard normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = function(x) dt(x, df = 2),\n                aes(color = \"t_2\")) +\n  stat_function(fun = function(x) dt(x, df = 25),\n                aes(color = \"t_25\")) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1),\n                aes(color = \"normal\")) +\n  labs(title = \"Student's t-distributions vs Standard normal distribution\",\n       subtitle = \"t-distribution (df = 2 and df = 25)\",\n       x = \"t or z\",\n       y = \"Probability\") +\n  scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\", \"darkgreen\")) +\n  theme_bw()\n\n\n\n\nIn the above graph, you can see that the t-distribution with degrees of freedom = 25 (green) is very similar to the standard normal distribution.\n\n7.7 Log-normal distribution\nThe log-normal distribution is a probability distribution of variable ‚Äòx‚Äô whose log-transformed values follow a normal distribution. Like the normal distribution, log-normal distribution has a mean value and a standard deviation which are estimated from log-transformed values. Some real-life examples which follow the log-normal distribution are; the length of chess games, blood pressure in adults, and the number of hospitalizations in the 2003 SARS outbreak.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a log normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = dlnorm, args = list(meanlog = 2.2, sdlog = 0.44), \n                colour = \"red\") +\n  labs(title = \"Log normal distribution\",\n       subtitle = \"Log normal distribution [mean_log = 2.2 and sd_log = 0.44]\",\n       x = \"x\",\n       y = \"Probability\") +\n  theme_bw()"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#the-central-limit-theorem",
    "href": "tutorials/stat_basic/intro_stat.html#the-central-limit-theorem",
    "title": "Introductory statistics with R",
    "section": "\n8 The central limit theorem",
    "text": "8 The central limit theorem\nImagine we are rolling a die 5 times and we are calculating the mean of the results.\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times\nsample_of_5 <- sample(die, 5, replace = T)\nsample_of_5\n\n[1] 1 1 5 6 2\n\n# Calculating the mean fo the results\nmean(sample_of_5)\n\n[1] 3\n\n\nNow imagine we are repeating the experiment of rolling a die 5 times for 10 trials and then we are calculating the mean for each trial.\n\nlibrary(dplyr)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repreating it 10 times\nsample_of_5 <- replicate(10, sample(die, 5, replace = T) %>% mean())\n\n# Mean values for the 10 trials\nsample_of_5\n\n [1] 3.2 4.0 2.4 4.8 4.8 3.2 3.0 4.8 3.2 3.6\n\n\nLet us go further and repeat this experiment for 100 and 1000 trials and visualize the means.\n\n\nFor 10 trials\nFor 100 trials\nFor 1000 trials\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(10, sample(die, 5, replace = T) %>% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(100, sample(die, 5, replace = T) %>% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(1000, sample(die, 5, replace = T) %>% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n\n\n\n\n\n\n\nYou can see that as the number of trials increases, the distribution of means reaches a normal distribution. This is a result of the central limit theorem.\n\nThe central limit theorem states that a sampling distribution of a statistic will approach a normal distribution as the number of trials increases, provided that the samples are randomly sampled and are independent.\n\nIn the above case, we can see that the mean of the samples approaches the central value or the ‚Äòexpected value‚Äô of 3.5.\nThe central limit theorem also applies to other statistics such as the standard deviation.\n\n\nFor 10 trials\nFor 100 trials\nFor 1000 trials\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(10, sample(die, 5, replace = T) %>% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(100, sample(die, 5, replace = T) %>% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(1000, sample(die, 5, replace = T) %>% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n\n\n\n\n\n\n\nThus as a result of the central limit theorem, we can take multiple samples from a large population and estimate different statistics which would be an accurate estimate of the population statistic. Thus we can circumvent the difficulty of sampling the whole population and still be able to measure population statistics and make useful inferences."
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#data-transformation",
    "href": "tutorials/stat_basic/intro_stat.html#data-transformation",
    "title": "Introductory statistics with R",
    "section": "\n9 Data transformation",
    "text": "9 Data transformation\nSometimes it is easier to visualise data after transforming it rather than trying to see it in its raw format, especially if we are working with skewed data. Given below are different datasets which have different skewness to them. For each data, the quantile-quantile plot is also plotted. The quantile-quantile plot or simply called the Q-Q plot plots the normal quantiles of the data distribution on the x-axis and the quantiles of the dataset on the y-axis. If our data is normally distributed, then the normal quantile and the data quantile would be the same and will be in a straight line. Deviation from this linear nature can be a result of the skewness of the dataset and can be visualized easily using a Q-Q plot.\n\n9.1 Normal distribution\nWe have seen what a normal distribution is, now let us look at its Q-Q plot.\n\n\nNormal distriubtion\nQ-Q plot\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a normal distribution\ndata.frame(events = 0:100000, \n           pdf = rnorm(n = 0:100000, 1)) %>%\n  ggplot(aes(pdf)) +\n  geom_density() +\n  labs(title = \"Normal distribution\",\n       x = \"x\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a q-q plot\ndata <- data.frame(events = 0:100000, \n           pdf = rnorm(n = 0:100000, 1))\nggplot(data, aes(sample = pdf)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot\",\n       subtitle = \"For a normal distribution, Q-Q plot is a straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n\n\n\n\n\n\n\nIf our data is normally distributed then it is easier to do statistical analyses with it. Finding a correlation between two normally distributed variables also becomes straightforward. In R, we can use the cor() function to find the correlation estimate between two variables. The correlation coefficient (r) lies between -1 and 1. The magnitude of the ‚Äòr‚Äô denotes the strength of the relationship and the sign denotes the type of relationship.\n\nCorrelation coefficient (r) = 0: No relationship between the two variables\nCorrelation coefficient (r) = -1: Strong negative relationship between the two variables\nCorrelation coefficient (r) = 1: Strong positive relationship between the two variables\n\n\ndata <- data.frame(x = 0:100, \n           y = 100:200)\n\n# Plotting x and y\nggplot(data, aes(x,y)) + geom_point() +\n  labs(title = \"Relationship between x and y\") +\n  theme_bw()\n\n\n\n# Finding correlation between x and y\ncor(data$x, data$y)\n\n[1] 1\n\n\nWe have a linear relationship between x and y. And we got a correlation coefficient value of 1 (in real life scenario this is next to impossible to obtain). But there are instances where we would not get a straightforward linear relationship between two variables and we would have to transform our data to make it easier to find the relationship. Before getting into data transformation, let us see different types of skewed distribution and plot their Q-Q plots.\n\n9.2 Negative or left-skewed distribution\nGiven below are graphs showing negative or left-skewed distribution, and its Q-Q plot.\n\n\nNegative or left-skewed distribution\nQ-Q plot\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a negative or left skewed distribution\ndata.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 5, 2)) %>%\n  ggplot(aes(y)) +\n  geom_density() +\n  labs(title = \"Negative or left skewed distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_bw()\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a q-q plot\ndata <- data.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 5, 2))\nggplot(data, aes(sample = y)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot for negative or left skewed distribution\",\n       subtitle = \"The tail ends of Q-Q plot is bend towards the right side of the straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n\n\n\n\n\n\n\nYou can see that for negative or left-skewed distribution, the tail ends of the Q-Q plot are bent towards the right side of the straight line.\n\n9.3 Positive or right-skewed distribution\n\n\nPositive or right-skewed distribution\nQ-Q plot\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a positive or right skewed distribution\ndata.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 2, 5)) %>%\n  ggplot(aes(y)) +\n  geom_density() +\n  labs(title = \"Positive or right skewed distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_bw()\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a q-q plot\ndata <- data.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 2, 5))\nggplot(data, aes(sample = y)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot for positive or right skewed distribution\",\n       subtitle = \"The tail ends of Q-Q plot is bend towards the left side of the straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n\n\n\n\n\n\n\nYou can see that for positive or right-skewed distribution, the tail ends of the Q-Q plot bend towards the left side of the straight line.\n\n9.4 Different types data transformations\nDepending on the nature of the dataset, different transformations can be applied to better visualize the relationship between two variables. Given below are different data transformation functions which are used depending on the skewness of the data (S 2010).\n\nFor right-skewed data, the standard deviation is proportional to the mean. In this case, logarithmic transformation (log(x)) works best.\nIf the variance is proportional to the mean (in the case of Poisson distributions), square root transformation (\\sqrt{x}) is preferred. This happens more in the case of variables which are measured as counts e.g., number of malignant cells in a microscopic field, number of deaths from swine flu, etc.\nIf the standard deviation is proportional to the mean squared, a reciprocal transformation (\\frac{1}{x}) can be performed. Reciprocal transformation is carried out for highly variable quantities such as serum creatinine.\nOther transformations include the square transformation (x^2) and exponential transformation (e^x) which can be used for left skewed data.\n\n\nDifferent types of data transformations and their usage criteria\n\n\n\n\n\n\nTransformation\nWhen is it used?\nWhen it cannot be used?\n\n\n\nLogarithmic transformation(log(x))\nScaling large values to small valuesand making them fit a normal distribution\nCannot be used when there are 0 and negative values.Can be circumvented by subtracting the whole datasetwith the min value in the database\n\n\nSquare root transformation(\\sqrt{x})\nTo inflate small values and to stabilize large values\nNot applicable to negative values\n\n\nReciprocal transformation(\\frac{1}{x})\nHighly varying data\nNot applicable when there are zero values"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#conlusion",
    "href": "tutorials/stat_basic/intro_stat.html#conlusion",
    "title": "Introductory statistics with R",
    "section": "\n10 Conlusion",
    "text": "10 Conlusion\nWe have completed the basics of statistics and also learned how to implement them in R. In summary we learned about;\n\nDescriptive and Inferential statistics.\nMeasures of centre: mean, median and mode\nMeasures of spread: variance, standard deviation, mean absolute deviation and interquartile range\nDistributions: binomial, normal, standard normal, Poisson, exponential, student‚Äôs t and log-normal\nCentral limit theorem\nData transformation: Skewness, Q-Q plot and data transformation functions\n\nIn the next chapter will we see how to use R for hypothesis testing."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html",
    "href": "tutorials/stat_model/glmer_stat_model.html",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "",
    "text": "TL;DR\n\n\n\nIn this article you will learn;\n\nWhat is a hierarchical model/mixed effects model?\nWhat are fixed effects and random effects?\nBuilding a linear mixed effects model\nAdding random intercept group and random effect slope\nRandom effect syntaxes used in lme4 packages\nPlotting linear mixed effects model, the residuals and the confidence intervals\nModel comparison using ANOVA\nBuilding generalized mixed effects models: Logistic and Poisson\nPlotting generalized mixed effects models\nRepeated measures data\nPaired t-test and repeated measures ANOVA is a special case of mixed effects model"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#prologue",
    "href": "tutorials/stat_model/glmer_stat_model.html#prologue",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n1 Prologue",
    "text": "1 Prologue\nThis is the fourth tutorial in the series: Statistical modelling using R. Over the past three tutorials we learned about linear models and generalized linear models. In this tutorial, we will learn how to build and understand both linear mixed effect models and generalised linear mixed effect models. They are needed to analyse datasets which have nested structures. We will learn more about it in the coming sections."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/glmer_stat_model.html#making-life-easier",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Run the following lines of code\n\n# Packages used in this tutorial\ntutorial_packages <- rlang::quos(AER, broom, lme4, broom.mixed, ggeffects,\n                                 lmerTest, datasets)\n\n# Install required packages\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  install.packages,\n  character.only = TRUE\n)\n\n# Loading the libraries\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  library,\n  character.only = TRUE\n)\n\n# Datasets used in this tutorial\ndata(\"CASchools\")\ndata(\"CreditCard\")\ndata(\"RecreationDemand\")\ndata(\"sleep\")"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#what-is-a-hierarchical-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#what-is-a-hierarchical-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n3 What is a hierarchical model?",
    "text": "3 What is a hierarchical model?\nConsider a dataset of test scores of students in different classrooms across different schools. If we take a closer look into this dataset, we can see that students can be grouped into their respective genders. This group of genders will be ‚Äònested‚Äô in the group of students. Likewise the student group is nested inside the group of classrooms which is further nested into the group of schools. This is shown in the flowchart given below (Use the scroll bar below the flowchart to navigate).\n\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\nflowchart TB\n  A[Education] --> B(School 1)\n  A --> C(School 2)\n  A --> D(School 3)\n  B --> E(Classroom 1)\n  B --> F(Classroom 2)\n  C --> G(Classroom 1)\n  D --> H(Classroom 1)\n  D --> I(Classroom 2)\n  D --> J(Classroom 3)\n  E --> K(Males: 30, Females: 25)\n  F --> L(Males: 15, Females: 45)\n  G --> M(Males: 23, Females: 10)\n  H --> N(Males: 10, Females: 40)\n  I --> O(Males: 0, Females: 30)\n  J --> P(Males: 45, Females: 0)\n  K --> Q(Test scores)\n  L --> R(Test scores)\n  M --> S(Test scores)\n  N --> T(Test scores)\n  O --> U(Test scores)\n  P --> V(Test scores)\n  subgraph G_1[Schools]\n  B\n  C\n  D\n  end\n  style G_1 fill:#f77f00\n  subgraph G_2[Classrooms]\n  E\n  F\n  G\n  H\n  I\n  J\n  end\n  style G_2 fill:#fcbf49\n  subgraph G_3[Students]\n  K\n  L\n  M\n  N\n  O\n  P\n  end\n  style G_3 fill:#eae2b\n  subgraph G_4[Test Scores]\n  Q\n  R\n  S\n  T\n  U\n  V\n  end\n  style G_4 fill:#f8f1ae\n\n\n\n\n\n\n\n\nStudent test scores are nested under the group of students. Students themselves form a group of males and females. Building from this, students are nested under classrooms. Now classrooms follow the same route and group themselves inside schools which further forms yet another group. This is an example of nested data or hierarchical data where the data is nested within itself or has a well-defined hierarchy within itself.\nLet us take a scenario where we are introducing a new study method to students which have the potential to raise test scores. We can train students using the new method and compare the test scores to the earlier test scores before the introduction of the novel method.\nOur goal is to see if the new study method imparts an effect on the test scores of students and we hypothesise that it will improve the scores. But because of the nature of the data, the test scores can also be affected by the sample size of the students, the sex of the student, the classroom and the school, all of which can impact the test scores. Further, we are sampling the scores of the same students over time after the introduction of the novel test method. This makes the test scores a repeated measure variable and they are not independent across time. Thus our dataset is both nested and sampled across time.\nSo how do we test if the novel study method affects the test scores by accounting for nestedness and repeated measures?\nWe can see that linear models won‚Äôt do a good job at this. Then, from our understanding of generalized linear models, we might be able to tackle this question. Since we test scores which count data, we can use the Poisson model to predict ‚Äòtest scores‚Äô using ‚Äòtest method‚Äô as our main explanatory variable and use other variables like student sex, classroom size, school etc. as covariates. But in doing so we are also including the variances in test scores resulting from all the covariates present in our data. This is where mixed effect models or hierarchical models shine as they treat the covariates as ‚Äòrandom effects‚Äô and pool shared information on the means of the test scores across different groups. Mixed effect models try to account for the nestedness of the data by eliminating some of the variance brought by the random effect variables on the response variable by ‚Äòcontrolling‚Äô them. Also, since we have different sample sizes for classroom students, classrooms with lower sample sizes will be impacted more by outliers. Thus treating classroom size as a random effect can mitigate this problem.\nHere, the ‚Äòrandom effects‚Äô are the variables; sex of the student, classroom size, and school and the ‚Äòfixed effect‚Äô, which is the variable that we are interested in, is the study method. A statistical model which has both random and fixed effect variables is called a ‚Äòmixed effect‚Äô model or a ‚Äòhierarchical‚Äô model.\nThe definitions of random effect and fixed effect are ambiguous in the scientific literature and there are multiple definitions of what constitutes a random effect and a fixed effect (Gelman 2005).\nGelman (2005) suggest parting ways with the terms ‚Äòrandom‚Äô and ‚Äòfixed‚Äô and propose to view them as constant and varying;\n\nWe define effects (or coefficients) in a multilevel model as constant if they are identical for all groups in a population and varying if they are allowed to differ from group to group. For example, the model y_{ij} = Œ±_j +Œ≤x_{ij} (of units i in groups j ) has a constant slope and varying intercepts, and y_{ij} = Œ±_j + Œ≤_j x_{ij} has varying slopes and intercepts. (Gelman 2005)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#building-a-mixed-effects-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#building-a-mixed-effects-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n4 Building a mixed-effects model",
    "text": "4 Building a mixed-effects model\nWe will start with a linear mixed-effects model. As seen before, we use this if our residuals follow a normal or simply if our data is normally distributed. If we have count data or binomial data which is not normally distributed, then instead of building a linear mixed effects model we build a generalized linear mixed effects model. We will use the lme4 package in R to build these models.\nWe will be using the CASchools dataset from the {AER} package in R. The dataset contains test scores which are on the Stanford 9 standardized test administered to 5th-grade students. The dataset has values for 420 elementary schools and the test scores are given as the mean score in reading and their ability to do maths. Other variables correspond to the school characteristics which we will see later along the way."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#linear-mixed-effects-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#linear-mixed-effects-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n5 Linear mixed effects model",
    "text": "5 Linear mixed effects model\nWith the CASchools dataset, we are interested in seeing if the reading scores (read) are predicted by the school expenditure (expenditure). Let us start by plotting the data.\n\nif (!require(AER)) install.packages('AER')\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Plotting the data\nggplot(CASchools, aes(expenditure, read)) + geom_point() +\n  theme_bw() + labs(title = \"Reading scores vs Expenditure\",\n                    x = \"Expenditure per student (in dollars)\",\n                    y = \"Average reading score\")\n\n\n\n\nThere does not seem to be a general trend that higher expenditure leads to higher scores. Also, most counties tend to have expenditures between 4500 and 5500 dollars.\nNow let us try to build a linear model to predict the average reading score (read) with expenditure per student (expenditure) using the lm() function.\n\nif (!require(broom)) install.packages('broom')\nlibrary(AER)\nlibrary(broom)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm <- lm(read ~ expenditure, data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lm)\n\n\n\n  \n\n\n\nFrom the coefficient values, we can see that the slope value for expenditure per student is almost zero. This was more or less visible to us from the plot. So does it mean that expenditure has no association with the average reading scores? Before jumping to a conclusion, if we take a closer look at the data, we can see that the scores are nested within counties (county). So let us try adding the variable county to the linear model.\n\nlibrary(AER)\nlibrary(broom)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm <- lm(read ~ expenditure + county, data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lm)\n\n\n\n  \n\n\n\nThere are a total of 45 counties, so we get 44 intercept values and a global intercept value for the first county in the dataset (which is Alameda). We can also see that the slope for expenditure decreased furthermore and the standard error for the slope estimate also decreased as compared to the previous model. As for the intercepts for the respective counties, all counties as compared to Alameda county have negative intercepts. Now let us check the coefficients for the respective counties without any base county comparisons.\n\nlibrary(AER)\nlibrary(broom)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm <- lm(read ~ expenditure + county - 1, data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lm)\n\n\n\n  \n\n\n\nThe coefficient for Alameda county is the greatest out of all the counties, which is why we got all the intercept values as negative in the earlier case. Also, the p-value associated with expenditure (p = 0.54) is greater than 0.05 suggesting that expenditure does not explain the variance seen in the average reading scores.\n\n5.1 Adding a random intercept\nWhat if we only want to see how expenditure affects the reading score without the county differences? Let us build a linear mixed effects model with county as the ‚Äòrandom effect‚Äô and ‚Äòexpenditure‚Äô as the ‚Äòfixed effect‚Äô. To build a linear mixed effects model we use the function lmer() from the lme4 package in R.\nThe syntax for building a linear mixed effects model is very similar to the syntax we used while using the lm() and glm() functions. Here we are considering county as a random effect, moreover, the county variable is categorical and therefore, we are essentially considering county as a random intercept.\nThe notation for specifying random effect within the model formula is (1 | county). If we do not specify a random effect term in the model formula, the lmer() function won‚Äôt run. Without further ado, let‚Äôs see it in action.\nThe tidy() function from the broom package in R only work for models built using lm() and glm(). To extend its usage to mixed effects models, we have to install an additional package called broom.mixed\n\nif (!require(lme4)) install.packages('lme4')\nif (!require(broom.mixed)) install.packages('broom.mixed')\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lmer)\n\n\n\n  \n\n\n\nIf we look at the slope for expenditure, it is greater than the slope value we got from the linear model but still is almost zero. The standard error for the estimated slope is also less as compared to the earlier built models. But there is a strong role of counties on the average reading score of stundets. How did I know that? Let us first compare the summary of the linear model to that of the linear mixed effects model using the summary() function.\n\nsummary(model_lmer)\n\n\n\nSummary of the linear mixed effects model\n\n\n\nIn section 1, we can see the model formula that we used to model the data. It also tells us that the model has been fitted by REML, which stands for restricted maximum likelihood method, the method by which lmer models fit the data.\nIn section 2, there is a value denoting the REML criterion at convergence. I don‚Äôt know exactly what this is, so if I learn more about it and understand it, then I will update this text. From what I read so far, I learned that this value can be a helpful model diagnostic if our model is not ‚Äòconverging‚Äô.\nSection 3 outputs the quantile, min-max and median values of the residual distances of the model.\nIn section 4, the variance and the standard deviation of both the random intercept variable (county) and the residual (as explained by expenditure) are given. You can see that the variance of counties is more than half the variance of the residual. This variance value is meant to capture all the influences of counties on the average reading score. If we estimate the percentage of variance county contributes as compared to the total variance, then we have;\n\n\ncounty_variance = 138.5\nresidual_variance = 253.7\ntotal_variance = county_variance + residual_variance\n\n# Calculating the percentage of variance of county to the total variance\ncounty_variance / total_variance\n\n[1] 0.3531362\n\n\nSo the differences between counties explain ~35% of the variance that‚Äôs ‚Äúleftover‚Äù after the variance explained by our fixed effects (which is expenditure here). If we had county variance as 0, then the percentage of the variance of counties to the total variance would be 0, which suggests that county is truly a random variable which does not explain the variance seen in the average reading scores.\n\nIn section 5, we have the coefficient estimates of the fixed effect variable, something which we have seen extensively in the previous tutorials. You can see that the slope of expenditure (0.001955) is close to zero which indicates that expenditure does not explain the differences seen in the average reading score. We saw the same case when we built a linear model using the same dataset.\nIn section 6, intercept and expenditure coefficients correlate -0.966. This is another value which I am not sure what it means. From what I read, it tells us that if we were to repeat the analysis using a newly sampled dataset, then with newly estimated coefficients of the fixed effect variable, the correlation value tells us how many of those fixed effect coefficients might be associated. You can read more about it here.\n\nNow that we got a sense of the model summary of a linear mixed effects model, let us see some more functions in R that would be useful for extracting information from the model.\n\nTo extract the coefficients of the fixed effect variables, use the fixef()\n\n\n\n# Extracting the coefficients of the fixed effects\nfixef(model_lmer)\n\n (Intercept)  expenditure \n6.454923e+02 1.954755e-03 \n\n\n\nTo extract the coefficients of the random effect variables, use the ranef()\n\n\n\n# Extracting the coefficients of the fixed effects\nranef(model_lmer)\n\n$county\n                 (Intercept)\nAlameda          11.87352086\nButte            -7.99357120\nCalaveras         2.53892633\nContra Costa     15.33621739\nEl Dorado        10.40224040\nFresno          -17.67664299\nGlenn             6.54693608\nHumboldt          7.30873017\nImperial        -13.90694256\nInyo              2.52932318\nKern            -16.86794250\nKings            -6.14912869\nLake             -7.25773124\nLassen            6.16396434\nLos Angeles      -9.68442015\nMadera           -2.26427074\nMarin            22.22849626\nMendocino        -5.18932165\nMerced          -17.55856455\nMonterey        -13.20359371\nNevada           10.83326539\nOrange           -2.23662764\nPlacer           11.61203226\nRiverside       -11.18209338\nSacramento      -13.86645226\nSan Benito       -4.01536933\nSan Bernardino   -5.97196372\nSan Diego         3.11246174\nSan Joaquin      -9.12283927\nSan Luis Obispo   6.55996266\nSan Mateo        11.56548692\nSanta Barbara     9.16868646\nSanta Clara       8.23860811\nSanta Cruz       15.92725798\nShasta            3.42269880\nSiskiyou         -3.53860484\nSonoma           10.40368948\nStanislaus        1.60937097\nSutter           -0.00741143\nTehama           -1.72590741\nTrinity          10.21145799\nTulare          -16.47382765\nTuolumne          1.76305887\nVentura          -7.73323819\nYuba              4.27007248\n\nwith conditional variances for \"county\" \n\n\n\nTo calculate the confidence intervals of the fixed effect variables, use the confint()\n\n\n\n# Calculating the confidence intervals of the fixed effects\nconfint(model_lmer)\n\n                    2.5 %       97.5 %\n.sig01       8.886716e+00 1.526132e+01\n.sigma       1.483855e+01 1.711469e+01\n(Intercept)  6.298888e+02 6.608656e+02\nexpenditure -8.340782e-04 4.796441e-03\n\n\nBy default the confint() function calculates the confidence intervals at 95% level. The displayed result from the function is symmetric over the 95% level. The interpretation of 95% CI for expenditure, in this case, would be; that if we were to repeat the experiment 100 times, then, the 95 of those mean values of the slope would fall between -0.000834 and 0.004796. We have a rather wide confidence interval which includes both a negative and a positive limit which shows that we cannot say whether expenditure decrease or increase the average reading scores.\n\nWe can also use the tidy() function by loading both the broom and broom.mixed packages to display the estimates and the confidence intervals.\n\n\nlibrary(broom)\nlibrary(broom.mixed)\n\n# Calculating the estimates and the confidence intervals of the model\ntidy(model_lmer, conf.int = T)\n\n\n\n  \n\n\n\nYou can see that the random effect variables do not have standard error values. If we compare the model summary between the linear model and the linear mixed effects model we built, we can also see that p-values are not reported for the linear mixed effects model. The author of the lme4 package, Dr.¬†Doug Bates views random effects as latent variables which do not have standard deviations and thus standard errors or p-values are not calculated for them. In addition to this, it is an open research question on how to calculate the p-values for random effects in a mixed-effects model. But using the lmerTest package in R we can perform ad-hoc analyses to report p-values for the fixed effect variables. We will see how to do this further down the section.\n\n5.2 Adding a random slope\nSo far we have seen how to input random intercepts, now we will see how to input random slopes. The default syntax for random effect in R is (continuous_predictor | random_effect_group). Thus a random slope is calculated, it also estimates a random effect intercept.\nIn the CASchools dataset, we looked at whether the average reading score is predicted by the expenditure by treating counties as the random intercept group. Now in our datasets, we also have data on the number of students and this could be different for different counties and can affect the average reading score. So this time let us build a model by treating students as the random effect variable.\n\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model using random slope and random intercept\nmodel_lmer2 <- lmer(read ~ expenditure + (students | county), data = CASchools)\n\n# Extracting coefficients from the model\nsummary(model_lmer2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: read ~ expenditure + (students | county)\n   Data: CASchools\n\nREML criterion at convergence: 3593.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.67995 -0.64452  0.00953  0.66738  2.52830 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.  Corr \n county   (Intercept) 3.475e+02 18.642621      \n          students    1.354e-06  0.001164 -0.95\n Residual             2.342e+02 15.303481      \nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 6.467e+02  7.706e+00  83.918\nexpenditure 6.341e-04  1.413e-03   0.449\n\nCorrelation of Fixed Effects:\n            (Intr)\nexpenditure -0.958\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe variance of students is very less and therefore might not be affecting the scores that much. Also, note that we got an error message in the output saying boundary (singular) fit. This generally indicates that the estimate for the random effect variable that we used is very small. In our, it means that the random effect slopes of students are very small which we can see using the ranef() function. So we can remove students from our model.\n\nranef(model_lmer2)\n\n$county\n                (Intercept)      students\nAlameda          24.4785039 -1.441929e-03\nButte            -3.4180179  1.775858e-04\nCalaveras         7.9826401 -4.681762e-04\nContra Costa     25.4891245 -1.424787e-03\nEl Dorado        17.9072929 -1.012206e-03\nFresno          -14.0623193  8.308013e-04\nGlenn            12.4209156 -7.319719e-04\nHumboldt         13.8328480 -8.081011e-04\nImperial        -12.2869252  7.016373e-04\nInyo              7.7750632 -4.532749e-04\nKern            -12.5277014  4.649777e-04\nKings            -1.0528421  1.423524e-05\nLake             -5.6736709  3.339620e-04\nLassen           12.5326580 -7.406065e-04\nLos Angeles      -2.0794607 -2.426704e-04\nMadera            2.8489240 -1.656228e-04\nMarin            34.7443486 -2.051422e-03\nMendocino        -4.6943124  2.762367e-04\nMerced          -14.2569750  7.113461e-04\nMonterey         -8.4968704  2.996187e-04\nNevada           18.6703498 -1.085232e-03\nOrange            8.6097926 -6.360256e-04\nPlacer           19.4641692 -1.099682e-03\nRiverside       -10.6949200  6.195711e-04\nSacramento      -12.6097930  7.103399e-04\nSan Benito        0.1126231 -3.550850e-05\nSan Bernardino    2.9274439 -3.988533e-04\nSan Diego        14.1621214 -8.451098e-04\nSan Joaquin      -6.2010253  3.738934e-04\nSan Luis Obispo  13.5823846 -7.993061e-04\nSan Mateo        24.7224461 -1.651387e-03\nSanta Barbara    20.4963141 -1.417024e-03\nSanta Clara      22.7310750 -1.376439e-03\nSanta Cruz       26.5648214 -1.605235e-03\nShasta           10.2142367 -6.282100e-04\nSiskiyou          2.5622804 -1.403162e-04\nSonoma           18.0499478 -1.078872e-03\nStanislaus        9.0155040 -5.928210e-04\nSutter            4.5333780 -2.714554e-04\nTehama            3.3731884 -2.120293e-04\nTrinity          19.7049606 -1.160587e-03\nTulare          -12.6199437  7.048954e-04\nTuolumne          7.6541244 -4.484524e-04\nVentura          -1.4092318 -1.002546e-04\nYuba             10.9917466 -6.477161e-04\n\nwith conditional variances for \"county\" \n\n\n\n5.3 Adding a non-correlated random effects\nPlease also note that in the model summary given above, a correlation value (-0.95) is given for the correlation between students and county. If we don‚Äôt want to specify any correlation between the random effects then instead of using | we use || to specify no correlation.\n\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model (no correlation between random effects)\nmodel_lmer3 <- lmer(read ~ expenditure + (students || county), data = CASchools)\n\n# Extracting coefficients from the model\nsummary(model_lmer3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: read ~ expenditure + ((1 | county) + (0 + students | county))\n   Data: CASchools\n\nREML criterion at convergence: 3592.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.73060 -0.62225  0.01112  0.63847  2.40069 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n county   (Intercept) 1.311e+02 11.44819\n county.1 students    2.434e-06  0.00156\n Residual             2.400e+02 15.49234\nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 6.503e+02  7.738e+00  84.039\nexpenditure 1.321e-03  1.401e-03   0.943\n\nCorrelation of Fixed Effects:\n            (Intr)\nexpenditure -0.966\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 8.66886 (tol = 0.002, component 1)\nModel is nearly unidentifiable: very large eigenvalue\n - Rescale variables?\nModel is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\n\nThe model outputs a lot of error values because students and county is correlated as seen before.\n\n5.4 Adding the same variable as both fixed and random effect\nSometimes we can have a model with both the fixed effect and the random effect as the same variable. Suppose we are interested in seeing the average effect of expenditure on the average reading scores across counties. In that case, we use expenditure both as a fixed effect and as a random effect. Before using expenditure in the model formula, it is better to use the scale() function to rescale the data values for better numerical stability.\n\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Rescaling expenditure values\nCASchools_scaled <- CASchools\nCASchools_scaled$expenditure_scaled <- scale(CASchools_scaled$expenditure)\n\n# Building a linear mixed effects model (same fixed and random effect)\nmodel_lmer4 <- lmer(read ~ expenditure + (expenditure | county), data = CASchools)\n\n# Extracting coefficients from the model\nsummary(model_lmer4)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: read ~ expenditure + (expenditure | county)\n   Data: CASchools\n\nREML criterion at convergence: 3563.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.63568 -0.62932  0.02378  0.64384  2.60189 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.  Corr \n county   (Intercept) 6.566e+02 25.623717      \n          expenditure 3.880e-05  0.006229 -0.94\n Residual             2.254e+02 15.014211      \nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 6.534e+02  9.100e+00  71.801\nexpenditure 3.237e-04  1.828e-03   0.177\n\nCorrelation of Fixed Effects:\n            (Intr)\nexpenditure -0.976\noptimizer (nloptwrap) convergence code: 0 (OK)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\n\n\nThe table below shows all the different types of random effect syntaxes we can use in the lmer() function.\n\nDifferent random effect syntaxes used in the lme4 package\n\nRandom effect syntax\nWhat it means\n\n\n\n(1 | group)\nRandom intercept with fixed mean\n\n\n(1 | g1/g2)\nIntercepts vary among g1 and g2 within g1\n\n\n(1 | g1) + (1 | g2)\nRandom intercepts for 2 variables\n\n\nx + (x | g)\nCorrelated random slope and intercept\n\n\nx + (x || g)\nUncorrelated random slope and intercept"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#plotting-lmer-models",
    "href": "tutorials/stat_model/glmer_stat_model.html#plotting-lmer-models",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n6 Plotting lmer models",
    "text": "6 Plotting lmer models\nEarlier we had functions like fmodel() from the {statisticalModeling} package and arguments like geom_smooth() or stat_smooth() from the ggplot2 package in R to help us plot linear models and generalized linear models. Thanks to the ggeffects package in R, we have an easy of plotting our lmer models. But first, let us see how we got it with the ggplot2 package in R.\nWe will have to do some manual tidying and wrangling of our model object to extract values which we need to plot using the ggplot2 package.\nTo plot the trend line, we can use the predict() function to predict values using the training dataset itself and save those predicted values into a new column in the dataset. Then plot the trend lines them using those predicted values.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm <- lm(read ~ expenditure + county, data = CASchools)\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Saving predicted values for each model\ndata <- CASchools %>% mutate(predicted_lm = predict(model_lm),\n                             predicted_lmer = predict(model_lmer))\n\n# Plotting the predicted values as lines\nggplot(data, aes(x = expenditure, y = read, color = county)) +\n  geom_point() +\n  geom_line(aes(x = expenditure, y = predicted_lm)) +\n  geom_line(aes(x = expenditure, y = predicted_lmer), linetype = 'dashed') +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Expenditure per student (in dollars)\",\n       y = \"Average reading score\") +\n  theme_bw()\n\n\n\n\nWe have 54 different counties which make the plot somewhat overwhelming.\nWe can also plot the predicted values as data points in the plot. Let us build another model without any groups using the same dataset. We will use the number of teachers in the school (teachers) as a random effect instead of the counties.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm <- lm(read ~ expenditure + teachers, data = CASchools)\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + (1 | teachers), data = CASchools)\n\n# Saving predicted values for each model\ndata <- CASchools %>% mutate(predicted_lm = predict(model_lm),\n                             predicted_lmer = predict(model_lmer))\n\n# Plotting the predicted values as points\nggplot(data, aes(x = expenditure, y = read)) +\n  geom_point() +\n  geom_point(data = data,\n             aes(x = expenditure, y = predicted_lm),\n             color = \"red\", alpha = 0.5) +\n  geom_point(data = data,\n             aes(x = expenditure, y = predicted_lmer),\n             color = 'blue', alpha = 0.5) +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Expenditure per student (in dollars)\",\n       y = \"Average reading score\") +\n  theme_bw()\n\n\n\n\nWith the ggeffects package, plotting a lmer model is as easy as two lines of code. Since we have 54 counties, I am not plotting them via counties, so it is not included in the terms = \"c()\" argument.\n\nif (!require(ggeffects)) install.packages('ggeffects')\nlibrary(lme4)\nlibrary(AER)\nlibrary(ggeffects)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Predicting values\nmodel_lmer_predict <- ggpredict(model_lmer, terms = c(\"expenditure\"))\n\n# Plotting the model\nplot(model_lmer_predict)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#plotting-the-residulas",
    "href": "tutorials/stat_model/glmer_stat_model.html#plotting-the-residulas",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n7 Plotting the residulas",
    "text": "7 Plotting the residulas\nWe can use the base plot() function to plot the residuals of the models.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm <- lm(read ~ expenditure + county, data = CASchools)\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Plotting the linear model diagnostics\npar(mfrow=c(2,2))\nplot(model_lm)\n\n\n\n# Plotting the linear mixed effects model diagnostics (just the residuals actually)\nplot(model_lmer)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#plotting-the-cofidence-intervals",
    "href": "tutorials/stat_model/glmer_stat_model.html#plotting-the-cofidence-intervals",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n8 Plotting the cofidence intervals",
    "text": "8 Plotting the cofidence intervals\nWe can also plot the confidence intervals of the fixed effect variables using the ggplot() function.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Extracting the coefficients\nmodel_lmer_coef <- tidy(model_lmer, conf.int = T)\nmodel_lmer_coef1 <- model_lmer_coef %>% \n  filter(effect == \"fixed\" & term != \"(Intercept)\")\n\n# Plotting the 95% CI\nmodel_lmer_conf <- ggplot(model_lmer_coef1, aes(x = term, y = estimate,\n                             ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, color = 'red') + \n  geom_point() +\n  geom_linerange() +\n  coord_flip() +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Regression coefficient\",\n       y = \"Coefficient estimate and 95% CI\") +\n  theme_bw()\n\nmodel_lmer_conf\n\n\n\n\nLet us try including more fixed effect terms in the model. We will include the number of teachers in the school (teachers) and the number of students in the school (students) as fixed effects.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + teachers + students + (1 | county),\n                   data = CASchools)\n\n# Extracting the coefficients\nmodel_lmer_coef <- tidy(model_lmer, conf.int = T)\nmodel_lmer_coef1 <- model_lmer_coef %>% \n  filter(effect == \"fixed\" & term != \"(Intercept)\")\n\n# Plotting the 95% CI\nmodel_lmer_conf <- ggplot(model_lmer_coef1, aes(x = term, y = estimate,\n                             ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, color = 'red') + \n  geom_point() +\n  geom_linerange() +\n  coord_flip() +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Regression coefficient\",\n       y = \"Coefficient estimate and 95% CI\") +\n  theme_bw()\n\nmodel_lmer_conf"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#null-hypothesis-testing",
    "href": "tutorials/stat_model/glmer_stat_model.html#null-hypothesis-testing",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n9 Null hypothesis testing",
    "text": "9 Null hypothesis testing\nAs mentioned earlier, p values are not calculated for fixed terms using the lme4 package for various reasons. So if we have to calculate the p values, we have to use the lmerTest package in R. By loading the lmerTest package, the lme4 package is automatically loaded. We don‚Äôt have to provide any special syntax for calculating the p-values, instead, p-values are automatically calculated when calling the lmer() function.\n\nif (!require(lmerTest)) install.packages('lmerTest')\nlibrary(lmerTest)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer <- lmer(read ~ expenditure + teachers + students + (1 | county),\n                   data = CASchools)\n\n# Printing model summary\nsummary(model_lmer)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: read ~ expenditure + teachers + students + (1 | county)\n   Data: CASchools\n\nREML criterion at convergence: 3590.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7343 -0.6384  0.0471  0.6343  2.2823 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n county   (Intercept) 138.6    11.77   \n Residual             239.6    15.48   \nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept) 652.737953   7.767375 412.474266  84.036   <2e-16 ***\nexpenditure   0.001005   0.001404 410.295073   0.715   0.4749    \nteachers      0.085024   0.058332 391.351530   1.458   0.1458    \nstudents     -0.005188   0.002825 392.820325  -1.836   0.0671 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) expndt techrs\nexpenditure -0.964              \nteachers     0.141 -0.163       \nstudents    -0.152  0.170 -0.997\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nFor fixed effects, p-values are calculated to test if the coefficients are significantly different from zero."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#model-comparison-using-anova",
    "href": "tutorials/stat_model/glmer_stat_model.html#model-comparison-using-anova",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n10 Model comparison using ANOVA",
    "text": "10 Model comparison using ANOVA\nWe can use ANOVA to compare two models to see which one is better. The ANOVA test is called by the function anova() and it is used to compare models by comparing the amount of variability explained by each model.\nLet us test if the number of teachers significantly affects the average reading score of students across different counties. First, we will build a ‚Äònull model‚Äô with just county as the random effect. Then we will build another model with teachers as the fixed effect and the rest the same as the previous model.\n\nlibrary(lme4)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model with only random effect (null model)\nmodel_lmer_null <- lmer(read ~ (1 | county),\n                   data = CASchools)\n\n# Building a linear mixed effects model with fixed effect\nmodel_lmer_teacher <- lmer(read ~ teachers + (1 | county),\n                   data = CASchools)\n\n# Comparing models using ANOVA\nanova(model_lmer_null, model_lmer_teacher)\n\n\n\n  \n\n\n\nFrom the ANOVA result, it seems like adding the variable teacher made the model better as it yielded significant p-values."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#generalized-linear-mixed-effects-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#generalized-linear-mixed-effects-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n11 Generalized linear mixed effects model",
    "text": "11 Generalized linear mixed effects model\nSame way as we made the transition from linear models to generalized linear models, we now transition from linear mixed effects model to generalized mixed effects model. As you might have guessed, we will be dealing with datasets which do not follow the normal distribution and are in the form of a count or binomial type.\nWe use the glmer() function to build generalized linear mixed effects models (glmer models) and use the family = \" \" argument to specify the distribution of residuals of the data. If it‚Äôs non-normal and is count then we use family = \"poisson\" and if it‚Äôs binomial then we use family = \"binomial\".\n\n11.1 Logistic mixed effects model\nFor this exercise, we will be using the CreditCard dataset from the {AER} package in R. The dataset contains the credit history of a sample of applicants for a type of credit card. We will be building a glmer model to predict the success of the acceptance of the credit card application (card) using yearly income (income) as the fixed effect and the house ownership status (owner) as the random effect.\nLet us first build a plot and visualize the data before building the model.\n\nlibrary(ggplot2)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Plotting the data\nggplot(CreditCard, aes(income, card, col = owner)) +\n  geom_jitter(width = 0, height = 0.05) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw()\n\n\n\n\nIt seems like individuals with low yearly income tend to have the most number of credit card application rejections.\nNow let us build the model.\n\nlibrary(lme4)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic mixed effects model\nmodel_glmer_logistic <- glmer(card ~ income + (1 | owner), family = \"binomial\",\n                           data = CreditCard)\n\n# Printing the summary of the model\nsummary(model_glmer_logistic)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: card ~ income + (1 | owner)\n   Data: CreditCard\n\n     AIC      BIC   logLik deviance df.resid \n  1384.1   1399.6   -689.0   1378.1     1316 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2271  0.3708  0.4626  0.6115  0.6626 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n owner  (Intercept) 0.09638  0.3105  \nNumber of obs: 1319, groups:  owner, 2\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.99357    0.27913   3.560 0.000371 ***\nincome       0.09610    0.04779   2.011 0.044338 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nincome -0.564\n\n\nFrom a quick look, it seems like income does significantly explain the variance seen in the success of accepting a credit card application.\nWe learned from logistic GLMs that it‚Äôs easier to interpret the coefficients as odds ratio by exponentiating them. The same principle follows here.\n\nlibrary(lme4)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic mixed effects model\nmodel_glmer_logistic <- glmer(card ~ income + (1 | owner), family = \"binomial\",\n                           data = CreditCard)\n\n# Extracting the odds ratios\ntidy(model_glmer_logistic, exponentiate = T)\n\n\n\n  \n\n\n\nWe can see that the odds ratio for income is very close to 1, which is why the p-value associated with income is close to the level of significance of 0.05. So there is a weak effect on income in deciding the success of accepting credit card applications. There might be some other variables that we are missing to incorporate into the model.\n\n11.2 Plotting a logistic mixed effects model\nAs we saw in the case with the linear mixed effects model, we will be using the ggeffects package.\n\nlibrary(lme4)\nlibrary(ggeffects)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic mixed effects model\nmodel_glmer_logistic <- glmer(card ~ income + (1 | owner), family = \"binomial\",\n                           data = CreditCard)\n\n# Predicting values\nmodel_glmer_logistic_predict <- ggpredict(model_glmer_logistic, \"income\")\n\n# Plotting the model\nplot(model_glmer_logistic_predict)\n\n\n\n\n\n11.3 Poisson mixed effects model\nFor this exercise, we will be using the RecreationDemand dataset from the {AER} package in R. The data is on the number of recreational boating trips to Lake Somerville, Texas, in 1980, based on a survey administered to 2,000 registered leisure boat owners in 23 counties in eastern Texas, USA.\nWe will be predicting the number of boating trips (trips) using the annual household income of the respondent (income) as the fixed effect and water-skiing status at the lake (ski) as the random effect.\nLet us first build a plot and visualize the data before building the model.\n\nlibrary(ggplot2)\nlibrary(AER)\n\ndata(\"RecreationDemand\")\n\n# Plotting the data\nggplot(RecreationDemand, aes(income, trips, col = ski)) + \n  geom_jitter(width = 0.05, height = 0) +\n  labs(title = \"Does income affect the number of recreational boating trips?\",\n       x = \"Annual household income of the respondent (in 1,000 USD)\",\n       y = \"Number of recreational boating trips\") +\n  theme_bw()\n\n\n\n\nThe graph tells us that most boating trips are made by people with less annual household income. Please note that there are many zeros in the dataset, essentially we have a zero-inflated dataset. But for now, we will ignore it. Skiing status doesn‚Äôt seem to affect the number of boating trips made.\nNow let us build the model and see.\n\nlibrary(lme4)\nlibrary(AER)\n\ndata(\"RecreationDemand\")\n\n# Building a poisson mixed effects model\nmodel_glmer_poisson <- glmer(trips ~ income + (1 | ski), family = \"poisson\",\n                           data = RecreationDemand)\n\n# Printing the summary of the model\nsummary(model_glmer_poisson)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: trips ~ income + (1 | ski)\n   Data: RecreationDemand\n\n     AIC      BIC   logLik deviance df.resid \n  5454.7   5468.2  -2724.4   5448.7      656 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-2.192 -1.506 -1.291 -0.242 56.932 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ski    (Intercept) 0.09025  0.3004  \nNumber of obs: 659, groups:  ski, 2\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.42434    0.22221   6.410 1.46e-10 ***\nincome      -0.15402    0.01674  -9.199  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nincome -0.269\n\n\nSeems like income has a significant negative impact on the number of boating trips as seen earlier in the graph.\n\n11.4 Plotting a Poisson mixed effects model\nAgain we will be using the ggeffects package.\n\nlibrary(lme4)\nlibrary(ggeffects)\nlibrary(AER)\n\ndata(\"RecreationDemand\")\n\n# Building a poisson mixed effects model\nmodel_glmer_poisson <- glmer(trips ~ income + (1 | ski), family = \"poisson\",\n                           data = RecreationDemand)\n\n# Predicting values\nmodel_glmer_poisson_predict <- ggpredict(model_glmer_poisson,\n                                         terms = c(\"income\", \"ski\"))\n\n# Plotting the model\nplot(model_glmer_poisson_predict)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#repeated-measures-data",
    "href": "tutorials/stat_model/glmer_stat_model.html#repeated-measures-data",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n12 Repeated measures data",
    "text": "12 Repeated measures data\nIf we measure variables across time for the same set of observations then essentially we have repeated measures data. Repeated measures are a special case of a mixed-effects model. Examples of repeated measures include; tracking test scores of students across an academic year, measuring drug effects on patients over time, measuring the growth of bacteria over time, measuring flower phenology over different seasons etc.\nWe will be using the sleep dataset from the {datasets} package in R. The data show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients. In this case, we are measuring sleep difference in hours across time (extra) for two groups (group), essentially repeated measured data.\nFirst, let us visualize the dataset.\n\nif (!require(datasets)) install.packages('datasets')\nlibrary(datasets)\nlibrary(ggplot2)\n\ndata(\"sleep\")\n\n# Plotting the graph\nggplot(sleep, aes(group, extra, group = ID)) + geom_line() +\n  labs(title = \"Effect of two soporific drugs on sleep in 10 patients\",\n       x = \"Drug group\",\n       y = \"Increase in hours of sleep compared to control\") +\n  theme_bw()\n\n\n\n\nThe graph shows that drug 2 as compared to drug 1 increases the amount of sleep in hours when compared to control. Now let us see if this difference is statistically significant.\nIn introductory statistic classes, we would have learned that to compare means across time for this ‚Äòpaired‚Äô dataset, we can use a paired t-test. Performing a t-test in R is through the t.test() function. If we use t.test(paired = T) then essentially we are doing a paired t.test. Let us do a paired t-test on the above-mentioned dataset.\n\nlibrary(datasets)\n\ndata(\"sleep\")\n\n# Performing a paired t.test\nt.test(extra ~ group, paired = T, data = sleep)\n\n\n    Paired t-test\n\ndata:  extra by group\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\nThe t-test results suggest a significant difference between the mean of the sleep differences and the difference between the mean of the sleep difference between the two groups is -1.58. In other words, group 2 has a 1.58 value greater mean as compared to group 1.\nNow let us run a linear mixed effects model with extra as the response variable, group as the fixed effect and the ID as the random effect.\n\nlibrary(datasets)\n\ndata(\"sleep\")\n\n# Building a linear mixed effect model\nsummary(lmer(extra ~ group + (1 | ID), data = sleep))\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: extra ~ group + (1 | ID)\n   Data: sleep\n\nREML criterion at convergence: 70\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.63372 -0.34157  0.03346  0.31511  1.83859 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 2.8483   1.6877  \n Residual             0.7564   0.8697  \nNumber of obs: 20, groups:  ID, 10\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)   \n(Intercept)   0.7500     0.6004 11.0814   1.249  0.23735   \ngroup2        1.5800     0.3890  9.0000   4.062  0.00283 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\ngroup2 -0.324\n\n\nIf we closely compare the paired t-test result and the summary of the linear mixed effect model, we can notice some similarities.\n\nThe test statistic t-value is the same in both the results if we ignore the sign (4.062).\nThe p-value is the same (0.00283)\nThe coefficient for the group2 is the same as the mean difference denoted in the paired t-test result after ignoring the sign (1.5800).\n\nPaired t-test is a special case of repeated measures ANOVA. If we have only two dependent groups and if it‚Äôs normally distributed, then we use the paired t-test to compare their means. If we have more than 2 dependent groups, then we go repeated measures ANOVA. On the other hand, repeated measures ANOVA is a special case of the mixed effects model.\nSo to compare the means for paired data, we can go for two different statistical approaches;\n\nRegression analysis\nANOVA type approach\n\nWe already saw the regression analysis approach which is what we did earlier. Here, we build a linear mixed model and then examine if the drug group coefficient differs greatly from zero. In our case, the coefficient for group2 is 1.58 and is significantly greater than 0. So we can directly see that group2 significantly affects extra hours of sleep as compared to group1, without the need for any post hoc test which is needed if we are running an ANOVA.\nSo the null and alternate hypotheses for the regression analysis of our data would be;\n\n\nH_0 = Drug group coefficient is zero\n\nH_a = Drug group coefficient is not zero\n\nIn the second approach, we examine if the drug group covariate explains a significant amount of variability within the model. There will be no effect on the drug group if the amount of sleep the study participants get in both groups is similar. ANOVA will only tell that there is a significant difference between some groups in the model, to see which groups differ from each other, we have to go for a post hoc test.\nSo the null and alternate hypothesis for the ANOVA type approach for our data would be;\n\n\nH_0 = Drug group does not significantly explain the differences in extra sleep hours\n\nH_a = Drug group significantly explain the differences in extra sleep hours\n\nGoing forward with the ANOVA type approach can be as easy as using the function anova() by giving the model as the input.\n\nlibrary(datasets)\n\ndata(\"sleep\")\n\n# Building a linear mixed effects model\nsleep_model <- lmer(extra ~ group + (1 | ID), data = sleep)\n\n# Performing a ANOVA\nanova(sleep_model)\n\n\n\n  \n\n\n\nAs we have seen earlier, the drug group does significantly explain the differences in extra sleep hours of the patients. We can also see that the p-values calculated are identical to that of the model summary we saw before."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#fixed-effect-models-lm-and-glm-vs-mixed-effects-models-lmer-and-glmer",
    "href": "tutorials/stat_model/glmer_stat_model.html#fixed-effect-models-lm-and-glm-vs-mixed-effects-models-lmer-and-glmer",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n13 Fixed effect models (lm and glm) vs Mixed effects models (lmer and glmer)",
    "text": "13 Fixed effect models (lm and glm) vs Mixed effects models (lmer and glmer)\nWe have reached the final level of the tutorial. Here we will compare the fixed effects models and mixed effects models and get an overall picture of the differences between them.\n\nReview of fixed effects model and mixed effects model\n\n\n\n\n\nFixed effects model\nMixed effects model\n\n\n\nGenerally more impacted by outliers\nMore robust to outliers\n\n\nUse lm() or glm() in base R\nUse lmer() or glmer() from lme4 package in R\n\n\nEasier to explain\nComputationally harder to fit\n\n\nEasier to build\nMore robust to small group sizes\n\n\n\nModels nested distributions"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#conlusion",
    "href": "tutorials/stat_model/glmer_stat_model.html#conlusion",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n14 Conlusion",
    "text": "14 Conlusion\nFinally, we have completed the most needed things in statistical modelling that concerns a PhD student, especially if that student is studying Ecology. In summary, we learned the following things;\n\nWhat is a hierarchical model/mixed effects model?\nWhat are fixed effects and random effects?\nBuilding a linear mixed effects model\nAdding random intercept group and random effect slope\nRandom effect syntaxes used in lme4 packages\nPlotting linear mixed effects model, the residuals and the confidence intervals\nModel comparison using ANOVA\nBuilding generalized mixed effects models: Logistic and Poisson\nPlotting generalized mixed effects models\nRepeated measures data\nPaired t-test and repeated measures ANOVA is a special case of mixed effects model\n\nNow where to go from here? Normally, whatever is covered till now will be enough for most purposes as a student of science. Now if you the reader is still interested in learning more, then I welcome you to learn ‚ÄòNon-linear Modelling in R with Generalized Additive Models (GAM)‚Äô which would raise our modelling repertoire to even higher levels. Congratulations for making it this far, I wish you all the best üëç"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html",
    "href": "tutorials/stat_model/glm_stat_model.html",
    "title": "Generalized linear models in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() => null)"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#prologue",
    "href": "tutorials/stat_model/glm_stat_model.html#prologue",
    "title": "Generalized linear models in R",
    "section": "\n1 Prologue",
    "text": "1 Prologue\nThis is the third tutorial in the series: Statistical modelling using R. In this tutorial we will work with datasets that fail the assumptions of linear models. We will first see what these assumptions are and then we will learn about generalised linear models which is an extension of the linear model architecture. So let‚Äôs go!"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/glm_stat_model.html#making-life-easier",
    "title": "Generalized linear models in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Run the following lines of code\n\n# Packages used in this tutorial\ntutorial_packages <- rlang::quos(Stat2Data, modelsummary, devtools, COUNT, broom,\n                                 cherryblossom, datasets)\n\n# Install required packages\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  install.packages,\n  character.only = TRUE\n)\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading the libraries\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  library,\n  character.only = TRUE\n)\n\n# Datasets used in this tutorial\ndata(\"HorsePrices\")\ndata(\"rwm1984\")\ndata(\"run17\")\ndata(\"mtcars\")"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#understanding-the-coeffecients-in-a-linear-model",
    "href": "tutorials/stat_model/glm_stat_model.html#understanding-the-coeffecients-in-a-linear-model",
    "title": "Generalized linear models in R",
    "section": "\n3 Understanding the coeffecients in a linear model",
    "text": "3 Understanding the coeffecients in a linear model\nTo quickly recap, a linear model tries to explain the variability seen in the response variable by estimating the coefficients for explanatory variables. These coefficients are nothing but your effect size or the slope of the explanatory variable calculated from the model. Consider the following example.\nWe will use the HorsePrices dataset from the Stat2Data package in R. We will model the price of the horses in this dataset to their height and age. We will use the summary() function to output the summary of the linear model.\n\nif (!require(Stat2Data)) install.packages('Stat2Data')\nif (!require(statisticalModeling)) install.packages('statisticalModeling')\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm <- lm(Price ~ Age + Sex, data = HorsePrices)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"Price ~ Age + Height\")\n\n\n\n# Getting the summary of the linear model\nsummary(model_lm)\n\n\nCall:\nlm(formula = Price ~ Age + Sex, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24988 -10296  -1494   8286  27653 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18199.7     4129.3   4.407 6.03e-05 ***\nAge           -220.1      393.8  -0.559    0.579    \nSexm         17008.6     3639.6   4.673 2.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12540 on 47 degrees of freedom\nMultiple R-squared:  0.3283,    Adjusted R-squared:  0.2997 \nF-statistic: 11.48 on 2 and 47 DF,  p-value: 8.695e-05\n\n# Calculating the effect sizes\neffect_size(model_lm, ~ Age)\n\n\n\n  \n\n\n\nIn the summary of the model, in the coefficients section, the estimate column is nothing but effect sizes. Compare the estimate for ‚Äòage‚Äô in the model summary and the effect size of ‚Äòage‚Äô calculated from the effect_size() function, both are the same.\nRecall that the equation for a straight line is;\n y = mx+c  Where m is the slope of the line and c is the y-intercept.\nWe can see from the graph that there are two straight lines, therefore there will be two equations for the straight lines. Imagine y_1 to be the equation of the straight line for females (the red line) and similarly, y_2 is for males (the blue line). The equations for the straight lines are;\n y_1 = m_1x_1 + c_1 \\\\\ny_2 = m_2x_2 + c_2\nFrom the graph, you can see that the slopes for both lines are the same. This means that the effect size of age is the same across both the sex. Also, the slope should be a negative value because as age increases, the price is decreasing for both sexes. From the model summary, the slope value for age is -220.10. Thus we have;\nm_1 = m_2 = -220.1\nNow recall how I explained that the effect size for a categorical exploratory variable is expressed in terms of change and that change is in terms of their y-intercepts.\nNow R calculates intercepts in two ways. The default way or the first way is what is shown in the model summary. In the (Intercept) row, the estimated value is 18199.7 and this value is the y-intercept value for the straight line for ‚Äòfemale‚Äô. You can easily verify this from the red line in the graph. Notice how the y-intercepts are the same as shown in the model summary and the plot. Now in the row ‚ÄòSexm‚Äô, the estimated value shows how much difference is there from the estimated value shown in the (Intercept) row or otherwise how much difference in y-intercept is there between females and males. Since the difference is positive this means the y-intercept for males is 17008.6 higher than the y-intercept for females. Thus we have;\nc_1 = 18199.7\\\\c_2 - c_1 = 17008.6\nThe intercept value by default is shown for the first level in the exploratory variable which here is female.\nThe second way is to individually show the corresponding y-intercept values for males and females. For that, we can use the expression -1 in the model formula.\n\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building a linear model showing mean for each level of the response variable\nmodel_lm <- lm(Price ~ Age + Sex - 1, data = HorsePrices)\n\n# Getting the summary of the linear model\nsummary(model_lm)\n\n\nCall:\nlm(formula = Price ~ Age + Sex - 1, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24988 -10296  -1494   8286  27653 \n\nCoefficients:\n     Estimate Std. Error t value Pr(>|t|)    \nAge    -220.1      393.8  -0.559    0.579    \nSexf  18199.7     4129.3   4.407 6.03e-05 ***\nSexm  35208.2     3497.7  10.066 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12540 on 47 degrees of freedom\nMultiple R-squared:  0.8429,    Adjusted R-squared:  0.8329 \nF-statistic: 84.05 on 3 and 47 DF,  p-value: < 2.2e-16\n\n\nNow you get individual y-intercept values for all the levels in the data. There will be slight decimal differences while adding up the values but the concept is the same.\nc_1 = 18199.7\\\\c_2 = 35208.2 = 17008.6 + 18199.7\nIn algebraic form, the default model formula for a linear model used in lm() is;\ny \\sim \\beta_0 + \\beta_2x_2 + \\beta_3x_3 + .. + \\epsilon And when -1 is used in the model formula, then it becomes;\ny \\sim \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + .. + \\epsilon\nSo the first way compares differences between groups by taking the first group as the reference and giving a global intercept (\\beta_0) and in a second way, we can see the actual change in the respective groups as their respective coefficients (\\beta_1, \\beta_2 etc.)."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#limitations-of-linear-models",
    "href": "tutorials/stat_model/glm_stat_model.html#limitations-of-linear-models",
    "title": "Generalized linear models in R",
    "section": "\n4 Limitations of linear models",
    "text": "4 Limitations of linear models\nFour assumptions are required to be satisfied to be able to do linear modelling;\n\nThe values in the datasets are normally distributed\nThe residuals are normally distributed\nThe values in the datasets are continuous\nVariables form a linear relationship with each other\n\nMany datasets fail these assumptions. Survival data with binary responses or data with count values are both non-normal and discontinuous. Linear modelling won‚Äôt do any good in analysing these datasets. So what is the solution? The solution is to extend linear models and generalize them so that they can accept non-normal distributions. In R, the process is simple and we have seen it being used in logistic regression in tutorial 1 using the glm() function. Therefore glm() function calls for the generalized model in R and the type of model is specified by the distribution of the data. If the data is count type then we can use the Poisson family distribution and if the data is binomial then use the binomial family distribution. We will see both of these in detail further in the tutorial. The process of generalizing a linear model is through ‚Äònon-linear link functions‚Äô which link the regression coefficients to the distribution and allow the linear model to be generalised. We will learn more about this in the later sections.\nThe syntax for the glm() function is as follows;\n\nglm(formula, data, family = \"\")\n\nFor the Poisson regression model we can specify family = \"poisson\" and for the binomial regression model we can specify family = \"binomial\".\nSince the lm() function is only applicable for normal or Gaussian distributions;\nglm(formula, data, family = \"gaussian\") = lm(formula, data)\nTherefore a linear model is a generalized linear model with the Gaussian family."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#poisson-regression-model",
    "href": "tutorials/stat_model/glm_stat_model.html#poisson-regression-model",
    "title": "Generalized linear models in R",
    "section": "\n5 Poisson regression model",
    "text": "5 Poisson regression model\nAs mentioned earlier, the Poisson model is used if the dataset contains count data. Count data can be no of things sold in a shop, no people with cancer in a city or no of visitors in a shop. Intuitively, we can say that count data ranges from 0 to infinity. There is no negative or decimal count data. Thus your dataset should only contain 0 or positive integer values for the Poisson model to work. Also, the values in the dataset should have been obtained in the same manner. For example: let‚Äôs say your dataset contains count data for the no. of people coming to the shop per day, then the same dataset should not contain values for no. of people coming to the shop per week, as they are sampled differently in terms of time. Lastly, if your dataset has a variance greater than the mean or if you have lots of zeros in your data then some modifications have to be done to be able to use the Poisson regression model.\nIn short, you should not use the Poisson regression model if you have the following conditions;\n\nDataset contains negative and/or non-integer values\nDataset is sampled across in a different manner (no. of people per day vs.¬†no. of people per week)\nDataset has a variance greater than its mean (Over-dispersed data)\nDataset has lots of zero values (Zero-inflated data)\n\nLet us build a Poisson model. We will use the rwm1984 dataset from the {COUNT} package in R. The dataset is a German health registry for the year 1984. We are interested in seeing whether the number of visits to the doctor during a year (docvis) is associated with age (age). Since we have count data we will build a Poisson model to see this association.\n\nif (!require(COUNT)) install.packages('COUNT')\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model output\nsummary(model_poisson)\n\n\nCall:\nglm(formula = docvis ~ age, family = \"poisson\", data = rwm1984)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.229  -2.263  -1.271   0.326  26.383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.0983153  0.0399295  -2.462   0.0138 *  \nage          0.0273395  0.0008204  33.325   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25791  on 3873  degrees of freedom\nResidual deviance: 24655  on 3872  degrees of freedom\nAIC: 31742\n\nNumber of Fisher Scoring iterations: 6\n\n\nApart from the summary() function, we also use the tidy() function from the broom package in R to print the outputs of the model as a data frame.\n\nif (!require(broom)) install.packages('broom')\nlibrary(COUNT)\nlibrary(broom)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model output using tidy()\ntidy(model_poisson)\n\n\n\n  \n\n\n\nSometimes we are interested in extracting the coefficients or finding out the confidence intervals from the model.\n\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Extracting the coefficients\ncoef(model_poisson)\n\n(Intercept)         age \n-0.09831529  0.02733953 \n\n# Estimating the confidence interval\nconfint(model_poisson)\n\n                 2.5 %      97.5 %\n(Intercept) -0.1767936 -0.02027057\nage          0.0257330  0.02894895"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#logistic-regression-model",
    "href": "tutorials/stat_model/glm_stat_model.html#logistic-regression-model",
    "title": "Generalized linear models in R",
    "section": "\n6 Logistic regression model",
    "text": "6 Logistic regression model\nIn tutorial 1, we briefly saw what logistic regression was. We use the logistic model when;\n\nData is binary (0/1)\nData has only two mutually exclusive values (Alive/Dead or Yes/No or Win/Lose or Pass/Fail)\nData has only two values choices or behaviour (Non-veg/Veg, Wet/Dry)\n\nIn short, for a logistic regression model, the model outputs probabilities (p) for binary outcomes in the response variable (y).\ny = Binomial(p)\nLet us build a logistic model using run17 dataset from the cherryblossom package in R. We have seen this dataset in the first tutorial. The dataset contains details for all 19,961 runners in the 2017 Cherry Blossom Run, which is an annual road race that takes place in Washington, DC, USA. The Cherry Blossom Run has two events; a 10 Mile marathon and a 5 Km run. We will be building a logistic model to whether event choice is predicted by the participant‚Äôs age.\n\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(dplyr)\nlibrary(cherryblossom)\n\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model\nmodel_binomial <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Printing model output\nsummary(model_binomial)\n\n\nCall:\nglm(formula = event ~ age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1471   0.4963   0.5102   0.5285   0.6241  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.250155   0.074991  30.006  < 2e-16 ***\nage         -0.008390   0.001897  -4.423 9.74e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15130  on 19959  degrees of freedom\nResidual deviance: 15111  on 19958  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 15115\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n6.1 Bernouli and binomial distributions\nBoth Bernoulli and binomial distributions form the basis for logistic regression. The main difference between these two distributions is that Bernoulli is associated with probabilities of a result in a single event whereas binomial is associated with probabilities of a result in multiple events. For example; the chance of getting a head in a single coin toss follows a Bernoulli distribution and the chance of getting a head in 10 coin tosses follows a binomial distribution.\nIn R we have the option to either use binomial or Bernoulli. So how do we know when to use which? The short answer is that it depends on the data structure. For wide format data, the binomial distribution is used and for long format data, the Bernoulli distribution is used. You can learn more about long format and wide format data here.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Long format\nhead(run17_tidy)\n\n\n\n  \n\n\n# Converting the dataset to wide format\nrun17_wide <- run17_tidy %>% group_by(age, event) %>%\n  summarise(count = n())\n\nrun17_wide <- run17_wide %>% pivot_wider(names_from = \"event\", values_from = \"count\")\nrun17_wide[is.na(run17_wide)] <- 0\n\n# Wide format\nhead(run17_wide)\n\n\n\n  \n\n\n\nIn the above case, for long format data, we have a single entry in each row corresponding to individual data but in the wide format, we have each row entry corresponding to a single age group. Another way of expressing the data in a wide format is to calculate the percentage of choices, which is by dividing the total number of choices for the first category by the total number of choices for both categories.\nSo for wide format data; If our data structure has the absolute value of the no. of choices made then we use cbind() in the model formula (also called as the Wilkinson-Rogers format). If we have percentages, then we use weights = () in the glm() function.\nSo in summary we choose between Bernoulli and binomial by checking the following questions;\n\nIs the data in long or wide format?\nDo we have individual or group data?\nAre we interested in individuals or groups?\n\nLet us build logistic models for both long format and wide format data.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with long format\nmodel_logistic_long <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Converting the dataset to wide format\nrun17_tidy_1 <- run17_tidy %>% mutate(event_choice = ifelse(event == \"Ten_Mile\", 1, 0))\n\nrun17_wide <- run17_tidy_1 %>% group_by(age, event) %>%\n  summarise(count = n())\n\nrun17_wide <- run17_wide %>% pivot_wider(names_from = \"event\", values_from = \"count\")\nrun17_wide[is.na(run17_wide)] <- 0\n\n# Building a logistic model with wide format\n# We use cbind() to bind the two choices\nmodel_logistic_wide_1 <- glm(cbind(Five_Km, Ten_Mile) ~ age,\n                             data = run17_wide, family = \"binomial\")\n\n# Building a logistic model with wide format\n# We use percentage of choice and the weight or number of observations per group\nrun17_wide$prect_ten_mile <- run17_wide$Ten_Mile / (run17_wide$Ten_Mile + run17_wide$Five_Km)\n\nmodel_logistic_wide_2 <- glm(prect_ten_mile ~ age, \n                             data = run17_wide, family = \"binomial\",\n                             weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\n# Printing model outputs\nsummary(model_logistic_long)\n\n\nCall:\nglm(formula = event ~ age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1471   0.4963   0.5102   0.5285   0.6241  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.250155   0.074991  30.006  < 2e-16 ***\nage         -0.008390   0.001897  -4.423 9.74e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15130  on 19959  degrees of freedom\nResidual deviance: 15111  on 19958  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 15115\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(model_logistic_wide_1)\n\n\nCall:\nglm(formula = cbind(Five_Km, Ten_Mile) ~ age, family = \"binomial\", \n    data = run17_wide)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9381  -0.6116   0.4472   1.7556   7.5007  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.245070   0.074961 -29.950  < 2e-16 ***\nage          0.008267   0.001897   4.358 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 411.90  on 79  degrees of freedom\nResidual deviance: 393.17  on 78  degrees of freedom\nAIC: 711.65\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(model_logistic_wide_2)\n\n\nCall:\nglm(formula = prect_ten_mile ~ age, family = \"binomial\", data = run17_wide, \n    weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-7.5007  -1.7556  -0.4472   0.6116   2.9381  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.245070   0.074961  29.950  < 2e-16 ***\nage         -0.008267   0.001897  -4.358 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 411.90  on 79  degrees of freedom\nResidual deviance: 393.17  on 78  degrees of freedom\nAIC: 711.65\n\nNumber of Fisher Scoring iterations: 4\n\n\nIn summary we have three data formats for logistic regression\n\nBinary format\n\n\n# Binary: y = 0 or 1\nmodel_logistic_long <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n\nWilkinson-Rogers format (the ‚Äòcbind‚Äô format)\n\n\n# Wilkinson-Rogers: cbind(success, failure)\nmodel_logistic_wide_1 <- glm(cbind(Five_Km, Ten_Mile) ~ age,\n                             data = run17_wide, family = \"binomial\")\n\n\nWeighted format\n\n\n# Weighted format: y = 'proportions', weights = 'totals'\nmodel_logistic_wide_2 <- glm(prect_ten_mile ~ age, \n                             data = run17_wide, family = \"binomial\",\n                             weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\nPlease note that all three models give the same coefficients but the model with the long format structure has higher degrees of freedom as compared to the other two models with the wide format."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#link-functions",
    "href": "tutorials/stat_model/glm_stat_model.html#link-functions",
    "title": "Generalized linear models in R",
    "section": "\n7 Link functions",
    "text": "7 Link functions\nThe ‚Äòlink function‚Äô is what enables us to generalize a linear model so that we can use it in datasets which do not meet the assumptions of a linear model. In a way, it is a function which transforms the coefficients of the explanatory variables to form a linear combination with each other like in the case of a linear regression model.\n\nFor poisson family models, the link function is log and thus the coefficients of the model are in log-scale.\nFor ‚Äòbinomial‚Äô family models, there are two link functions; logit and probit\n\n\nThe logit link function transforms the probabilities to ‚Äòlog-odds values‚Äô which are real numbers ranging from -\\infty to +\\infty and gives rise to a linear equation similar to what is seen in a linear regression model. In logistic regression, by default, the logit link function is used.\nlogit(p) = m_1x_1 + c_1 = log(\\frac{p}{1-p}) The probit link function is also similar to the logit link but is based on the cumulative standard normal distribution function. By default, the logistic regression in the glm() function uses the logit link function.\nTo use another link function, we should specify the link function in the family;\nfamily = \"binomial\" = family = binomial(link = \"probit\"): By default, logit link function is used\nfamily = binomial(link = \"probit\"): Using the probit link function\nLook at the graphs given below. Use the slider to compare them. You can see that the model with the logit link function has a slightly longer or fatter tail, as both the tail ends reaches the limit slower as compared to the model with the probit link function. Therefore, in general, the logit link function is better at modelling outliers or rare events as compared to probit.\n\nCodelibrary(datasets)\nlibrary(statisticalModeling)\n\n# Building a logistic model with logit link which is the default\nmtcars_logit <- glm(vs ~ mpg, data=mtcars, family= \"binomial\")\n\n# Building a logistic model with probit link which is the default\nmtcars_probit <- glm(vs ~ mpg, data=mtcars, family=binomial(link=\"probit\"))\n\n# Plotting logistic model with logit link\nfmodel(mtcars_logit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with logit link)\")\n\n# Plotting logistic model with probit link\nfmodel(mtcars_probit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with probit link)\")\n\n\n\n\nlibrary(datasets)\nlibrary(statisticalModeling)\n\n# Building a logistic model with logit link which is the default\nmtcars_logit <- glm(vs ~ mpg, data=mtcars, family= \"binomial\")\n\n# Building a logistic model with probit link which is the default\nmtcars_probit <- glm(vs ~ mpg, data=mtcars, family=binomial(link=\"probit\"))\n\n# Plotting logistic model with logit link\nfmodel(mtcars_logit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with logit link)\")\n\n\n\n# Plotting logistic model with probit link\nfmodel(mtcars_probit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with probit link)\")"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#simulating-logit-and-probit",
    "href": "tutorials/stat_model/glm_stat_model.html#simulating-logit-and-probit",
    "title": "Generalized linear models in R",
    "section": "\n8 Simulating logit and probit",
    "text": "8 Simulating logit and probit\nWe can simulate a logit distribution using the plogis() and the rbinom() functions together. The rbinom() is a random number generator function which has additional input parameters;\nn = Number of random numbers to generate size = Number of trails p = Probability of success\n\n# Converting a logit scale value to probability value\np <- plogis(2)\n\n# Simulating a logit distribution\nrbinom(n = 10, size = 1, prob = p)\n\n [1] 1 1 1 1 1 0 1 1 1 1\n\n\nA probit distribution can be made using the pnrom() and the rbinom() functions together.\n\n# Converting a probit scale value to probability value\np <- pnorm(2)\n\n# Simulating a probit distribution\nrbinom(n = 10, size = 1, prob = p)\n\n [1] 1 1 1 1 1 1 1 1 1 1"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-poisson-model",
    "href": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-poisson-model",
    "title": "Generalized linear models in R",
    "section": "\n9 Understanding the coefficients in a Poisson model",
    "text": "9 Understanding the coefficients in a Poisson model\nThe link function for the Poisson regression model is the log function. Now we will see how to interpret the coefficients/effect sizes of a Poisson regression model. Let us quickly recall how we interpreted the coefficients/effect sizes of a linear regression model. We will use our earlier example but with only ‚ÄòSex‚Äô as the exploratory variable.\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm <- lm(Price ~ Sex, data = HorsePrices)\n\n# Getting the summary of the linear model\nsummary(model_lm)\n\n\nCall:\nlm(formula = Price ~ Sex, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-23730 -10061  -1255   8495  28495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    16505       2783   5.931 3.20e-07 ***\nSexm           17225       3593   4.794 1.62e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12450 on 48 degrees of freedom\nMultiple R-squared:  0.3238,    Adjusted R-squared:  0.3097 \nF-statistic: 22.98 on 1 and 48 DF,  p-value: 1.619e-05\n\n\nIn algebraic form, the model formula for a linear model is;\ny \\sim \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon From the model summary, we have;\ny = Price\\\\ \\beta_0 = 16505 = Reference\\,intercept\\,(female)\\\\ \\beta_1 = 17225 = Difference\\,from\\,the\\,reference\\,intercept\\,(for\\,male)\\\\ \\epsilon = Error\\,value\\,of\\,the\\,model\nWe also have;\n\\beta_0 + \\beta_1 = Average\\,price\\,of\\,male\\,horses This shows that the coefficient in a linear model is additive. But when we come to the Poisson model, the link function is a log function which takes in the parameter \\lambda of the Poisson distribution. We have;\nParameter\\,of\\,poisson\\,distribution:\\lambda = e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon}\\\\ Link\\,function: ln(\\lambda) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon\nHere the coefficients are no longer additive as they are in log scale. Therefore the coefficients are multiplicative. Let us look at the Poisson regression model we created earlier;\n\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model outputs\nsummary(model_poisson)\n\n\nCall:\nglm(formula = docvis ~ age, family = \"poisson\", data = rwm1984)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.229  -2.263  -1.271   0.326  26.383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.0983153  0.0399295  -2.462   0.0138 *  \nage          0.0273395  0.0008204  33.325   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25791  on 3873  degrees of freedom\nResidual deviance: 24655  on 3872  degrees of freedom\nAIC: 31742\n\nNumber of Fisher Scoring iterations: 6\n\n\nHere we have;\ny = No.\\,of\\,days\\,of\\,doctor\\,visits\\\\ \\beta_0 = -0.0983153 = Reference\\,intercept\\\\ \\beta_1 = 0.0273395 = Difference\\,from\\,the\\,reference\\,intercept\\\\ \\epsilon = Error\\,value\\,of\\,the\\,model\nNow to find the average effect of age on the no. of days of doctor visits, we have to multiply \\beta_0 and \\beta_1 instead adding the values like in the case of a linear model.\n\\beta_0 * \\beta_1 = ln(average\\,effect\\,of\\,age\\,on\\,the\\,no.\\,of\\,days\\,of\\,doctor\\,visits)\\\\ e^{\\beta_0 + \\beta_1} = average\\,effect\\,of\\,age\\,on\\,the\\,no.\\,of\\,days\\,of\\,doctor\\,visits\nBut if we exponentiate the coefficients, then we get the raw value, which we can add like in the case of a linear model."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#plotting-a-poisson-model",
    "href": "tutorials/stat_model/glm_stat_model.html#plotting-a-poisson-model",
    "title": "Generalized linear models in R",
    "section": "\n10 Plotting a Poisson model",
    "text": "10 Plotting a Poisson model\nYou can either use the fmodel() function from the {statisticalModeling} package or use the ggplot() function from the ggplot2 package.\nFor the ggplot() function, we specify ‚Äòpoisson‚Äô model in the method.args = list(family = \" \") parameter inside geom_smooth() argument.\n\nlibrary(COUNT)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Plotting using the fmodel() function\nfmodel(model_poisson) + geom_point(data = rwm1984) +\n  theme_bw() +\n  labs(title = \"docvis ~ age (poisson model)\")\n\n\n\n# Plotting using the ggplot() function\nggplot(data = rwm1984, aes(age, docvis)) +\n  geom_jitter(width = 0.05, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = 'poisson')) +\n  theme_bw() +\n  labs(title = \"docvis ~ age (poisson model)\")"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-logistic-model",
    "href": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-logistic-model",
    "title": "Generalized linear models in R",
    "section": "\n11 Understanding the coefficients in a logistic model",
    "text": "11 Understanding the coefficients in a logistic model\nIn a linear model, the coefficients were straight forward and in the Poisson model, after exponentiating the coefficients, the values were similar to that of the linear model. But for a logistic model transformation won‚Äôt help and it‚Äôs not as straightforward as compared to the other models. So instead, the coefficient values are converted to odds ratio. The odds ratio calculates the relative odds of two events occurring. For example, let us imagine a football team that is expected to win 10 games for every game they lose. Then the odds of winning the odds of losing would be 10 to 1. And the odds ratio would be 10/1 which is 10.\nAs mentioned earlier, the default link function for a logistic model is logit, so our coefficients start as ‚Äòlog-odds‚Äô which when exponentiated will give us the odds ratio.\nIn general, odds-ratio values are interpreted in the following way;\n\nOR = 1: Coefficient has no effect\nOR < 1: Coefficient decreased the odds\nOR > 1: Coefficient increases the odds\n\nImagine a football team is singing two players; A and B. Odds-ratio of winning is 3 if player A joins the team, which means, in that season, if player A joins, the team is bound to win 3 games for every game they lose. Likewise, if player B joins let‚Äôs say the odds ratio is 0.5, which means they will win 1 game for every 2 games they lose. The odds ratio is often reported along the 95% confidence intervals.\nLet us calculate the odds ratio and the confidence interval of the model we previously created. We will use the tidy() function from broom package in R.\n\nlibrary(dplyr)\nlibrary(broom)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model\nmodel_binomial <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Finding the odds ratio and confidence interval\ntidy(model_binomial, exponentiate = T, conf.int = T)\n\n\n\n  \n\n\n\nThe odds ratio of age to the event is ~ 0.9916 which is almost 1. This suggests that when age increases by 1 year, there 50% chance that the participant chooses 5Km over 10 Mile event run (the odds are 1 to 1). So basically age does not seem to have an association with event choice. Also note that for the 95% confidence interval, the upper bound almost includes 1 which further reinforces the notion that age is not associated with the event choice."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#plotting-a-logistic-model",
    "href": "tutorials/stat_model/glm_stat_model.html#plotting-a-logistic-model",
    "title": "Generalized linear models in R",
    "section": "\n12 Plotting a logistic model",
    "text": "12 Plotting a logistic model\nLike earlier, we can either use fmodel() function from the {statisticalModeling} package or use the ggplot() function from the ggplot2 package.\nFor the ggplot() function, we specify the ‚Äòbinomial‚Äô model in the method.args = list(family = \" \") parameter inside geom_smooth() argument. Converting the event to numeric form and subtracting 1 from it will make the choice values stay between zero and one. In the ggplot2 graph, the value 1 corresponds to the 10 Mile run and the value 0 corresponds to the 5 Km run.\n\nlibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with logit link\nmodel_logistic <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the logistic model with link function using fmodel()\nfmodel(model_logistic) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n# Plotting the logistic model with link function using ggplot()\nggplot(run17_tidy, aes(age, as.numeric(event) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = \"binomial\")) +\n  theme_bw() + labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\nWe can also plot for different link functions in the logistic model.\n\nlibrary(dplyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with logit link\nmodel_logistic_long <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the logistic model with logit and probit link function using ggplot()\n# Same graph as the earlier one\nggplot(run17_tidy, aes(age, as.numeric(event) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'logit')),\n              colour = \"red\") +\n  geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'probit')),\n              colour = \"green\") + \n  theme_bw() + labs(title = \"event ~ age (logistic model; red: logit, green: probit)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\nAs you can see the lines corresponding to logit and probit link functions are very similar."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#multiple-regression-with-glms",
    "href": "tutorials/stat_model/glm_stat_model.html#multiple-regression-with-glms",
    "title": "Generalized linear models in R",
    "section": "\n13 Multiple regression with GLMs",
    "text": "13 Multiple regression with GLMs\nAs seen with linear models, GLMs are also affected by collinearity. Therefore, the order of the explanatory variables in the model formula matters for GLMs, if there exists a correlation between the explanatory variables used. To check the correlation between two variables we can use the cor() function.\nLet us look at this in action using the logistic model we created earlier, but this time we will also add the pace_sec variable which tells the average time (in seconds) to complete a mile.\n\nlibrary(dplyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Finding the correlation between pace_sec and age\n# Remove any NAs prior to finding the correlation\nrun17_tidy <- drop_na(run17_tidy)\ncor(run17_tidy$pace_sec, run17_tidy$age)\n\n[1] 0.1620674\n\n# Building a logistic model wiht pace_sec + age\nmodel_logistic_1 <- glm(event ~ pace_sec + age, data = run17_tidy, family = \"binomial\")\n\n# Building a logistic model wiht age + pace_sec\nmodel_logistic_2 <- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Printing the results\nsummary(model_logistic_1)\n\n\nCall:\nglm(formula = event ~ pace_sec + age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2235   0.1997   0.3203   0.4770   1.2549  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  8.4756604  0.1606570  52.756  < 2e-16 ***\npace_sec    -0.0107900  0.0002218 -48.650  < 2e-16 ***\nage          0.0154362  0.0022611   6.827 8.69e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15118  on 19956  degrees of freedom\nResidual deviance: 11404  on 19954  degrees of freedom\nAIC: 11410\n\nNumber of Fisher Scoring iterations: 6\n\nsummary(model_logistic_2)\n\n\nCall:\nglm(formula = event ~ age + pace_sec, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2235   0.1997   0.3203   0.4770   1.2549  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  8.4756604  0.1606570  52.756  < 2e-16 ***\nage          0.0154362  0.0022611   6.827 8.69e-12 ***\npace_sec    -0.0107900  0.0002218 -48.650  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15118  on 19956  degrees of freedom\nResidual deviance: 11404  on 19954  degrees of freedom\nAIC: 11410\n\nNumber of Fisher Scoring iterations: 6\n\n\nYou can see that the model summary is identical, even with different ordering of the exploratory variables. This is because the correlation coefficient is ~ 0.17 which means there is a weak correlation between the used exploratory variables. So before building a model it‚Äôs a good practice to check if the exploratory variables exhibit collinearity."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#assumptions-of-glms",
    "href": "tutorials/stat_model/glm_stat_model.html#assumptions-of-glms",
    "title": "Generalized linear models in R",
    "section": "\n14 Assumptions of GLMs",
    "text": "14 Assumptions of GLMs\nGLMs extend the linear model via link function which transforms the coefficient into a linear combination which can be interpreted in a similar way to that seen in linear models. Nevertheless, there are important assumptions which are to be met by the data to apply any GLM.\n\nSimpson‚Äôs paradox does not occur\n\nSimpson‚Äôs paradox occurs when we fail to include an exploratory variable which significantly changes the model output after its addition. Consider the graphs given below. Please use the slider to compare them.\n\nCodelibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with age\nmodel_logistic_1 <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_1) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n# Building a logistic model with age + pace_sec\nmodel_logistic_2 <- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_2) + theme_bw() +\n  labs(title = \"event ~ age + pace_sec (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\nlibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with age\nmodel_logistic_1 <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_1) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n# Building a logistic model with age + pace_sec\nmodel_logistic_2 <- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_2) + theme_bw() +\n  labs(title = \"event ~ age + pace_sec (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\n\nIn the first model, predicting event choice with just age suggests that with increasing age, there is less chance that people will participate in the 10 Mile marathon. But when you also add the pace of participants into the model formula then you get completely different results (and absurd too!). The second model indicates that participants with a high pace (shorter y-axis values in the graph) irrespective of age will almost always choose the 10 Mile marathon. So our interpretation is completely changed and this is an example of Simpson‚Äôs paradox.\n\nVariables follow linear relationship and are monotonic\n\nLike in the case of linear models, GLMs also require that the variables follow a linear change. Monotonic means that the change is either always increasing or decreasing.\n\nVariables should independent\n\nThe effect sizes of one variable should not influence the effect size of another variable. We have seen this in detail for linear models. If they are independent, then we have to represent them as interaction terms. In this case, the order also matters while inputting them into the model formula.\n\nOver dispersion\n\nOverdispersion occurs when the datasets have a lot of zeros or ones in the case of the binomial distribution and a lot of zeros in the case of the Poisson distribution. Also if the variance is greater than the mean of the data, then again the model is over-dispersed."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#conclusion",
    "href": "tutorials/stat_model/glm_stat_model.html#conclusion",
    "title": "Generalized linear models in R",
    "section": "\n15 Conclusion",
    "text": "15 Conclusion\nThis marks the end of the introduction to generalised linear models. In summary, we learned;\n\nUnderstanding the coefficients in a linear model\nWhy do linear models fail for some datasets and what are the limitations of a linear model\nTwo types of GLMs: Poisson and Logistic\nLink functions\nUnderstanding the coefficients in a Poisson and logistic models\nPlotting Poisson and logistic models\nBrief introduction to multiple regression using GLMs\nAssumptions of GLMs: When to use GLMs\n\nIn the next tutorial, we will take our understanding of GLMs to the next level and learn about ‚ÄòHierarchical and Mixed Effects Models in R‚Äô. See you then!"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html",
    "href": "tutorials/stat_model/inter_stat_model.html",
    "title": "Intermediate statistical modelling in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() => null)"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#prologue",
    "href": "tutorials/stat_model/inter_stat_model.html#prologue",
    "title": "Intermediate statistical modelling in R",
    "section": "\n1 Prologue",
    "text": "1 Prologue\nThis tutorial serves as a sequel to the tutorial post: Introduction to statistical modelling in R. In this tutorial, you will gain knowledge in intermediate statistical modelling using R. You will learn more about effect sizes, interaction terms, total change and partial change, bootstrapping and collinearity."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/inter_stat_model.html#making-life-easier",
    "title": "Intermediate statistical modelling in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Libraries used in this tutorial\ninstall.packages('NHANES')\ninstall.packages('devtools')\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\ninstall.packages('mosaicData')\ninstall.packages('mosaicModel')\ninstall.packages('Stat2Data')\ninstall.packages('cherryblossom')\ninstall.packages(\"tidyverse\")\ninstall.packages(\"mosaic\")\n\n# Loading the libraries\ntutorial_packages <- c(\"cherryblossom\", \"mosaicData\", \"NHANES\",\n                 \"Stat2Data\", \"tidyverse\", \"mosaic\", \"mosaicModel\", \"statisticalModeling\")\n\nlapply(tutorial_packages, library, character.only = TRUE)\n\n# Datasets used in this tutorial\ndata(\"NHANES\") # From NHANES\ndata(\"SwimRecords\") # From mosaicData\ndata(\"Tadpoles\") # From mosaicModel\ndata(\"HorsePrices\") # From Stat2Data\ndata(\"run17\") # From cherryblossom\ndata(\"Oil_history\") # From statisticalModeling\ndata(\"SAT\") # From mosaicData"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#effect-size-when-response-variable-is-categorical",
    "href": "tutorials/stat_model/inter_stat_model.html#effect-size-when-response-variable-is-categorical",
    "title": "Intermediate statistical modelling in R",
    "section": "\n3 Effect size when response variable is categorical",
    "text": "3 Effect size when response variable is categorical\nIn the last tutorial, we learned how to calculate the effect size for an explanatory variable. To recall, we learned that;\n\n\n\n\n\n\n\nExploratory variable type\nEffect size\nError\n\n\n\nNumerical\nRate of change (slope)\nMean of the square of the prediction error (m.s.e)\n\n\nCategorical\nDifference (change)\nLikelihood value\n\n\n\nSo depending on the nature of the exploratory variable, the nature of the effect size will also change. For a numerical exploratory variable, the effect size is represented as the slope of the line, which represents the rate of change. For a categorical response variable, the effect size is represented as the difference in model output values, when the variable changes from one category to the other.\nWe also learned that, for a categorical response variable, it‚Äôs helpful to represent the model output as probability values rather than class values. Therefore, for calculating errors, if we have a numerical response variable then the error metric is the mean of the square of the prediction error and for a categorical response variable, the error would be the likelihood value. We saw in detail how to find each of them.\nBut what we did not calculate back then was the effect size, if we had a categorical response variable. So what intuition would effect size make here for this case? Let us find out.\nWe will be using the NHANES dataset from the {NHANES} package in R, the same one we saw last time while learning to interpret the recursive partitioning model plots. Back then, our exploratory analysis found that depression is having a strong association with the interest in doing things on a given day. Let us build that model using the rpart() function. Here the model will have Depressed as the response variable and LittleInterest as the explanatory variable. Please note that here both the variables are categorical.\nThe levels for Depressed are; ‚ÄúNone‚Äù = No sign of depression, ‚ÄúSeveral‚Äù = Individual was depressed for less than half of the survey period days, and ‚ÄúMost‚Äù = Individual was depressed more than half of the days.\nThe levels for LittleInterest are also similar to that of Depressed; ‚ÄúNone‚Äù, ‚ÄúSeveral‚Äù and ‚ÄúMost‚Äù, and are analogous to levels of Depressed.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(rpart)) install.packages('rpart')\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(rpart)\nlibrary(statisticalModeling)\n\n# Building the recursive partitioning model\nmodel_rpart <- rpart(Depressed ~ LittleInterest, data = NHANES)\n\n# Calculating the effect size\neffect_size_rpart <- effect_size(model_rpart, ~ LittleInterest)\n\n# Printing the effect size\neffect_size_rpart\n\n\n\n  \n\n\n\nAll right, we got the effect size values and they are given in two columns; ‚Äòchange.None‚Äô and ‚Äòchange.Several‚Äô. Since our explanatory variable is categorical, effect size values with a categorical response variable give the change in probability values. Here, notice that the effect size is calculated as the difference when the response variable is changing its category from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô. That difference is the probability difference from the base value, which is zero.\nTo make more sense, imagine we get an effect size value of zero, in both the columns, in the above case. This means that when the level of little interest changes from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô, there is no increase or decrease in the probability of depression is ‚ÄòNone‚Äô or ‚ÄòSeveral‚Äô. This means, that if a person is having no depression, changing little interest from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô will not evoke a change in that person‚Äôs depressed state. Thus zero is taken as the base value.\nNow let us look at the actual effect size values we got. As the level of little interest is changing from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô, we get a probability difference of -0.4466272 = ~ 0.45 for depression is ‚ÄòNone‚Äô. This means that there is a reduction of 45% in the base value of probability. In other words, the person is 45% less likely to have depression set to ‚ÄòNone‚Äô. Likewise, in the same notion, the person is 36% more likely to have a depressed state of ‚ÄòSeveral‚Äô, when little interest changes from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô. I hope this was clear to you.\nNow, let us look at the case when, again, our response variable is categorical but this time, our explanatory variable is numerical.\nLet us make another recursive partitioning model. We will have SmokeNow as the response variable and our explanatory variable will be BMI, which denotes the body mass index of the participants. The SmokeNow variable denotes smoking habit and has two levels; ‚ÄúYes‚Äù = smoked 100 or more cigarettes in their lifetime., and ‚ÄúNo‚Äù = did not smoke 100 or more cigarettes.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(rpart)) install.packages('rpart')\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(rpart)\nlibrary(statisticalModeling)\n\n# Building the recursive partitioning model\nmodel_rpart <- rpart(SmokeNow ~ BMI, data = NHANES)\n\n# Calculating the effect size\neffect_size_rpart <- effect_size(model_rpart, ~ BMI)\n\n# Printing the effect size\neffect_size_rpart\n\n\n\n  \n\n\n\nLooks like we got the effect size values as zero. Going by our previous notion, this means that when BMI is changed from 26 to 33, there is no change in smoking habit. Does this mean that BMI has no relationship with the smoking habit?\nBefore we conclude, let us try a different model architecture. We learned about logistic modelling last time. Let us build a model using it for the same question.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model\nmodel_logistic <- glm(SmokeNow == \"No\" ~ BMI, data = NHANES, family = \"binomial\")\n\n# Calculating the effect size\neffect_size_logistic <- effect_size(model_logistic, ~ BMI)\n\n# Printing the effect size\neffect_size_logistic\n\n\n\n  \n\n\n\nWe got a very small effect size value but a non-zero value. Let us try plotting both models. We will use the fmodel() function in the {statisticalModeling} package to easily plot the model. Use the slider to view between the plots.\n\n# Plotting both recursive partitioning model\nfmodel(model_rpart) + ggplot2::theme_bw() + \n  ggplot2::labs(title = \"SmokeNow ~ Age (recursive partitioning model)\")\n\n# Plotting both logistic model\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"SmokeNow ~ Age (logistic model)\")\n\n\n\n# Plotting both recursive partitioning model\nfmodel(model_rpart) + ggplot2::theme_bw() + \n  ggplot2::labs(title = \"SmokeNow ~ Age (recursive partitioning model)\")\n\n\n\n# Plotting both logistic model\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"SmokeNow ~ Age (logistic model)\")\n\n\n\n\n\nPlease note that the fmodel() function, while plotting for the recursive partitioning model takes in the ‚Äòfirst‚Äô level in the variable. You can easily check which level is represented in the y-axis by checking the level order.\n\nlibrary(NHANES)\n\n# Checking levels\nlevels(NHANES$SmokeNow)\n\n[1] \"No\"  \"Yes\"\n\n\nThe level ‚ÄúNo‚Äù will be represented in the y-axis for the recursive partitioning model plot. Therefore for increasing y-axis values, the probability of not smoking will increase.\nAfter comparing both the plots you can quickly notice that the plot for the recursive partitioning model shows step-wise change or sudden change and for the logistic model plot, the increase is gradual. Interestingly the plot shows that in higher BMI groups, i.e.¬†in groups of obese people, a smoking habit is uncommon. Anyway, in the recursive partitioning model plot, there is a sudden increase in y-axis values between 20 and 30 in the axis. Let us calculate the effect size of both models at this range.\n\n# Calculating the effect size for recursive partitioning model\neffect_size_rpart <- effect_size(model_rpart, ~ BMI, BMI = 20, to = 10)\n\n# Printing the effect size for recursive partitioning \neffect_size_rpart\n\n\n\n  \n\n\n# Calculating the effect size for logistic model\neffect_size_logistic <- effect_size(model_logistic, ~ BMI, BMI = 20, to = 10)\n\n# Printing the effect size for logistic\neffect_size_logistic\n\n\n\n  \n\n\n\nNote how the effect size for the recursive partitioning model is higher as compared to the logistic model. This is evident from the graph as the slope changes much faster at the given range of x-axis values for the recursive partitioning model as compared to the logistic model. Thus, recursive partitioning works best for sharp, discontinuous changes, whereas logistic regression can capture smooth, gradual changes.\nNow coming back to interpreting the values, let‚Äôs take the case we just discussed. Let us take the effect sizes in the recursive partitioning model. Here, as BMI changes from 20 to 30, there is a 1.8% increased chance that the smoking habit drops. Also ass the response variable is dichotomous, the opposite happens for the other case; 1.8% reduced chance that smoking habit is seen. With such low chances, BMI might not have any association with smoking habit at that particular range of the x-axis. values."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#plotting-the-model-output",
    "href": "tutorials/stat_model/inter_stat_model.html#plotting-the-model-output",
    "title": "Intermediate statistical modelling in R",
    "section": "\n4 Plotting the model output",
    "text": "4 Plotting the model output\nWith the help of the fmodel() function in the {statisticalModeling} package, we can easily plot the model outputs as a plot. Let us plot some plots.\nWe will again use the NHANES dataset from the {NHANES} package in R. We will DiabetesAge as the response variable and see if it has any association with Age. The Diabetes describes the participant‚Äôs age at which they were declared diabetic.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model\nmodel_logistic <- glm(Diabetes ~ Age, data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"Diabetes ~ Age (logistic model)\")\n\n\n\n\nWe get a rather unsurprising graph. As age increases, the risk of getting diabetes increases. Let us add more variables to the model and plot them one by one. Let us add the following variables; Gender, Depressed and Education.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model (adding gender)\nmodel_logistic <- glm(Diabetes ~ Age + Gender, data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"DiabetesAge ~ Age + Gender (logistic model)\")\n\n\n\n\nThe addition of the third variable introduced the colour aesthetic to the plot. Let us add another variable.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model (adding depressed state)\nmodel_logistic <- glm(Diabetes ~ Age + Gender + Depressed, data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"DiabetesAge ~ Age + Gender + Depressed (logistic model)\")\n\n\n\n\nAdding a fourth variable faceted the plot into the levels of the fourth variable. Let us add one last variable.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model (adding Education)\nmodel_logistic <- glm(Diabetes ~ Age + Gender + Depressed + Education,\n                      data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"DiabetesAge ~ Age + Gender + Depressed + Education (logistic model)\")\n\n\n\n\nThe plot undergoes another layer of faceting. Now we get a rather complex plot as compared to the plot with just one explanatory variable. The plot shows some interesting info. Males are more prone to become diabetic as compared to females in all the groups. For college graduates who have no depression, both sexes have a reduced chance of having diabetes at younger ages as compared to all other groups. People with high school level of education and college level education have the most chance of becoming diabetic out of all groups. I leave the rest of the interpretations to the readers.\nIn summary, the fmodel() function syntax with four explanatory variables would be;\n\nfmodel(response ~ var1 + var2 + var3 + var4, data = dataset)\n\nHere, the response variable will always be plotted on the y-axis. The var1 will be plotted on the x-axis and will be the main exploratory variable that we are interested in. The var2 will assume the colour aesthetic. The var3 and var4 will invoke faceting. Therefore we have;\n\nfmodel() variables as plot components\n\nVariable\nPlot component\n\n\n\nvar1\ny-axis\n\n\nvar2\ncolour\n\n\nvar3\nfacet\n\n\nvar4\nfacet"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#interactions-among-explanatory-variables",
    "href": "tutorials/stat_model/inter_stat_model.html#interactions-among-explanatory-variables",
    "title": "Intermediate statistical modelling in R",
    "section": "\n5 Interactions among explanatory variables",
    "text": "5 Interactions among explanatory variables\nTill now, when making models, we assumed that our explanatory variables are independent of each other, i.e.¬†their effect sizes are independent of each other. But often this is not the case in real life, we can have instances where the effect size of one variable changes with the other explanatory variables. When this happens, we say that both of those variables are interacting with each other.\nTo denote interacting terms in the formula, instead of a + sign, we use a *.\nLet us see it with an example. We will use the SwimRecords dataset from the mosaicData package in R. The dataset contains world records for 100m swimming for men and women over time from 1905 through 2004. We will build a linear model, using time as the response variable and, year and sex as the explanatory variable. Here time denotes the time of the world record.\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm <- lm(time ~ year + sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year + sex (linear model)\")\n\n\n\n\nFrom the graph, you can see that the slopes for both the lines are decreasing. This means that as years progressed, the world record times for both the sexes reduced. Also, men‚Äôs record time is faster compared to women‚Äôs. What happens to this plot if we introduce an interaction between year and sex?\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm_int <- lm(time ~ year * sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm_int) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year * sex (linear model)\")\n\n\n\n\nThe introduction of interacting terms seems to converge the two lines. For better comparison, use the slider to compare the plots as given below.\n\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm <- lm(time ~ year + sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year + sex (linear model without interaction)\")\n\n\n\n# Building the linear model with interaction term\nmodel_lm_int <- lm(time ~ year * sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm_int) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year * sex (linear model with interaction)\")\n\n\n\n\n\nWe get some interesting results. First of all, compared to the plot without the interaction term, the slope changes for both the sexes, in the plot with interaction terms. Also, the lines are not parallel when interaction terms are introduced, this is direct evidence showing that both year and sex variables are truly interacting with each other. This is why with increasing years, the gap between the lines decreased, indicating a reduced effect size of sex with increasing years.\nTo see if this interaction term makes the model better, we can calculate the mean of the square of prediction errors (m.s.e) for each model and do a t.test using them (something we learned in the previous tutorial).\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm <- lm(time ~ year + sex, data = SwimRecords)\n\n# Building the linear model with interaction term\nmodel_lm_int <- lm(time ~ year * sex, data = SwimRecords)\n\n# Calculating the m.s.e\nt.test(mse ~ model, data = cv_pred_error(model_lm, model_lm_int))\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = 7.9906, df = 5.9783, p-value = 0.0002086\nalternative hypothesis: true difference in means between group model_lm and group model_lm_int is not equal to 0\n95 percent confidence interval:\n 3.031932 5.711848\nsample estimates:\n    mean in group model_lm mean in group model_lm_int \n                  17.14633                   12.77444 \n\n\nFor \\alpha = 0.05 level of significance, we have a p-value < 0.05. Thus we conclude that the m.s.e values between the two models are significantly different from each other. Also, from the t.test summary, we can see that the m.s.e value is lowest for the model with the interaction term as compared to the model without the interaction term. This means interaction made the model better.\nSome takeaway points about effect sizes, from analysing the plots are;\n\n\nPoint 1\nPoint 2\nPoint 3\nPoint 4\nPoint 5\n\n\n\n The slope of the line is the effect size of the x-axis exploratory variable on the y-axis response variable.\n\n\n The difference in y-intercepts of both the lines for a given x-axis exploratory variable value gives the effect size of the colour aesthetic on the y-axis response variable.\n\n\n If lines are parallel, then there is no interaction between the x-axis variable and the colour aesthetic variable.\n\n\n\nThe difference between the values of the slopes tells how the colour aesthetic variable is affecting the effect size of the x-axis variable.\n\n\n\nThe rate of change of the y-intercepts tells how the x-axis variable is affecting the effect size of the colour aesthetic variable.\n\n\n\nNow we are equipped with a strong intuition of how the interaction between variables affects the effect size of those variables and how they can be visualized from the plot."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#polynomial-regression",
    "href": "tutorials/stat_model/inter_stat_model.html#polynomial-regression",
    "title": "Intermediate statistical modelling in R",
    "section": "\n6 Polynomial regression",
    "text": "6 Polynomial regression\nLet us look at the Tadpoles dataset from the {mosaicModel} package in R. We saw this dataset while we learned about covariates in the previous tutorial. The dataset contains the swimming speed of tadpoles as a function of the water temperature and the water temperature at which the tadpoles had been raised. Let us plot the temperature at which the tadpoles were allowed to swim versus the maximum swimming speed.\n\nif (!require(mosaicModel)) install.packages('mosaicModel')\nlibrary(mosaicModel)\nlibrary(ggplot2)\n\ndata(Tadpoles)\n\n# Building a plot\nTadpoles |> ggplot(aes(rtemp, vmax)) + geom_point() + theme_bw()\n\n\n\n\nNow let us try building a linear model using vmax as the response variable and rtemp as the exploratory variable.\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Tadpoles)\n\n# Building the linear model\nmodel_lm <- lm(vmax ~ rtemp, data = Tadpoles)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::geom_point(data = Tadpoles) +\n  ggplot2::theme_bw()\n\n\n\n\nWe see that the model out shows a weak relationship between the temperature at which the tadpoles were allowed to swim and the maximum swimming speed. But our intuition tells us that most of the maximum values for vmax peaked at temperatures of 15¬∞C and 25¬∞C hence, the line should curve upwards at these temperature regions. Essentially, instead of a straight line relationship, a parabolic relationship would be better for defining the relationship between rtemp and vmax. To tell the model to consider rtemp as a second-order variable, we use the I() function. So here, including I(rtemp^2) in the formula will inform the model to regress vmax against rtemp^2. You might be tempted to ask, why not just use rtemp^2 in the model formula? If we use vmax ~ rtemp^2 as the model formula, then notation wise it is equivalent to using vmax ~ rtemp * rtemp. If we run the model using this formula, the model will omit one of the rtemp variables as it is redundant to regress against a variable interacting with itself.\nOn a more technical note, in R, the symbols \"+\", \"-\", \"*\" and \"^\" are interpreted as formula operators. Therefore if rtemp^2 is used inside the formula, it will consider it as rtemp * rtemp. Thus to make things right, we use the I() function, so that R will interpret rtemp^2 as a new predictor with squared values.\nLet us plot a polynomial model for the same question as above.\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Tadpoles)\n\n# Building the linear model with rtemp in second order using I()\nmodel_lm_2 <- lm(vmax ~ rtemp + I(rtemp^2), data = Tadpoles)\n\n# Plotting the model\nfmodel(model_lm_2) + ggplot2::geom_point(data = Tadpoles) +\n  ggplot2::theme_bw()\n\n\n\n\nMuch better plot than the earlier one. The same can also be achieved by using the poly() function. The syntax for it is; poly(variable, the order)\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Tadpoles)\n\n# Building the linear model with rtemp in second order using poly()\nmodel_lm_2 <- lm(vmax ~ poly(rtemp, 2), data = Tadpoles)\n\n# Plotting the model\nfmodel(model_lm_2) + ggplot2::geom_point(data = Tadpoles) +\n  ggplot2::theme_bw()"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#total-and-partial-change",
    "href": "tutorials/stat_model/inter_stat_model.html#total-and-partial-change",
    "title": "Intermediate statistical modelling in R",
    "section": "\n7 Total and partial change",
    "text": "7 Total and partial change\nFrom the previous tutorial and current tutorial, we built our notion of effect size bit by bit. In a gist, the flowchart below shows our efforts till now;\n\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\ngraph TB\n  A[Understanding effect sizes] --o B(In the context of numerical explanatory variable)\n  B --> C(In the context of categorical explanatory variable)\n  C --> D(Magnitude and the sign of effect size value)\n  D --> E(Units of effect size)\n  E --> F(In the context of categorical response variable)\n  F --> G(How interaction terms affect effect sizes:<br>Understanding it from graphs)\n  G -.-> H(How interaction terms affect effect sizes:<br>Understanding it from calculations)\n  style H fill:#f96\n  subgraph Flowchart on current understanding of effect size\n  A\n  B\n  C\n  D\n  E\n  F\n  G\n  H\n  end\n\n\n\n\n\n\n\n\nFrom the flowchart, we can appreciate the growing complexity in the understanding of effect size. We have already begun to understand effect sizes in the context of interaction terms as seen from previous graphs. Now let us try to calculate effect sizes with interaction terms.\nWe will use the HorsePrices dataset from the Stat2Data package in R. The dataset contains the price and related characteristics of horses listed for sale on the internet. The variables in the dataset include; Price = Price of the horse ($), HorseID = ID of the horse, Age = Age of the horse, Height = Height of the horse and Sex = Sex of the horse.\nSuppose we are interested in the following questions\n\nHow does the price of the horse change between sexes?\nHow does the price of the horse change between sexes for the same height and age?\n\nIn the first question, we are checking for the price difference between the sexes. Here, when sex changes, the covariate variables; height and age of the horse will also change with sex. Therefore, here we are measuring the change in price by allowing all other variables in the model to change along with the exploratory variable sex. This is termed the ‚Äòtotal change in price as sex changes‚Äô.\nIn the second question, unlike the first, we want to specifically see what is the price difference between sexes. To see this change, we keep height and age constant. Therefore, the only change seen in this case would be coming from the sex change. This is termed the ‚Äòpartial change of price as sex changes‚Äô.\nFor the first question, we will calculate the effect size of sex on price using the following lines of code. Since our question focuses on the total change in price with sex, we make a model by excluding all the other covariates that we want to change as sex changes.\n\n# Loading libraries\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building the linear model\nmodel_lm <- lm(Price ~ Sex, data = HorsePrices)\n\n\n# Calculating the effect size\neffect_size(model_lm, ~ Sex)\n\n\n\n  \n\n\n\nThe effect size value tells us that, changing from a male horse to a female, the price depreciates by 17225 dollars. Now to answer the second question, to calculate the partial change in price because of a change in sex, we build a model by including all the other covariates that we want to keep constant when we change sex.\n\n# Loading libraries\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building the linear model\nmodel_lm_partial <- lm(Price ~ Sex + Height + Age, data = HorsePrices)\n\n# Calculating the effect size\neffect_size(model_lm_partial, ~ Sex)\n\n\n\n  \n\n\n\nThe effect size value, in this case, tells us that, a male horse of age 6 and 16.5m in height will be 9928 dollars more costly than a female horse of the same physical attributes. Here it‚Äôs clear that for the given height and age, there is a price difference between male and female horses, something which we were trying to answer."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-model-output",
    "href": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-model-output",
    "title": "Intermediate statistical modelling in R",
    "section": "\n8 R-squared in terms of model output",
    "text": "8 R-squared in terms of model output\nSo far we have seen metrics like the mean of the square of prediction error and likelihood values to evaluate a model‚Äôs predictive abilities. In terms of variables used in the model formula, we know how to interpret them using effect sizes. There is also another metric called R-squared (R^2).\nOne of the implications of a model is to account for variations seen within the response variable to the model output values. The R-squared value tells us how well this has been done. By definition;\nR^2 = \\frac{variance\\,of\\,model\\,output\\,values}{variance\\,of\\,response\\,variable\\,values} The R-squared value is always positive and is between 0 and 1. R-squared value of 1 means that the model accounted for all the variance seen within the actual dataset values of the response variable. R-squared value of 0 means that the model does not account for any variance seen in the response variable.\nBut the R-squared value has its flaws which we will see soon.\nLet us calculate the R-squared value for some models. We will again use the HorsePrices dataset from the Stat2Data package in R. Let us build the earlier model again and calculate the R-squared value for the model. We will use the evaluate_model() function from the statisticalModeling package to get the predicted values. For now, we will use the training dataset itself as the testing dataset. We will use the var() formula to calculate the variance.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building the linear model\nmodel_lm <- lm(Price ~ Sex, data = HorsePrices)\n\n# Getting the predicted values\nmodel_lm_output <- evaluate_model(model_lm, data = HorsePrices)\n\n# Calculating the R-squared value\nr_squared <- with(data = model_lm_output, var(model_output)/var(Price))\n\n# Printing R-squared value\nr_squared\n\n[1] 0.3237929\n\n\nWe get an R-squared value of 0.32. What this means is that 32% of the variability seen in the price of the horses is explained by the sex of the horses.\nNow let us add in more explanatory variables and calculate the R-squared value. We will also remove any NA values in the model variables.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new <- tidyr::drop_na(HorsePrices)\n\n# Building the linear model\nmodel_lm <- lm(Price ~ Sex + Height + Age, data = HorsePrices_new)\n\n# Getting the predicted values\nmodel_lm_output <- evaluate_model(model_lm, data = HorsePrices_new)\n\n# Calculating the R-squared value\nr_squared <- with(data = model_lm_output, var(model_output)/var(Price))\n\n# Printing R-squared value\nr_squared\n\n[1] 0.4328061\n\n\nWe got a higher R-squared value than before. Instead of 32%, we now can account for 43% of variability seen in price by all the other variables.\nDoes this mean that this model is better than the previous one? Before we jump in, let us do a fun little experiment. Let us see what happens if we add in random variables with no predicted power whatsoever to the model and run a model using them.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new <- tidyr::drop_na(HorsePrices)\n\n# Adding random variable\nset.seed(56)\nHorsePrices_new$Random <- rnorm(nrow(HorsePrices_new)) > 0\n\n# Building the linear model\nmodel_lm_random <- lm(Price ~ Sex + Height + Age + Random, data = HorsePrices_new)\n\n# Getting the predicted values\nmodel_lm_random_output <- evaluate_model(model_lm_random, data = HorsePrices_new)\n\n# Calculating the R-squared value\nr_squared <- with(data = model_lm_random_output, var(model_output)/var(Price))\n\n# Printing R-squared value\nr_squared\n\n[1] 0.4437072\n\n\nWe got an R-squared value of 0.44. The random variable should throw off the model output, then why did we get a higher R-squared value?\nThe R-squared value of a model will increase with increasing explanatory variables. To remind you again, the R-squared value comes partly from the model output and our model output comes from the design of the model. The model design includes selecting the appropriate explanatory variables which are up to the user. Therefore, stupidly adding variables which have no relationship whatsoever with the response variable can lead us to the wrong conclusion. Let us calculate the mean square of the prediction error for our last two models and compare them.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new <- tidyr::drop_na(HorsePrices)\n\n# Adding random variable\nset.seed(56)\nHorsePrices_new$Random <- rnorm(nrow(HorsePrices_new)) > 0\n\n# Building the linear model\nmodel_lm <- lm(Price ~ Sex, data = HorsePrices_new)\n\n# Building the linear model with random variable\nmodel_lm_random <- lm(Price ~ Sex + Random, data = HorsePrices_new)\n\n# Calculating the mean of square of prediction errors for trials\nmodel_errors <- cv_pred_error(model_lm, model_lm_random)\n\n# Calculating the mean of square of prediction errors\nboxplot(mse ~ model, model_errors)\n\n\n\n# Conducting t-test\nt.test(mse ~ model, model_errors)\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = -2.9348, df = 6.9143, p-value = 0.02218\nalternative hypothesis: true difference in means between group model_lm and group model_lm_random is not equal to 0\n95 percent confidence interval:\n -11481425  -1221092\nsample estimates:\n       mean in group model_lm mean in group model_lm_random \n                    153536030                     159887288 \n\n\nThe plot and the t.test results can be used to check if the model with the random variable is indeed a poor one compared to the model without any random variable, something the R-squared value failed to report. Therefore R-squared value has its flaw, but it is widely used by people. Therefore, we should be very careful in interpreting the R-squared values of different models. Different models made from the same data can have different R-squared values."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-residuals",
    "href": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-residuals",
    "title": "Intermediate statistical modelling in R",
    "section": "\n9 R-squared in terms of residuals",
    "text": "9 R-squared in terms of residuals\nFrom the above example, I mentioned that the R-squared value tells us how much percentage of the variability seen within the response variable is captured by the predicted model output values. Another way of seeing the same thing is through residuals. The residual is the distance between the regression line and the response variable value, or in other words, the difference between the fitted value and the corresponding response variable value. We can find R-squared using the following formula also;\nR^2 = \\frac{variance\\,of\\,response\\,variable\\,values - variance\\,of\\,residuals}{variance\\,of\\,response\\,variable\\,values}\nWe can use the following code to calculate the R-squared value via the above formula.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new <- tidyr::drop_na(HorsePrices)\n\n# Building the linear model\nmodel_lm <- lm(Price ~ Sex + Height + Age, data = HorsePrices_new)\n\n# Getting the predicted values\nmodel_lm_output <- evaluate_model(model_lm, data = HorsePrices_new)\n\n# Calculating the R-squared value using model output values\nr_squared_mo <- with(data = model_lm_output, var(model_output)/var(Price))\n\n# Calculating the R-squared value using residuals\nr_squared_res <- (var(HorsePrices_new$Price) - var(model_lm$residuals)) / var(HorsePrices_new$Price)\n\n# Checking if both R-squared values are the same\nr_squared_mo\n\n[1] 0.4328061\n\nr_squared_res\n\n[1] 0.4328061\n\nround(r_squared_mo) == round(r_squared_res)\n\n[1] TRUE\n\n\nAs you can see, they are both the same. Therefore, the R-squared value of 1 means that the model has zero residuals, which means that the model is a perfect fit; all data points lie on the regression line. R-squared value of 0 means that the variance of residuals is the same as that of the variance of the response variable. This means the explanatory variables we chose do not account for the variation seen within the response variable."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#bootstrapping-and-precision",
    "href": "tutorials/stat_model/inter_stat_model.html#bootstrapping-and-precision",
    "title": "Intermediate statistical modelling in R",
    "section": "\n10 Bootstrapping and precision",
    "text": "10 Bootstrapping and precision\nAll the datasets that we have worked on till now have been collected from a large population. Therefore, from these samples, we calculated different test statistics like the mean prediction error and effect sizes. Suppose we sample data again from the same population and calculate these test statistics, if they are close to earlier calculated values, then we can say that our test statistics are precise. But re-sampling the data is often costly and tedious to do. Then how can we check for precision? We can do that using a technique called bootstrapping. Similar to resampling, instead of collecting new samples from the population, we resample the data from the already collected data itself. Essentially, we are treating our existing dataset as a population dataset from which resampling occurs. Needless to say, this would only work effectively with large datasets.\n\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\ngraph LR\n  A[Population] --> B(Random sample)\n  B --> C(Calculating sample statistics)\n  A --> D(Random sample)\n  D --> E(Calculating sample statistics)\n  A --> F(\"---\")\n  F --> G(\"---\")\n  A --> H(Random sample)\n  H --> I(Calculating sample statistics)\n  J[Population] --> K(Random sample)\n  J --> L(Random sample)\n  J --> M(Collected random sample)\n  M --> N(Resample 1)\n  M --> O(Resample 2)\n  M --> P(Resample 3)\n  J --> Q(Random sample)\n  subgraph Bootstrapping\n  J\n  K\n  L\n  M\n  N\n  O\n  P\n  Q\n  end\n  subgraph Resampling from the population\n  A\n  B\n  C\n  D\n  E\n  F\n  G\n  H\n  I\n  end\n\n\n\n\n\n\n\n\nLet us perform a bootstrap trial code by code. We will be using the run17 dataset from the cherryblossom package in R. It‚Äôs a relatively large dataset that we can use to study how to do a bootstrap.\nWe will first build a model and then calculate a sample statistic, which in this case would be the effect size.\n\n# Loading libraries\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_lm <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Calculating the effect size\neffect_size(model_lm, ~ age)\n\n\n\n  \n\n\n\nUsing the sample() function in R, we will sample the row indices in the original dataset. Then using these row indices, we build a resampled dataset. Then using this resampled dataset, we will build a new model and then calculate the effect size.\n\n# Loading libraries\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Collecting the row indices \nrow_indices <- sample(1:nrow(run17_marathon), replace = T)\n\n# Resampling data using the rwo indices\nresample_data <- run17_marathon[row_indices, ]\n\n# Building a linear model\nmodel_lm_resample <- lm(net_sec ~ age + sex, data = resample_data)\n\n# Calculating the effect size\neffect_size(model_lm_resample, ~ age)\n\n\n\n  \n\n\n\nWe get a slightly different effect size value in each case. If we repeat this process, we can get multiple effect size values that we can plot to get the sampling distribution of the effect size. doing this code by code is tedious and therefore we will automate this process using the ensemble() function in the statisticalModeling package in R. Using nreps inside the ensemble() function, we can specify how many times we want to resample the data. Normally, 100 resampling trials are performed.\n\n# Loading libraries\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_lm <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Resampling 100 times\nmodel_trials <- ensemble(model_lm, nreps = 100)\n\n# Calculating the effect size for each resampled data\nresampled_effect_sizes <- effect_size(model_trials, ~ age)\n\n# Printing effect sizes\nresampled_effect_sizes\n\n\n\n  \n\n\n\nNow to estimate the precision in the effect size, we can calculate its standard deviation. This is the bootstrapped estimate of the standard error of the effect size, a measure of the precision of the quantity calculated on the original model. We can also plot a histogram of the effect size values we got from each of the trails to get the sampling distribution of the effect size.\n\n# Calculating the standard error of effect size of age\nsd(resampled_effect_sizes$slope)\n\n[1] 0.652797\n\n# Plotting a histogram\nhist(resampled_effect_sizes$slope)"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#scales-and-transformations",
    "href": "tutorials/stat_model/inter_stat_model.html#scales-and-transformations",
    "title": "Intermediate statistical modelling in R",
    "section": "\n11 Scales and transformations",
    "text": "11 Scales and transformations\nDepending on the class of the response variables, we can apply relevant transformations to change the scale of the variables to make the model better. We have seen how to use appropriate model architecture depending on the response variable, now we will see how to use relevant transformations.\n\n11.1 Logarithmic transformation\nIn this exercise, we will see log-transformation which is mainly used when the response variable varies in proportion to its current size. For example, variables denoting population growth, exponential growth, prices or other money-related variables.\nWe will be using the Oil_history dataset from the statisticalModeling package in R. It denotes the historical production of crude oil, worldwide from 1880-2014. Let us first plot the data points and see how they are distributed. Here I am filtering the data till the year 1980 to get a nice looking exponential growth curve for the exercise.\n\nlibrary(statisticalModeling)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Plotting the data points\nOil_history %>% filter(year < 1980) %>%\n  ggplot(aes(year, mbbl)) + geom_point() +\n  labs(y = \"mbbl (oil production in millions of barrels)\") +\n  theme_bw()\n\n\n\n\nYou can see exponential growth in oil barrel production with increasing years. First, we will build a linear model looking at how years influenced change in barrel production. Here, mbbl would be our response variable and year our explanatory variable. Secondly, we will build another linear model but here our response variable; mbbl will be log-transformed. You can compare the plots using the slider.\n\nlibrary(statisticalModeling)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Filtering values till 1980\nOil_1980 <- Oil_history %>% filter(year < 1980)\n\n# Building a linear model\nmodel_lm <- lm(mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model\nfmodel(model_lm) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"mbbl (oil production in millions of barrels)\",\n                title = \"Oil production in millions of barrels ~ Year\") +\n  theme_bw()\n\n# Transforming mbbl values to logarithmic scale\nOil_1980$log_mbbl <- log(Oil_1980$mbbl)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log <- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model with log transformed response variable\nfmodel(model_lm_log) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"log(mbbl) [oil production in millions of barrels]\",\n                title = \"log(Oil production in millions of barrels) ~ Year\") +\n  theme_bw()\n\n\n\nlibrary(statisticalModeling)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Filtering values till 1980\nOil_1980 <- Oil_history %>% filter(year < 1980)\n\n# Building a linear model\nmodel_lm <- lm(mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model\nfmodel(model_lm) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"mbbl (oil production in millions of barrels)\",\n                title = \"Oil production in millions of barrels ~ Year\") +\n  theme_bw()\n\n\n\n# Transforming mbbl values to logarithmic scale\nOil_1980$log_mbbl <- log(Oil_1980$mbbl)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log <- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model with log transformed response variable\nfmodel(model_lm_log) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"log(mbbl) [oil production in millions of barrels]\",\n                title = \"log(Oil production in millions of barrels) ~ Year\") +\n  theme_bw()\n\n\n\n\n\nIn the first graph, our linear model does not fit the values perfectly as opposed to the model which used the log transformation of the response variable. Why the second model fits better because initially mbbl and Year had an exponential relationship. But after log transformation, the relationship became a linear one and thus the model performs better. We can calculate the mean predictive error between these two models and see which one does a better job at predicting.\n\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Filtering values till 1980\nOil_1980 <- Oil_history %>% filter(year < 1980)\n\n# Transforming mbbl values to logarithmic scale\nOil_1980$log_mbbl <- log(Oil_1980$mbbl)\n\n# Building a linear model\nmodel_lm <- lm(mbbl ~ year, data = Oil_1980)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log <- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Evaluating the model\npredict_lm <- evaluate_model(model_lm, data = Oil_1980)\npredict_lm_log <- evaluate_model(model_lm_log, data = Oil_1980)\n\n# Transforming the model output values back to normal\npredict_lm_log$model_output_nonlog <- exp(predict_lm_log$model_output)\n\n# Calculating the mean square errors\nmean((predict_lm$mbbl - predict_lm$model_output)^2, na.rm = TRUE)\n\n[1] 17671912\n\nmean((predict_lm_log$mbbl - predict_lm_log$model_output_nonlog)^2, na.rm = TRUE)\n\n[1] 1861877\n\n\nWe get a much smaller mean predictive error for the model with log transformation as compared to the model which does not have log transformation.\nLet us calculate the effect size for these two models\n\nlibrary(statisticalModeling)\n\ndata(\"Oil_history\")\n\n# Building a linear model\nmodel_lm <- lm(mbbl ~ year, data = Oil_1980)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log <- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Calculating the effect sizes\neffect_size(model_lm, ~ year)\n\n\n\n  \n\n\neffect_size(model_lm_log, ~ year)\n\n\n\n  \n\n\n\nFor the model without a log-transformed response variable, we have slope = 254. This means, that from the year 1958 to 1988, the oil barrels production increased by 254 million barrels per year. What about the effect size of the model with a log-transformed response variable?\nThe effect size for a log-transformed value is in terms of the change of logarithm per unit of the explanatory variable. It‚Äôs generally easier to interpret this as a percentage change per unit of the explanatory variable, which also involves an exponential transformation: 100 * (exp(__effect_size__) - 1)\n\nlibrary(statisticalModeling)\n\ndata(\"Oil_history\")\n\n# Building a linear model with log transformed response variable\nmodel_lm_log <- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Calculating the effect size\neffect_lm_log <- effect_size(model_lm_log, ~ year)\n\n# Converting effect size to percentage change\n100 * (exp(effect_lm_log$slope) - 1)\n\n[1] 6.751782\n\n\nWe get a value of 6.75. This means that barrel production increased by 6.75% each year from 1958 to 1988.\nWe can take this one step further by using the bootstrapping method and thereby calculate the effect size. Then, after converting effect size to percentage change, we can calculate the 95% confidence interval on the percentage change in oil barrel production per year.\n\nlibrary(statisticalModeling)\n\ndata(\"Oil_history\")\n\n# Building a linear model with log transformed response variable\nmodel_lm_log <- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Bootstrap replications: 100 trials\nbootstrap_trials <- ensemble(model_lm_log, nreps = 100, data = Oil_1980)\n\n# Calculating the effect size\nbootstrap_effect_sizes <- effect_size(bootstrap_trials, ~ year)\n\n# Converting effect size to percentage change\nbootstrap_effect_sizes$precentage_change <- 100 * (exp(bootstrap_effect_sizes$slope) - 1)\n\n# Calculating 95% confidence interval\nwith(bootstrap_effect_sizes, mean(precentage_change) + c(-2, 2) * sd(precentage_change))\n\n[1] 6.496748 7.003000\n\n\nNice! We got a narrow 95% confidence interval of [6.50, 7.00].\n\n11.2 Rank transformation\nWe found that log transformation worked best for datasets showcasing exponential growth or where depicting percentage change of response variables makes more sense. Likewise, there is another transformation called rank transformation which works best with a dataset that deviates from normality and has outliers.\nConsider the dataset HorsePrices from the Stat2Data package in R. We will plot the prices for female horses with their age.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\n\n# Plotting the graph\nHorsePrices %>% filter(Sex == \"f\") %>%\n  ggplot(aes(Age, Price)) + geom_point() + theme_bw()\n\n\n\n\nYou can see that there are two outliers in the plot for y > 30000, let us label them.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\n\n# Making a function to identify outliers\nis_outlier <- function(x) {\n  x > 30000\n}\n\n# Filtering female horses and labelling the outliers\nfemale_horses <- HorsePrices %>% filter(Sex == \"f\") %>%\n  mutate(outliers = if_else(is_outlier(Price), Price, rlang::na_int))\n\n# Plotting the graph\nfemale_horses %>% filter(Sex == \"f\") %>%\n  ggplot(aes(Age, Price)) + geom_point() +\n  geom_text(aes(label = outliers), na.rm = TRUE, vjust = 1, col = \"red\") +\n  theme_bw()\n\n\n\n\nNow let us build a linear model with Price as the response variable and Age as the explanatory variable and then plot the model.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\n# Making a function to identify outliers\nis_outlier <- function(x) {\n  x > 30000\n}\n\n# Filtering female horses and labelling the outliers\nfemale_horses <- HorsePrices %>% filter(Sex == \"f\") %>%\n  mutate(outliers = if_else(is_outlier(Price), Price, rlang::na_int))\n\n# Building the linear model\nmodel_lm <- lm(Price ~ Age, data = female_horses)\n\n# Plotting the model\nfmodel(model_lm) + geom_point(data = female_horses) +\n  geom_text(aes(label = outliers), na.rm = TRUE, vjust = 1,\n            col = \"red\", data = female_horses) +\n  scale_y_continuous(breaks = seq(0,50000,10000), limits = c(0, 50000)) +\n  labs(title = \"Linear model\") +\n  theme_bw()\n\n\n\n\nThe outliers might be causing an effect on the slope of the regression line. This is where rank transformation comes into place. Using the rank() in R, we replace the actual value with the rank which that value occupies when arranged from ascending to descending order. Let us see how the plot will be after rank transformation.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\n# Making a function to identify outliers via ranks\nis_outlier <- function(x) {\n  x > 18\n}\n\n# Filtering female horses\nfemale_horses <- HorsePrices %>% filter(Sex == \"f\")\n\n# Assigning ranks\nfemale_horses$price_rank <- rank(female_horses$Price)\n\n# Labelling outliers via ranks\nfemale_horses<- female_horses %>%\n  mutate(outliers = if_else(is_outlier(price_rank), Price, rlang::na_int))\n\n# Building the linear model\nmodel_lm_rank <- lm(price_rank ~ Age, data = female_horses)\n\n# Plotting the model\nfmodel(model_lm_rank) + geom_point(data = female_horses) +\n  geom_text(aes(label = outliers), na.rm = TRUE, vjust = 1,\n            col = \"red\", data = female_horses) +\n  labs(y = \"Ranked Price\", title = \"Linear model with ranked price\") +\n  theme_bw()\n\n\n\n\nA quick comparison of both the graphs shows that the slope of the regression line has changed. For now, let us not worry if this made the model better. We will see in greater detail rank transformation in the coming tutorials. For now, keep in mind that rank transformation works best for data with outliers."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#collinearity-also-called-multicollinearity",
    "href": "tutorials/stat_model/inter_stat_model.html#collinearity-also-called-multicollinearity",
    "title": "Intermediate statistical modelling in R",
    "section": "\n12 Collinearity (also called Multicollinearity)",
    "text": "12 Collinearity (also called Multicollinearity)\nCollinearity is a phenomenon that occurs when two or more explanatory variables used in the model are in a linear relationship with each other. Consider a dataset with ‚Äòpoverty‚Äô as a variable, let‚Äôs say we are looking at the association between ‚Äòpoverty‚Äô and other variables such as ‚Äòeducation‚Äô and ‚Äòincome‚Äô. Now common sense tells us that, most often, education and income have a linear relationship with each other. Highly educated people often will have a high income. Therefore, changes seen in poverty status explained by education might often be a result of income rather than education itself and vice versa. In this case, we say that the variables ‚Äòeducation‚Äô and ‚Äòincome‚Äô are colinear.\nLet us look at a real-life example. Let us use the SAT dataset from the mosaicData package in R. The SAT data looks at the link between SAT scores and measures of educational expenditures.\nWe will build a linear model using sat as the response variable. The sat variable denotes the average total SAT score. We will also use expend as the explanatory variable. The expend variable denotes expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of US dollars). We will also do bootstrapping and find the 95% confidence interval.\n\nlibrary(mosaicData)\nlibrary(statisticalModeling)\nlibrary(ggplot2)\n\n# Building a linear model\nmodel_lm <- lm(sat ~ expend, data = SAT)\n\n# Plotting the model\nfmodel(model_lm) + theme_bw()\n\n\n\n# Bootstrap replications: 100 trials\nbootstrap_trials <- ensemble(model_lm, nreps = 100)\n\n# Calculating the effect sizes of salary from the bootstrap samples\nbootstrap_effect_sizes <- effect_size(bootstrap_trials, ~ expend)\n\n# Calculating the 95% confidence interval\nwith(bootstrap_effect_sizes, mean(slope) + c(-2, 2) * sd(slope))\n\n[1] -33.765958  -8.135405\n\n\nWe get a rather surprising plot and a 95% confidence interval value. The plot suggests that with increasing college expenditure, sat scores reduce. The confidence interval is adding evidence to it by showing that the mean of the effect size lies within a negative interval. So what‚Äôs happening? Should we believe our model?\nBefore we decide on anything, let us add a covariate to the model. We will add the variable frac to the model which denotes the percentage of all eligible students taking the SAT. Let us build the model.\n\nlibrary(mosaicData)\nlibrary(statisticalModeling)\nlibrary(ggplot2)\n\n# Building a linear model\nmodel_lm_cov <- lm(sat ~ expend + frac, data = SAT)\n\n# Plotting the model\nfmodel(model_lm_cov) + theme_bw()\n\n\n\n# Bootstrap replications: 100 trials\nbootstrap_trials_cov <- ensemble(model_lm_cov, nreps = 100)\n\n# Calculating the effect sizes of salary from the bootstrap samples\nbootstrap_effect_sizes_cov <- effect_size(bootstrap_trials, ~ expend)\n\n# Calculating the 95% confidence interval\nwith(bootstrap_effect_sizes_cov, mean(slope) + c(-2, 2) * sd(slope))\n\n[1] -33.765958  -8.135405\n\n\nGuess we got quite the opposite result now. Let us calculate the mean prediction error using the cross-validation technique and see which of these models is better.\n\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building a linear model\nmodel_lm <- lm(sat ~ expend, data = SAT)\n\n# Building a linear model with covariate\nmodel_lm_cov <- lm(sat ~ expend + frac, data = SAT)\n\n# Calculating the m.s.e values\ntrials_mse <- cv_pred_error(model_lm, model_lm_cov)\n\n# Printing mse\ntrials_mse\n\n\n\n  \n\n\n# Doing a t.test\nt.test(mse ~ model, trials_mse)\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = 142.26, df = 5.3575, p-value = 8.603e-11\nalternative hypothesis: true difference in means between group model_lm and group model_lm_cov is not equal to 0\n95 percent confidence interval:\n 3704.738 3838.352\nsample estimates:\n    mean in group model_lm mean in group model_lm_cov \n                  4921.496                   1149.951 \n\n\nFrom the t.test results, the model with the covariate has a lower mean prediction error value as compared to the model without the covariate. Adding frac as the covariate seems to significantly improve our model.\nThe reason why I emphasized covariates is because collinearity is introduced to the model as a result of our choice of covariates. To check for collinearity, we build a linear model with our group of explanatory variables that we want to check. Then we will find the R-squared value from the model, the greater the R-squared value, the greater the collinearity between these variables. Let us find the collinearity between expend and frac. We can either use the summary() function to get the model summary and then get the R-squared value from the summary or use the rsquared() function from the mosaic package in R.\n\nif (!require(mosaic)) install.packages('mosaic')\nlibrary(mosaicData)\nlibrary(mosaic)\n\n# Building a linear model\nmodel_cov <- lm(expend ~ frac, data = SAT)\n\n# Getting the summary of the model to get Rsquared value\nsummary(model_cov)\n\n\nCall:\nlm(formula = expend ~ frac, data = SAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7951 -0.7441 -0.2177  0.5983  2.8197 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.84179    0.26101  18.550  < 2e-16 ***\nfrac         0.03018    0.00592   5.097 5.78e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.109 on 48 degrees of freedom\nMultiple R-squared:  0.3512,    Adjusted R-squared:  0.3377 \nF-statistic: 25.98 on 1 and 48 DF,  p-value: 5.781e-06\n\n# Getting the Rsquared value\nrsquared(model_cov)\n\n[1] 0.3512072\n\n\nFrom the model summary, the value that we are interested in is given in the ‚ÄòMultiple R-squared‚Äô section, which is 0.35, which is the same as the output given by the rsquared() function. The value of 0.35 means that 35% of the variation seen in the expend variable is explained by frac. This suggests that there is some level of redundancy between the expend and frac. Greater the R-squared value, the greater the redundancy between the variables.\nR-squared is also represented in a different way called ‚Äòvariance inflation factor (VIF)‚Äô which measures the factor by which the correlations amongst the predictor variables inflate the variance of the standard error of the effect size. Okay, that was a handful, let us see what this means. First, let us see the formula for VIF;\nVIF = \\frac{1}{1-R^2}\nHere, if we have an R-squared value of 0, which means there is no collinearity between the variables, then VIF would be 1. Let us assume we get a VIF value of 2. In the current context, this means that the variance of the effect sizes calculated from bootstrap trials is increased by a factor of 2. If variance increases, then the standard error associated with the effect sizes increases and thereby it will lead to reduced precision. A result of collinearity is often getting a wider confidence interval thereby giving us an unprecise estimation of; for example effect sizes in this case.\nIf we take the square root of VIF, then we get the ‚Äòstandard error inflation factor‚Äô which tells us by how much factor the standard error of the effect sizes calculated from bootstrap trails inflate.\nLet us see all this in action from the codes below;\nWe will add another covariate to our earlier model; salary. The variable salary denotes the estimated average annual salary of teachers in public elementary and secondary schools.\nLet us first check for collinearity between the salary and expend.\n\nlibrary(mosaic)\nlibrary(mosaicData)\n\n# Building a linear model to check collinearity\nmodel_collinear_salary <- lm(expend ~ salary, data = SAT)\n\n# Getting the Rsquared value\nrsquared(model_collinear_salary)\n\n[1] 0.7565547\n\n\nLooks like we opened Pandora‚Äôs box. Now let us build two models, one without salary and one with salary and, see how the prediction error of the effect sizes changes. The two models will have the SAT scores as the response variable. We will calculate the standard error of effect sizes within the effect_size() formula by assigning bootstrap = TRUE.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\n\n# Model 1 without salary\nmodel_1 <- lm(sat ~ expend, data = SAT)\n\n# Model 2 with salary\nmodel_2 <- lm(sat ~ expend + salary, data = SAT)\n\n# Calculating effect sizes and standard error via bootstrapping\nhead(effect_size(model_1, ~ expend, bootstrap = TRUE), n = 1)\n\n\n\n  \n\n\nhead(effect_size(model_2, ~ expend, bootstrap = TRUE), n = 1)\n\n\n\n  \n\n\n\nYou can see that the standard error has increased which is due to the effect of collinearity between expend and salary.\nThe statisticalModeling package comes with the collinearity() function which can be used to calculate how much the effect size might (at a maximum) be influenced by collinearity with the other explanatory variables. Essentially, the collinearity() function calculates the square root of VIF which denotes the inflation of standard errors. Let us check the collinearity between expend and salary variables.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\n\n# Calculating the collinearity\ncollinearity(~ expend + salary, data = SAT)\n\n\n\n  \n\n\n\nInteraction between collinear variables can also increase the standard error of effect size.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\n\n# Model 3 with salary interaction\nmodel_3 <- lm(sat ~ expend * salary, data = SAT)\n\n# Calculating effect sizes and standard error via bootstrapping\nhead(effect_size(model_3, ~ expend, bootstrap = TRUE), n = 1)\n\n\n\n  \n\n\n\nAs you can see, we got a higher standard error value as compared to model_1. Let us also calculate the mean prediction error for the three models we created.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\nlibrary(dplyr)\n\n# Building the models\nmodel_1 <- lm(sat ~ expend, data = SAT)\nmodel_2 <- lm(sat ~ expend + salary, data = SAT)\nmodel_3 <- lm(sat ~ expend * salary, data = SAT)\n\n# Calculating the mean prediction error for 100 trials\nmodel_mse <- cv_pred_error(model_1, model_2, model_3, ntrials = 100)\n\n# Calculating the mean and sd of mse\nmodel_mse_mean <- model_mse %>% group_by(model) %>%\n  summarise(mean = mean(mse),\n            sd = sd(mse))\n\n# Printing the mean and sd of mse\nmodel_mse_mean\n\n\n\n  \n\n\n\nSince I did not set seed, you might not get the same results as I got. But essentially we can use the mean prediction error values to also choose the better model out of the three.\nIn the end, from these two tutorials, we can sum up our criteria for model comparison. We should check for the following while doing the comparison;\n\nCross-validated prediction error\nInflation due to collinearity\nThe standard error of effect size"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#conclusion",
    "href": "tutorials/stat_model/inter_stat_model.html#conclusion",
    "title": "Intermediate statistical modelling in R",
    "section": "\n13 Conclusion",
    "text": "13 Conclusion\nFirst of all, congratulations on completing the intermediate statistical modelling tutorial (bonus points if you completed the introduction tutorial also). In a nutshell, this is what we have learned from this tutorial;\n\nInterpreting effect size when the response variable is categorical\nPlotting model output using the fmodel() function from the {statisticalModeling} package\nInteraction terms\nPolynomial regression\nTotal and partial change\nInterpreting the R-squared value in terms of model output and residuals\nBootstrapping technique to measure the precision of model statistics\n\nScales and transformation\n\nLog transformation\nRank transformation\n\n\nCollinearity\n\nYou have mastered almost everything that is there in linear modelling. Congratulations! But, the journey is not over. The next stop is ‚ÄôGeneralized Linear Models in R`. This is a realm where we deal with complex datasets that cannot be analysed by our trusty linear regression models. Anyway, that‚Äôs a problem for tomorrow. Practice the concepts you gained from both the tutorials and come ready for the next tutorial ‚úåÔ∏è"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html",
    "href": "tutorials/stat_model/intro_stat_model.html",
    "title": "Introduction to statistical modelling in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() => null)"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#definition-of-statistical-model",
    "href": "tutorials/stat_model/intro_stat_model.html#definition-of-statistical-model",
    "title": "Introduction to statistical modelling in R",
    "section": "\n1 Definition of statistical model",
    "text": "1 Definition of statistical model\nBefore we get to know the definition of what a statistical model is, let us first try to understand what a model means. All of us have an intuitive idea of what a ‚Äòmodel‚Äô is. A model can be a fashion model posing in pretty clothes and makeup or it can be a blueprint which represents the overall layout of a building. In all these understandings of a model, we can summarise that it is representing a relationship.\nA statistical model also belongs to the same suite but is made using data. It is also representing ‚Äòrelationships‚Äô, but between variables within the data. Understanding a statistical model can lead us to understand how the variables within a dataset are related to each other.\nLet us consider the graph given below;\n\n\n\n\n\nThe x-axis shows speed and the y-axis shows stopping distance. From the graph, you can see that, as speed increases, the stopping distance also increases. Therefore we can say that the stopping distance linearly increases with speed. To showcase this linear relationship, we can plot a regression line (green line) as shown below.\n\n\n\n\n\nThe regression line can be extended to values outside the dataset and doing so would enable us to predict values outside our dataset, like predicting what the stopping distance will be if the speed was 35 mph. You can also see that many points are away from the regression line and do not lie on it, therefore the linear relationship we predicted for the variables is not a perfect one. Suppose instead of a linear relationship, we fit a polynomial regression line.\n\n\n\n\n\nIn this graph, especially at the extreme ends, most of the points are closer to the regression line than in the earlier case. Therefore, the residual distance, which is the distance between the actual data points and the fitted values by regression lines would be smaller indicating a better fit for the polynomial regression. Therefore a good fit can more accurately represent the relationship between the variables and therefore would have better predictive power. But how do we exactly compare the linear regression and the polynomial regression ‚Äòmodels‚Äô? Is there a metric which would allow this comparison easier? We will learn all about this later in this article.\nGoing back to the linear regression plot, suppose, instead of speed, we use the colour of the car and see if it affects the stopping distance at a constant speed. For obvious reasons, there should be no effect of colour on the stopping distances (duh!). Nevertheless, now we also know that some variables form relationships and some don‚Äôt.\nIn short, what we essentially were doing was linear modelling, a type of statistical modelling. And this exercise helped us realise that a model can inform us about;\n\nThe relationship between the variables in the data\nWhich variables form relationships\nPredict values outside our dataset\nModel comparisons (linear vs polynomial)\n\nThe textbook definition for statistical modelling is;\n\nA statistical model is a set of probability distributions P(S) on a sample space (S) (1).\n\nIn our previous modelling exercise, we saw how speed affects stopping distance. Here the variable ‚Äòspeed‚Äô is called the parameter or otherwise, better called the explanatory variable, which tries to explain the variations seen in the response variable, which is the stopping distance. The model we created was not perfect as there were many data points which did not lie on the regression line. One potential source of this error can arrive from the selection of the explanatory variables. The dataset had stopping distances of a particular vehicle at different speeds. The stopping distance can also be affected by the road terrain (friction) and the efficiency of the brakes, all of which were missing from the dataset. Therefore these parameters that we have not accounted for can also potentially affect our model. So why weren‚Äôt those included in the dataset? Often it is time-consuming to keep note of every parameter that can affect the variable we are interested in and therefore, they are often excluded from data collection. Thus, the model created using this data would only be a close approximation of what is happening in the real world. This particular drawback is emphasised in the following aphorism;\n\n‚ÄúAll models are wrong, but some are useful‚Äù - George Box (2)\n\nThus even though our model is not the perfect one1, if it reliably predicts values then the model is considered useful. Thus a statistical model is stochastic.\nFrom a mathematical point of view, an ideal model would contain all the parameters that affect the response variable. Thus the sample space (S) will contain all possible combinations of [explanatory variables, stopping distance] values. Therefore, the set of probability distributions corresponding to all these combinations will be P(S). But in real life, we cannot afford to measure all the parameters concerning the response variable and are only interested in a small set of variables. Thus we often have a subset of the total possible combinations of [explanatory variables, stopping distance] values (\\theta), and the corresponding probability distribution for these combinations will be P(\\theta). The model thus created with a small set of parameters is called a parametrised statistical model.\nFor all such models; P(\\theta) \\subset P(S).\nThis definition is not exactly needed to do modelling but it‚Äôs good to know the actual mathematical definition of what a statistical model is.\nNow that we have a rough idea of what a model is, let us know learn how to build models in R."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/intro_stat_model.html#making-life-easier",
    "title": "Introduction to statistical modelling in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Libraries used in this tutorial\ninstall.packages('cherryblossom')\ninstall.packages('rpart')\ninstall.packages('datasets')\ninstall.packages('NHANES')\ninstall.packages('rpart.plot')\ninstall.packages('mosaicModel')\ninstall.packages('devtools')\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading the libraries\ntutorial_packages <- c(\"cherryblossom\", \"rpart\", \"datasets\", \"NHANES\",\n                 \"rpart.plot\", \"mosaicModel\", \"statisticalModeling\")\n\nlapply(tutorial_packages, library, character.only = TRUE)\n\n# Datasets used in this tutorial\ndata(\"run17\") # From cherryblossom\ndata(\"ToothGrowth\") # From datasets\ndata(\"chickwts\") # From datasets\ndata(\"NHANES\") # From NHANES\ndata(\"Tadpoles\") # From mosaicModel\ndata(\"Used_Fords\") # From mosaicModel"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#building-a-model",
    "href": "tutorials/stat_model/intro_stat_model.html#building-a-model",
    "title": "Introduction to statistical modelling in R",
    "section": "\n3 Building a model",
    "text": "3 Building a model\nThe pipeline for building a model is as follows;\n\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\ngraph LR\n  A(Collect data) --> B(Select explanatory<br> and response variables) --> C(Select model architecture) --> D(Build the model)\n  subgraph Pipeline for making a statistical model\n  A\n  B\n  C\n  D\n  end\n\n\n\n\n\n\n\n\nThe first step in building a model is by acquiring data. Then we need to select the appropriate response variable (dependent variable) and the explanatory variables (independent variable). In a cancer drug trial experiment data, tumour size can be the response variable and the drug type and patient age can be the explanatory variables. After selecting the variables, the model architecture is chosen. In our earlier example, we used a linear model to explain the changes seen in stopping distances with speed. Choosing a model architecture depends on the nature of the data. For now, we will mostly be using the linear model lm(). But throughout the tutorial, we will also see other model architectures. The final step is to build the model, which is done by the computer.\nThe syntax for building a model in R is as follows;\n\nfunction(response ~ explanaotry1 + explanatory2, data = dataset_name)\n\n\n3.1 Linear model\nLet us try to plot some models using the linear model architecture using the lm()function in R. We will be using the run17 dataset from the cherryblossom package in R. The run17 dataset contains details for all 19,961 runners in the 2017 Cherry Blossom Run, which is an annual road race that takes place in Washington, DC, USA. Also, the Cherry Blossom Run has two events; a 10 Mile marathon and a 5 Km run. For now, we will be concerned with participants that participated in the 10 Mile marathon only.\nIn the dataset, we are interested to check whether the net time to complete the 10 Mile marathon (net_sec) is affected by the age and sex of the participant. So for making the model, we use net_sec as the response variable and, age and sex as the explanatory variables.\nWe can use the summary() function to summarise the model. For now, we will not worry about the summary details. Then, using the ggplot2 package, we will plot the model using the stat_smooth() function. Please not that there is another function geom_smooth() which is an alias of stat_smooth(). Both do the same thing.\n\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Checking headers in the data\nhead(run17)\n\n\n\n  \n\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_1 <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Get the summary of the model\n# For now don't worry about the details\nsummary(model_1)\n\n\nCall:\nlm(formula = net_sec ~ age + sex, data = run17_marathon)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2814.89  -672.65   -41.43   625.34  3112.81 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5475.8377    25.8841  211.55   <2e-16 ***\nage           17.9157     0.6789   26.39   <2e-16 ***\nsexM        -674.3765    14.8290  -45.48   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 947.5 on 17439 degrees of freedom\nMultiple R-squared:  0.1229,    Adjusted R-squared:  0.1228 \nF-statistic:  1222 on 2 and 17439 DF,  p-value: < 2.2e-16\n\n# Plotting the model using ggplot2\n# Use stat_smooth() function and specify \"lm\"\n\nrun17_marathon %>% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex\") + \n  theme_bw()\n\n\n\n\nFrom the graph, it seems that female participants take more time to complete the run compared to their male counterparts.\n\n3.2 Logistic model\nNow let us try to see whether the participant‚Äôs choice in choosing between the events is affected by their age and sex. We can hypothesize that older participants of both sexes would prefer the 5 km run over 10 Mile marathon. Thus, we use the variable event as the response variable and, age and sex as the explanatory variables. We are more interested to see the effect of age as compared to sex, therefore the variable sex is considered a covariate. We will learn later what covariate mean\nWe will use the lm() function to make a linear model. Here we will convert the event variable to boolean values for it to work with the formula. The corresponding values are; 1 = 10 Mile event and 0 = 5 Km event.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Converting event variable values into boolean \nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Building the linear model\nmodel_2 <- lm(event ~ age + sex, data = run17_boolean)\n\n# Plotting the model using ggplot2\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n\nFrom the graph, it seems like, for male participants, age doesn‚Äôt affect their event choice selection. But for females, compared to males, age does seem to affect it. Therefore older female participants prefer 5 km runs as compared to older male participants.\nLet us look at the dataset more closely, especially the response variable.\n\n# Checking the data type of the response variable\nclass(run17$event)\n\n[1] \"character\"\n\n# Printing the unique strings from the response variable\nunique(run17$event)\n\n[1] \"10 Mile\" \"5K\"     \n\n\nOur response variable is dichotomous and thus not continuous. The lm() function we used works best for continuous numerical data. So the model architecture we used was not an appropriate one here. So in these situations, we can use the logistic modelling architecture. Both logistic modelling and linear modelling are part of generalised linear modelling. We can use the glm() function in R and specify family = binomial to have a logistic model.\nThe syntax for a logistic model in R is;\n\nglm(response_variable ~ explanatory_variable, data = dataset_name, family = \"binomial\")\n\nNow let us try the glm() function and make a logistic model for the earlier case.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Converting event variable values into boolean \nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Building the logistic model\nmodel_3 <- glm(event ~ sex + age, data = run17_boolean, family = \"binomial\")\n\n# Plotting the model using ggplot2\n# Use stat_smooth() function and specify \"glm\"\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()\n\n\n\n\nThe graph looks very similar to the earlier one. For easy comparison, both the graphs are shown next to each other below. Please use the slider to compare between the graphs.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cherryblossom)\n\nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Linear model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n# Logistic model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cherryblossom)\n\nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Linear model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n# Logistic model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#model-evaluation",
    "href": "tutorials/stat_model/intro_stat_model.html#model-evaluation",
    "title": "Introduction to statistical modelling in R",
    "section": "\n4 Model evaluation",
    "text": "4 Model evaluation\nAfter building the model, we can evaluate it by providing new inputs to the model to get corresponding predicted output values. The predicted output values can then be cross-checked against the original output values and see how far off they are in terms of prediction (i.e.¬†the prediction error). Using the predict() function, we can predict for values outside the dataset or evaluate the model using the prediction error.\n\n4.1 Prediciting values\nWe will go back to the first example where we checked if the net time to complete the 10 Mile marathon (net_sec) is affected by the age and sex of the participants. We will create dummy data of participants with random age and sex values and use the model to predict their net time to complete the race.\n\nlibrary(cherryblossom)\n\n# Creating dummy data of different ages\nmale <- data.frame(\"age\" = c(seq(1, 100, 2)),\n                   \"sex\" = c(replicate(50, \"M\")))\nfemale <- data.frame(\"age\" = c(seq(1, 100, 2)),\n                   \"sex\" = c(replicate(50, \"F\")))\n\ndummy_data <- rbind(male, female)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building the linear model\nmodel_lm <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Predicting values\ndummy_data$net_sec <- predict(model_lm, newdata = dummy_data)\n\n# Plotting the predicted values\ndummy_data %>% ggplot(aes(age, net_sec, col = sex)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 100, by = 5)) +\n  labs(x = \"Age\",\n       y = \"Time to complete the marathon (sec)\",\n       title = \"Predicted values\") + \n  theme_bw()\n\n\n\n\nIn the plot, we can see that we have babies as young as 1 year old, who have been predicted to have completed the 10 Mile marathon faster that their older participants. So what on earth did we do to get these results?\n\n\nMy reaction after the model predictions\n\n\nLet us look the linear model plot that we had earlier.\n\n\n\n\n\nYou can see that there is a general trend of increasing ‚Äònet time to complete the race‚Äô as the ‚Äòage‚Äô value increases and decreasing net time as age decreases. Therefore our linear model favours reduced net times for lesser values of age, which is what we see as predicted values.\nSo our model has its limitations in predicting values for a certain range of ageüòÖ.\nThe moral of the story here is that models trained on data can be a bit wild when evaluated outside the range of the data. So we have to be mindful of its prediction abilities, otherwise, we can end up with superhuman babies who can run marathons faster than anyone.\n\n\nIdeal relationship between age and the time to complete the marathon\n\n\n\n4.2 Evaluating a model\nInstead of predicting new values outside the dataset, we can use the same dataset used for model training to predict values. Utilizing these predicted values, we can compare them back to the original values and calculate the prediction error. This is one way to compare different models and see which models predict values closer to the original values in the dataset.\nWe will use our earlier made linear model, where we looked at whether the total time to complete the race is affected by age and sex. We will also make a new model using the ‚Äòrecursive partitioning model architecture‚Äô using the same dataset so that we can have a model to compare with. Using the rpart() function in the rpart package in R, we can build a recursive partitioning model. The rpart() model works for both numerical (dichotomous and discontinuous) and categorical data. We will learn more about rpart models later in this article.\n\nif (!require(rpart)) install.packages('rpart')\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\n\n# Checking headers in the data\nhead(run17)\n\n\n\n  \n\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_lm <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Building a recursive partitioning model\nmodel_rpart <- rpart(net_sec ~ age + sex, data = run17_marathon)\n\n# Predicting values\nlm_predict <- predict(model_lm, newdata = run17_marathon)\nrpart_predict <- predict(model_rpart, newdata = run17_marathon)\n\n# Calculating error values\nlm_error <- with(run17_marathon, net_sec - lm_predict)\nrpart_error <- with(run17_marathon, net_sec - rpart_predict)\n\n# Printing few error values\nhead(as.data.frame(lm_error))\n\n\n\n  \n\n\nhead(as.data.frame(rpart_error))\n\n\n\n  \n\n\n\nNow we have data frames containing error values calculated between each of the original values in the dataset to the predicted values from the model. You can see that it‚Äôs tedious to compare the error values of the linear model to the logistic model, also some of these error values are negative, which makes the comparison even harder. So how can we know which model is better? Calculating the mean of the square of the prediction errors (m.s.e) would be a great way to start. The m.s.e will reflect the magnitude and not the sign of the errors.\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(lm_error ^ 2, na.rm = T)\n\n[1] 897630\n\nmean(rpart_error ^ 2, na.rm = T)\n\n[1] 905886.9\n\n\nThe linear model has a lower error value compared to the recursive partitioning model. Therefore the fitted values in the linear model are closer to the actual value in the dataset compare to the other model.\nWe can also plot the predicted values and see how they look. Please use the slider to compare the graphs.\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Plotting the linear model\nrun17_marathon %>% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n# Adding rpart predicted values to the dataset\nrpart_predict <- predict(model_rpart, newdata = run17_marathon)\nrun17_marathon$rpart_fitted_values <- rpart_predict\n\n# Plotting the recursive partitioning model\nrun17_marathon %>% ggplot(aes(age, rpart_predict, col = sex)) +\n  geom_line() +\n    labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (recursive partitioning model)\") + \n  theme_bw()\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Plotting the linear model\nrun17_marathon %>% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n# Adding rpart predicted values to the dataset\nrpart_predict <- predict(model_rpart, newdata = run17_marathon)\nrun17_marathon$rpart_fitted_values <- rpart_predict\n\n# Plotting the recursive partitioning model\nrun17_marathon %>% ggplot(aes(age, rpart_predict, col = sex)) +\n  geom_line() +\n    labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (recursive partitioning model)\") + \n  theme_bw()\n\n\n\n\n\n\n4.3 Choosing the explanatory variables\nIn this exercise, we will use the ToothGrowth dataset from the {datasets} package in R. The dataset is from an experiment that looked at the effect of vitamin C on tooth growth in guinea pigs. In the experiment, three different doses of vitamin C were administered to 60 guinea pigs. Vitamin C was administered either through orange juice or ascorbic acid. The length of odontoblasts (cells responsible for tooth growth) was measured to check for tooth growth.\nWe will be creating a linear model to check if tooth growth is affected by vitamin C dosages. At the same time, there is also a chance that tooth growth is affected by the method of vitamin C administration. Therefore, we have the choice of choosing between ‚Äòvitamin C dosage‚Äô and the ‚Äòmethod of vitamin C administration‚Äô as the explanatory variables. So how do we know which of these variables is better at predicting tooth growth values? To find the best predictor, we can first build a model using ‚Äòvitamin C dosage‚Äô as the only explanatory variable. Then later, we can build yet another model using both ‚Äòvitamin C dosage‚Äô and the ‚Äòmethod of vitamin C administration‚Äô as the explanatory variables. Then similiar to the ealrier case, we can use the mean of the square of the prediction errors can see which model is better.\nIn the ToothGrowth dataset, the variable len contains the length of the odontoblasts, supp contains the method of vitamin C administration (VC - ascorbic acid, OJ - Orange juice) and dose contains the dosage of vitamin C administered in mg/day units.\n\nif (!require(datasets)) install.packages('datasets')\nlibrary(datasets)\n\ndata(\"ToothGrowth\")\n\n# Building a linear model with only dose\nmodel_dose <- lm(len ~ dose, data = ToothGrowth)\n\n# Building a linear model with dose + supp\nmodel_dose_supp <- lm(len ~ dose + supp, data = ToothGrowth)\n\n# Predicting values using the trained dataset\npredict_dose <- predict(model_dose, newdata = ToothGrowth)\npredict_dose_supp <- predict(model_dose_supp, newdata = ToothGrowth)\n\n# Calculating error values\nerror_dose <- with(ToothGrowth, dose - predict_dose)\nerror_dose_supp <- with(ToothGrowth, dose - predict_dose_supp)\n\n# Calculate the mean of the square of the prediction errors\nmean(error_dose ^ 2, na.rm = T)\n\n[1] 341.2716\n\nmean(error_dose_supp ^ 2, na.rm = T)\n\n[1] 344.6941\n\n\nYou can see that the model with both vitamin C dosage and method of vitamin C administration as the explanatory variables has a greater error value compared to the model with only vitamin C dosage. Therefore adding the method of vitamin C administration as the explanatory variable did not improve the model and therefore can be excluded from the analysis.\n\n4.4 Cross validation\nSo far we have been using the training dataset for predicting values. But there is a problem in using it for our analysis as it allows models with the additional explanatory variable to have smaller prediction errors than the base model. Let‚Äôs see what this means in the following code.\nHere we use the chickwts dataset from the {datasets} package in R. In this dataset, there are two variables, weight which tells the weight (g) of the chick measured after 6 weeks and the feed variable, which tells the type of feed that was given to the chicks for those 6 weeks. There were 6 different types of feed used. The experiment was done to check whether the feed type has any effect on the weights of the chick.\nHere we will build a linear model with weight as the response variable and feed type as the explanatory variable. Then we will make a random variable within the dataset which contains values which have no explanatory power. It will be filled with random numbers which have no relationship with the original data. We will build yet another linear model like earlier but this time we shall use both the feed type and the random variable as the explanatory variable. Adding a random variable to the model should cause the model to perform poorly and therefore should lead to an increased m.s.e value when compared to the model with just the feed type variable as the explanatory variable. Let us see if that‚Äôs the case here.\n\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating a random variable\n# The variable contains random numbers\nset.seed(231)\nchickwts$random <- rnorm(nrow(chickwts))\n\n# Building a linear model with only feed\nmodel_feed <- lm(weight ~ feed, data = chickwts)\n\n# Building a linear model with dose + random\nmodel_feed_random <- lm(weight ~ feed + random, data = chickwts)\n\n# Predicting values using the trained dataset\npredict_feed <- predict(model_feed, newdata = chickwts)\npredict_feed_random <- predict(model_feed_random, newdata = chickwts)\n\n# Calculating error values\nerror_feed <- with(chickwts, weight - predict_feed)\nerror_feed_random <- with(chickwts, weight - predict_feed_random)\n\n# Calculate mean of the square of the prediction errors (m.s.e)\nmean(error_feed ^ 2, na.rm = T)\n\n[1] 2754.31\n\nmean(error_feed_random ^ 2, na.rm = T)\n\n[1] 2725.115\n\n\nWell, quite a surprise, right? The model with the random variable is having a lower error value as compared to the model with the correct explanatory variable. We will see later (not in this article though) why the error value was lower. But for now, keep in mind that when you use the same dataset to do both training and prediction in a model, the model with the additional explanatory variable will have smaller prediction errors than the base model, as seen here. Therefore this mistake can throw off our analysis and gives us a false positive that some of the explanatory variables form a relationship with the response variable when in reality there is no effect.\nTo mitigate this problem, we use a technique called ‚Äòcross-validation‚Äô. In this technique, we split the original data into two parts; a training set and a testing set. Both sets will have data points that are chosen at random from the original dataset. We train our model using the training set and then test our model using the testing set and then calculate the m.s.e. value. Thus the explanatory variable values in the testing set will be novel to the model. Let us see how we can do this.\nWe will be reusing our earlier example of chick weight and feed type.\nIn the code given below, the rnorm(nrow(chickwts)) > 0 function will assign TRUE and FALSE values at random to the row values. Looking at this function more closely, the rnorm() function will choose random numbers up to the number of rows in the dataset, and since there is an inequality (greater than zero), it will assign TRUE if the random number is greater than zero and vice versa. The row values with TRUE will go to the training set and rows with FALSE will form the testing set.\n\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating training set\nset.seed(231)\nchickwts$training_set <- rnorm(nrow(chickwts)) > 0\nchickwts$random <- rnorm(nrow(chickwts)) > 0\n\n# Building the linear model using the training set\nmodel_feed <- lm(weight ~ feed, data = subset(chickwts, training_set))\n\n# Building the linear model using the training set but with random variable\nmodel_feed_random <- lm(weight ~ feed + random, data = subset(chickwts, training_set))\n\n# Predicting values using the testing set\n# !training_set means row values with FALSE value\npredict_feed <- predict(model_feed, newdata = subset(chickwts, !training_set))\npredict_feed_random <- predict(model_feed_random, newdata = subset(chickwts, !training_set))\n\n# Calculating error values using the testing data\nerror_feed <- with(subset(chickwts, !training_set), weight - predict_feed)\nerror_feed_random <- with(subset(chickwts, !training_set), weight - predict_feed_random)\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(error_feed ^ 2, na.rm = T)\n\n[1] 2830.79\n\nmean(error_feed_random ^ 2, na.rm = T)\n\n[1] 2732.715\n\n\nFor the seed I have set, using the cross-validation method, we seem to not solve the problem we had earlier. The model with the random variable as the explanatory variable still has a lower error value as compared to the model without the random variable. But this was just an opportunistic case, as the training and testing sets are chosen at random. You might not get the same result as mine if you run this code (provided that the set.seed() is changed). We can deal with this randomness by rerunning the calculation many times to get a more consistent measure of the error value.\nWe will use the cv_pred_error() function from the statisticalModeling package to rerun the calculations many times. The function automatically makes the training and testing sets using the original dataset and also calculates the m.s.e for each trial. In the code given below, we store the results from the cv_pred_error() function into a variable called ‚Äòtrials‚Äô. The variable ‚Äòtrials‚Äô will have two columns in it; mse which denotes the mean of the square of the prediction errors (m.s.e) and model which denotes the name of the model given as input. Then in the final step, we compare the m.s.e values between the model using a simple t-test.\n\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\nlibrary(statisticalModeling)\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating a random variable\n# The variable contains random numbers\nset.seed(231)\nchickwts$random <- rnorm(nrow(chickwts)) > 0\n\n# Building a linear model with only feed\nmodel_feed <- lm(weight ~ feed, data = chickwts)\n\n# Building a linear model with dose + random\nmodel_feed_random <- lm(weight ~ feed + random, data = chickwts)\n\n# Rerunning the models (100 times for each model)\ntrials <- cv_pred_error(model_feed, model_feed_random, ntrials = 100)\n\n# Compare the two sets of cross-validated errors\nt.test(mse ~ model, data = trials)\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = -5.9019, df = 197.95, p-value = 1.537e-08\nalternative hypothesis: true difference in means between group model_feed and group model_feed_random is not equal to 0\n95 percent confidence interval:\n -118.53296  -59.16003\nsample estimates:\n       mean in group model_feed mean in group model_feed_random \n                       3309.232                        3398.079 \n\n\nFor \\alpha = 0.05 level of significance, we have a p-value < 0.05, which means that the mean error values between the models are not the same and are different from each other. From the t-test summary, we can see that the mean error value of the model without the random variable is lower than the model with the random variable (3309.232 < 3398.079). Therefore we can conclude that the addition of the random variable does not improve the model.\nTherefore through the cross-validation technique iterated over many times, in conjunction with the m.s.e values, we can identify which of the variables in our data should be considered as the explanatory variables.\n\n4.5 Prediction error for categorical response variable\nSo far when we were calculating the predictive error values, the response variable we had was numerical. But what if our response variable was a categorical value, then how will we compare models using the predictive error values?\nLet us go back to the run17 dataset from the cherryblossom package in R. In this dataset, we looked at whether the participants‚Äô choice of event was influenced by their age and sex. We hypothesised that the older participants of both sexes will prefer the 5 Km run as compared to the 10 Mile marathon. In the earlier example, we have used the logistic model, but here let us use the recursive partitioning model made using the rpart() function in the rpart package in R. We had learned briefly that the recursive partitioning model is appropriate when the response variable is categorical, which is the case here.\nLike earlier, we will use event as the response variable and sex and age as the explanatory variable. The hypothesis remains the same, irrespective of sex, older participants will prefer the 5 Km run as compared to the 10 Mile marathon. We will build two models, one with only age as the explanatory variable and the other with both age and sex as the explanatory variables.\nWhile predicting for values, we use type = \"class\" so that the model gives prediction values, which are either ‚Äú10 Mile‚Äù or ‚Äú5 Km‚Äù (gives prediction values as categorical values).\nThen we will evaluate these two models by comparing the m.s.e values. This will tell us whether adding the variable sex improves the model or not.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(rpart)\n\n# Creating training set\nset.seed(123)\nrun17$training_set <- rnorm(nrow(run17)) > 0\n\n# Training the model with the training set\n# Building the recursive partitioning model with just age\nmodel_rpart_age <- rpart(event ~ age, data = subset(run17, training_set))\n\n# Building the recursive partitioning model with age + sex\nmodel_rpart_age_sex <- rpart(event ~ age + sex, data = subset(run17, training_set))\n\n# Predicting values using the testing set\npredict_age <- predict(model_rpart_age,\n                       newdata = subset(run17, !training_set), type = \"class\")\npredict_age_sex <- predict(model_rpart_age_sex,\n                           newdata = subset(run17, !training_set), type = \"class\")\n\n# Printing a few row values\nhead(predict_age)\n\n      1       2       3       4       5       6 \n10 Mile 10 Mile 10 Mile 10 Mile 10 Mile 10 Mile \nLevels: 10 Mile 5K\n\nhead(predict_age_sex)\n\n      1       2       3       4       5       6 \n10 Mile 10 Mile 10 Mile 10 Mile 10 Mile 10 Mile \nLevels: 10 Mile 5K\n\nhead(run17$event)\n\n[1] \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\"\n\n\nIn the first 6 row values of the output, both models seem to agree with the values from the original dataset. But our dataset has 199961 rows of data. It would be crazy to even think that one can compare each of the row values between the models instead, we will try to quantify the error. In earlier cases, we could subtract the predicted values from the response variable values in the dataset to get the error value. But that is not possible here as the response variable is categorical.\nOne way to calculate the error values of these models is to see how many mistakes the model made. This can be calculated by checking how many times the predicted value by the model was not equal to the value in the dataset.\n\n# Calculating the sum of errors using the testing data\nwith(data = subset(run17, !training_set), sum(predict_age != event))\n\n[1] 1254\n\nwith(data = subset(run17, !training_set), sum(predict_age_sex != event))\n\n[1] 1254\n\n\nThe number of errors each model made is the same. Before we conclude anything let us try to see some more ways to quantify the error.\nInstead of sum, we can calculate the mean rate of errors also.\n\n# Calculating the mean of errors using the testing data\nwith(data = subset(run17, !training_set), mean(predict_age != event))\n\n[1] 0.1243061\n\nwith(data = subset(run17, !training_set), mean(predict_age_sex != event))\n\n[1] 0.1243061\n\n\nMean error values are the same (no surprises here).\nLet us go one step further. Till now our model predicted a deterministic value to the response variable. It can either be 10 Mile marathon or a 5 km run. Instead of this, we can use the model to predict the probability values to the categorical values present in the response variable. This means, our model will predict how likely a certain participant of a particular age and sex will choose between a 10 Mile marathon and a 5 Km run. To predict probability values, instead of type = \"class\", we will use type = \"prob\" within the predict() function.\n\n# Predicting probability values using the testing set\npredict_age_prob <- predict(model_rpart_age,\n                       newdata = subset(run17, !training_set), type = \"prob\")\npredict_age_sex_prob <- predict(model_rpart_age_sex,\n                           newdata = subset(run17, !training_set), type = \"prob\")\n\n# Comparing the predicted value to the actual value in the dataset\ntesting_data <- run17 %>% select(training_set, event) %>% filter(training_set == F)\ncompare_values <- data.frame(testing_data, predict_age_prob, predict_age_sex_prob)\n\n# Changing the column names for making sense of the column values\ncolnames(compare_values)[c(3:6)] <- c(\"Ten_Mile_age\", \"Five_km_age\", \"Ten_Mile_age_sex\", \"Five_km_age_sex\")\n\n# Printing a few row values\nhead(compare_values)\n\n\n\n  \n\n\n\nThe columns Ten_Mile_age and Five_km_age corresponds to probability values from the model_rpart_age model with just ‚Äòage‚Äô as the explanatory variable. The last two columns Ten_Mile_age_sex and Five_km_age_sex are from the model_rpart_age_sex model with both ‚Äòage‚Äô and ‚Äòsex‚Äô as the explanatory variables. The first-row value in the 10Mile_age column indicates that the model predicts a nearly 88% chance for that particular participant to be choosing the 10 Mile marathon. And with no surprise, you can see that the probability values across the models are the same (because we found that the error values are the same earlier).\nWe can condense these probability values to a single value which is called the ‚Äòlikelihood value‚Äô, which can then be used as a figure of merit to compare the models. This is similar to the mean of the square of the prediction errors (m.s.e) we had when the response variable was numerical. The likelihood values are calculated by multiplying the individual probability values. But since the probability values are in decimal values and are less than 1, multiplying them will lead to a very small value which would be difficult to compare. Therefore we first log transform our probability values and add them, which is mathematically equivalent to multiplying them before the log transformation\nSince the dataset is not ‚Äòtidy‚Äô, some codes are used to tidy it. The ‚Äú10 Mile‚Äù has a space in-between. While doing analysis, R might register ‚Äú10 Mile‚Äù as ‚Äú10‚Äù and ‚ÄúMile‚Äù. So this needs to be reformatted.\n\n# Splitting the data frame into two other data frames\n# Newly made data frames have values corresponding to the respective model\ncompare_values_age <- compare_values[,1:4]\ncompare_values_age_sex <- compare_values[,c(1,2,5,6)]\n\n# Tidying the data\ncompare_values_age$event <- recode(compare_values_age$event,\n                                   \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_km\")\ncompare_values_age_sex$event <- recode(compare_values_age_sex$event,\n                                       \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_km\")\n\n# Calculating the likelihood values\n# Ten_Mile_age and Five_km_age are column names\nlikelihood_age <- with(compare_values_age,\n                       ifelse(event == \"Ten_Mile\", Ten_Mile_age, Five_km_age))\nlikelihood_age_sex <- with(compare_values_age_sex,\n                           ifelse(event == \"Ten_Mile\", Ten_Mile_age_sex, Five_km_age_sex))\n\n# Likelihood value of model with age\nsum(log(likelihood_age)) \n\n[1] -3785.999\n\n# Likelihood value of model with age + sex\nsum(log(likelihood_age_sex)) \n\n[1] -3785.999\n\n\nAs the likelihood values are one and the same we can conclude that the variable ‚Äòsex‚Äô does not improve the model.\n\n4.6 Creating a null model\nThe null model gives the best estimate of our data when there are no explanatory variables modelled to it. The predicted value from the null model will always be a constant, no matter what testing data is provided. Let us see how to make a null model.\nWe will again use the run17 dataset from the cherryblossom package in R. To this dataset we add a new column which contains a constant value. We will make a model where net_sec is the response variable and the constant variable is the explanatory variable. We will use the recursive partitioning architecture to model the data.\n\nlibrary(cherryblossom)\nlibrary(rpart)\n\n# Creating a constant variable\nrun17$constant <- 10\n\n# Creating a null model\nnull_model <- rpart(net_sec ~ constant, data = run17)\n\n# Predicting values\n# Notice how all the values are the same\npredict_null <- predict(null_model, newdata = run17)\n\n# Prinitng a few predicted values\n# Notice how all the predicted values are the same\nhead(predict_null)\n\n[1] 5427.947 5427.947 5427.947 5427.947 5427.947 5427.947\n\n\nWe can calculate the mean of the square of the prediction errors of the null model. The null model essentially acts as a base of our model analysis, where we can compare the errors in the null model to the model of our interest.\n\nlibrary(cherryblossom)\nlibrary(rpart)\n\n# Creating a constant variable\nrun17$constant <- 10\n\n# Creating training set\nset.seed(12)\nrun17$training_set <- rnorm(nrow(run17)) > 0\n\n# Creating a null model using the training set\nnull_model <- rpart(net_sec ~ constant, data = subset(run17, training_set))\n\n# Predicting values using the testing set\npredict_null <- predict(null_model,\n                       newdata = subset(run17, !training_set))\n\n# Prinitng a few predicted values\nhead(predict_null)\n\n[1] 5436.2 5436.2 5436.2 5436.2 5436.2 5436.2\n\n# Calculating error values using testing set\nerror_null <- with(subset(run17, !training_set), net_sec - predict_null)\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(error_null ^ 2, na.rm = T)\n\n[1] 2273954\n\n# Calculating m.s.e by iterating the model 100 times\ntrials <- cv_pred_error(null_model)\n\n# Printing m.s.e values for each iteration\nhead(trials$mse)\n\n[1] 2289640 2289445 2289692 2289697 2289802"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#exploratory-modelling",
    "href": "tutorials/stat_model/intro_stat_model.html#exploratory-modelling",
    "title": "Introduction to statistical modelling in R",
    "section": "\n5 Exploratory modelling",
    "text": "5 Exploratory modelling\nTill now, we had a clear idea of the selection of the explanatory variables while making a model. But sometimes, you might just want to explore your dataset and see which variables affect your response variable without prior knowledge of it. Therefore, we can do exploratory modelling where we take a whole bunch of explanatory variables and see if any of them explain the changes seen in the response variable.\nFor this exercise, we will be using the NHANES dataset from the {NHANES} package in R. The dataset is a survey data collected by the US National Centre for Health Statistics (NCHS). A total of 75 variables concerning health are collected as data from around 7800 individuals in the US.\n\n5.1 Evaluating a recursive partitioning model\nFor our exploratory modelling, we will be using the recursive partitioning architecture. Using the dataset we will see what variables are related to depression.\nWe will use the formula Depressed ~ .\nThe single period on the right side of the Tilda indicates that we want to model using all the possible variables in the dataset. Finally, we will plot the model as a ‚Äòtree‚Äô using the rpart.plot() function from the rpart.plot package in R.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(rpart.plot)) install.packages('rpart.plot')\nlibrary(NHANES)\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Building the recursive partitioning model\nmodel_rpart <- rpart(Depressed ~ . , data = NHANES)\n\n# Plotting the model with sample sizes\nrpart.plot(model_rpart, extra = 1, type = 4)\n\n\n\n\nWe got this nice-looking tree plot with lots of information. Let‚Äôs see how to interpret them.\nThe response variable that we used was the Depressed variable in the dataset. This variable has three levels.\n\nlevels(NHANES$Depressed)\n\n[1] \"None\"    \"Several\" \"Most\"   \n\n\nThe meaning of each of these levels is; ‚ÄúNone‚Äù = No sign of depression, ‚ÄúSeveral‚Äù = Individual was depressed for less than half of the survey period days, ‚ÄúMost‚Äù = Individual was depressed more than half of the days.\nThe colour code in the plot corresponds to the levels of the response variable. In the beginning node of the tree plot, you can see the label ‚ÄúNone‚Äù with three sets of numbers. The numbers correspond to the respective sample numbers of the levels, i.e.¬†at this node, 5246 individuals belong to ‚ÄúNone‚Äù, 1009 individuals belong to ‚ÄúSeveral‚Äù and 418 individuals belong to ‚ÄúMost‚Äù. Therefore we have data for a total of 6673 individuals (5256 + 1009 + 418 = 6673). Also, in this node, the majority of individuals belong to the level ‚ÄúNone‚Äù. Therefore the node is coloured by the respective colour code for ‚ÄúNone‚Äù, which is orange colour here. You can also see that the node colour changes its brightness to correspond to the difference between the majority value and the other values.\nWorking with numbers can be tricky, so let us represent the sample sizes in percentiles for easy comparisons.\n\n# Plotting the model with percentile values\nrpart.plot(model_rpart, extra = \"auto\", type = 4)\n\n\n\n\nNow instead of the actual sample sizes, we have percentile values. Now let us look at the plot. The beginning node which contains the whole set of depressed individuals in the dataset is further split into two groups and this split is caused by the variable LittleInterest. The variable LittleInterest in the dataset denotes the self-reported number of days where the participant had little interest in doing things. And like the Depressed variable, we have three levels for the variable LittleInterest.\n\nlevels(NHANES$LittleInterest)\n\n[1] \"None\"    \"Several\" \"Most\"   \n\n\nThe meaning of these levels is the same as explained for the Depressed variable.\nThe first group contains 77% of the total depressed individuals and they recorded zero days where they had reduced interest to do things. The rest of the 23% individuals belong to the second group which showed a severe reduction or most severe reduction in interest in doing things. This second group is further split into two by the variable DaysMentHlthBad which denotes the self-reported number of days the participant‚Äôs mental health was not good out of the past 30 days. Here, 13% of people in the second group with 23% of the total set had bad mental days less than 6. The remaining 11% of people (13% + 11% = ~ 23% of the second group) had bad mental days for more than 6 days, which further splits into two by their categories in the LittleInterest variable. There is no splitting of the group after this. This is because the recursive partitioning architecture stops at a point where further subdivisions don‚Äôt lead to a big change in predictive ability.\nSo as a summary, by this exploratory modelling exercise, we were able to determine potential variables that could act as the explanatory variables for a given response variable that we are interested in. Overall this led us to understand the relationships between the variables in the dataset that otherwise would not have been possible with a simple linear model. But also keep in mind that these potential variables are not indicating a cause and effect, but rather a simple relationship."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#covariate",
    "href": "tutorials/stat_model/intro_stat_model.html#covariate",
    "title": "Introduction to statistical modelling in R",
    "section": "\n6 Covariate",
    "text": "6 Covariate\nWe briefly learned what are covariates when we were looking at the run17 dataset from the cherryblossom package in R. In that exercise, we looked at whether the participant‚Äôs choice in choosing between the events is affected by their age and sex. We hypothesized that older participants of both sexes would prefer a 5 km run over the 10 Mile marathon. Therefore we considered age as the main explanatory variable and sex as a covariate.\nCovariates are those variables which are of no immediate interest to the user but hold the potential to explain changes seen in the response variable. There is no special distinction between a covariate and an explanatory variable in the formula syntax of the function R code. It‚Äôs just a mental label given by the user.\nLet us use the Tadpoles dataset from the {mosaicModel} package in R. The dataset contains the swimming speed of tadpoles as a function of the water temperature and the water temperature at which the tadpoles had been raised. Since size is a major determinant of speed, the tadpole‚Äôs length was measured as well. It was hypothesized that tadpoles would swim faster in the temperature of water close to that in which they had been raised.\nAs an exercise, let us see if the maximum swimming speed achieved by the tadpole is affected by the temperature at which they were raised. In addition, let also add the size of the tadpole as a covariate, as size could also affect the swimming speeds. The variable vmax denotes the maximum swimming speed (mm/sec) and therefore will be our response variable. The variable group denotes whether the tadpoles were raised in a cold environment (‚Äúc‚Äù) or warm environment (‚Äúw‚Äù) and length denotes the length of the tadpole (mm). Here both group and length will be our explanatory variables where length is also our covariate.\nWhile predicting values using the model, we can keep the covariate unchanged or constant, and predict different values for the other variable.\n\nif (!require(mosaicModel)) install.packages('mosaicModel')\nlibrary(mosaicModel)\n\ndata(Tadpoles)\n\n# Building a linear model\nmodel_vmax <- lm(vmax ~ group + length, data = Tadpoles)\n\n# Predicting for vmax\npredict_vmax <- predict(model_vmax, newdata = Tadpoles)\nhead(predict_vmax)\n\n       1        2        3        4        5        6 \n27.08018 27.11388 25.32803 27.31605 27.51822 26.57476 \n\n# Keeping length constant, predicting for vmax\npredict(model_vmax, newdata = data.frame(group = \"c\" ,length = 5))\n\n      1 \n26.2378 \n\npredict(model_vmax, newdata = data.frame(group = \"w\" ,length = 5))\n\n       1 \n25.25733 \n\n\nSo while keeping length constant, our model predicts change in swimming speeds between cold and warm environments the tadpoles were raised in."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#effect-sizes",
    "href": "tutorials/stat_model/intro_stat_model.html#effect-sizes",
    "title": "Introduction to statistical modelling in R",
    "section": "\n7 Effect sizes",
    "text": "7 Effect sizes\nEffect sizes are a great way to estimate the change in the output value, given the change in the input. Thus they are a great tool to analyse covariates in the model.\nWhile calculating the effect size, the units of it depend on both the response variable and the explanatory variable for which the effect size is measured.\nIf we have a response variable called ‚Äòsalary‚Äô (Dollars) and we have a numerical explanatory variable called the ‚Äòyears of education‚Äô (years), then the effect size of ‚Äòyears of education on the ‚Äôsalary‚Äô would be in the units of dollars/year. Suppose we get an effect size of 100 dollars/year, this means that for every unit increase in years of education, the salary will increase by 100 dollars.\nHere the effect size is calculated as a rate of change and its unit will be;\n\\begin{align*}\nUnit\\,of\\,effect\\,size = \\frac{Unit\\,of\\,response\\,variable}{Unit\\,of\\,explaratory\\,variable}\n\\end{align*}\nBut if our explanatory variable is categorical, then the effect size is calculated as a difference. As categorical values have no units, the units of effect sizes here would be the same as that of the response variable.\nSuppose we have a response variable called ‚Äòrent‚Äô (Dollars) and we have a categorical explanatory variable called ‚Äòcity‚Äô. Let us say the variable ‚Äòcity‚Äô has two levels; Chicago and New York. The meaning of effect size values in this context would be the numerical difference in the response variable when input is changed from one category to another. If our effect size in moving from Chicago to New York is 500 Dollars, then this means that the rent increases by 500 when we move from Chicago to New York.\nHere the calculated effect size is will have the same unit as the response variable\nLook at the following exercise, we will use the Used_Fords dataset in the {mosaicModel} package in R. The dataset contains details about used Ford cars. We will see if the price of the cars (Price) has any relationship with both, how old the car is (Age) and the colour of the car (Color). We will build two linear models using Price as the response variable. But one model will have Age and the other will have Color as the explanatory variable. The units are; Price = USD and Age = years\nWe will calculate the effect sizes using the effect_size() function from the {statisticalModeling } package. The effect_size() function takes in two arguments; the model and a formula indicating which variable to vary when looking at the model output. The effect size to Age will be represented in dollars/year and the effect size to Colour will be represented as a difference in dollars when changing from one colour to another.\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Used_Fords)\n\n# Building a linear model with only age\nmodel_car_age <- lm(Price ~  Age, data = Used_Fords)\n\n# Calculating the effect sizes by varying age\neffect_size(model_car_age, ~ Age)\n\n\n\n  \n\n\n\nEffect size to Age is represented in the slope column. The value is -1124, which means for every unit increase in the age of the car (1-year increase), the price of the car depreciates by 1124 dollars.\n\n# Building a linear model with only colour\nmodel_car_age <- lm(Price ~  Color, data = Used_Fords)\n\n# Calculating the effect sizes by varying colour\neffect_size(model_car_age, ~ Color, Color = \"blue\", to = \"red\")\n\n\n\n  \n\n\n\nFor categorical explanatory variables, the effect_size() function automatically takes appropriate levels. However, we can manually change this behaviour. From the given code, the effect size to colour is calculated when the colour of the car changes from blue to red. Here the effect size is represented in the change column. The value is -3290. This means that the price of the car reduces by 3290 dollars when the colour of the car changes from blue to red."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#conclusion",
    "href": "tutorials/stat_model/intro_stat_model.html#conclusion",
    "title": "Introduction to statistical modelling in R",
    "section": "\n8 Conclusion",
    "text": "8 Conclusion\nI hope this article provided you with a good learning experience. In a nutshell, we learned;\n\n\nWhat is the meaning of a statistical model\n\nModel functions: lm(), glm() and rpart()\n\nModel formula: response ~ formula\n\nModel architectures: linear, logistic, recursive partitioning\n\n\n\nHow to build a model\n\nBuilding models using functions, formulas and model architectures\nPlotting the model output\n\n\n\nHow to evaluate a model\n\nUsing the model to predict values outside the data\nCalculating the mean of the square of the prediction errors\nUsing error to compare models to aid explanatory variable selection\nCross-validation technique and model iteration by cv_pred_error() function\nPrediction error for a categorical response variable\n\n\nHow to build a null model\n\nHow to do exploratory modelling\n\nEvaluation of a recursive partitioning model and plotting it\n\n\nWhat is a covariate\n\nHow to calculate effect size\n\nFor numerical explanatory variables\nFor categorical explanatory variables\n\n\n\nTake a break and practice the concepts learned from this tutorial. I will see you in the next tutorial where we will learn about ‚ÄòIntermediate statistical modelling using R‚Äô. See you there üëç"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorial posts",
    "section": "",
    "text": "ggplot2\n\n\ndata visualization\n\n\n\nLearn how to plot different types of graphs using the ggplot2 package\n\n\n\nJewel Johnson\n\n\nDec 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the aesthetics, labels and axes of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the theme and the colour palette of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggpubr\n\n\ndata visualization\n\n\n\nLearn how to make publication ready plots and visualize results of statistical tests directly on the plot using the ggpubr package\n\n\n\nJewel Johnson\n\n\nJan 6, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMaster the art of data visualization using the different packages in R"
  },
  {
    "objectID": "tutorials.html#data-manipulation-using-r",
    "href": "tutorials.html#data-manipulation-using-r",
    "title": "Tutorial posts",
    "section": "2 Data manipulation using R",
    "text": "2 Data manipulation using R\n\n\n\n\n\n\n\n\n\n\nChapter 1: Data tidying using tidyr\n\n\n\ntidyr\n\n\ndata wrangling\n\n\n\nLearn how to make your data tidy with the tidyr package\n\n\n\nJewel Johnson\n\n\nDec 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Data manipulation using dplyr (part 1)\n\n\n\ndplyr\n\n\ndata wrangling\n\n\n\nLearn how to manipulate your data with the dplyr package\n\n\n\nJewel Johnson\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Data manipulation using dplyr (part 2)\n\n\n\ndplyr\n\n\ndata wrangling\n\n\n\nLearn how to manipulate your data with the dplyr package\n\n\n\nJewel Johnson\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n\nTidy your data using powerful cleaning tools provided by the {tidyr} and {dplyr} packages in R"
  },
  {
    "objectID": "tutorials.html#introductory-statistics-using-r",
    "href": "tutorials.html#introductory-statistics-using-r",
    "title": "Tutorial posts",
    "section": "3 Introductory statistics using R",
    "text": "3 Introductory statistics using R\n\n\n\n\n\n\n\n\n\n\nIntroductory statistics with R\n\n\n\nbasic statistics\n\n\n\nLearn the basics of statistics using R\n\n\n\nJewel Johnson\n\n\nAug 31, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nBe confident in your foundations of statistics"
  },
  {
    "objectID": "tutorials.html#statistical-modelling-using-r",
    "href": "tutorials.html#statistical-modelling-using-r",
    "title": "Tutorial posts",
    "section": "4 Statistical modelling using R",
    "text": "4 Statistical modelling using R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHierarchical and Mixed Effects Models in R\n\n\n\nstatistical modelling\n\n\n\nGo beyond linear models and extend your skills to analyse non-normal datasets with random effects\n\n\n\nJewel Johnson\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized linear models in R\n\n\n\nstatistical modelling\n\n\n\nGo beyond linear models and extend them to analyse non-normal datasets\n\n\n\nJewel Johnson\n\n\nAug 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate statistical modelling in R\n\n\n\nstatistical modelling\n\n\n\nLearn about interaction terms, total and partial change, R-squared values, bootstrapping and collinearity\n\n\n\nJewel Johnson\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to statistical modelling in R\n\n\n\nstatistical modelling\n\n\n\nLearn how to build a model, predict values using it, evaluating it and plotting the model output in R\n\n\n\nJewel Johnson\n\n\nAug 4, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nEquip yourselves with powerful modelling tools to analyse data in a more creative and complex way."
  }
]