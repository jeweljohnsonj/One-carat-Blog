[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLearning starts in the embryo for fathead minnow fish\n\n\n\nresearch article\n\n\nscicom\n\n\n\nEmbryonic learning occurs when an organism can learn while in its embryo stage. Learn how researchers showed that fathead minnow embryos (Pimephales promelas) can detect‚Ä¶\n\n\n\nJewel Johnson\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe world‚Äôs largest bacteria is bigger than a housefly\n\n\n\nresearch article\n\n\nscicom\n\n\n\nMeet Thiomargarita magnifica, a recently discovered species of bacteria that is a whopping 1 cm in length, making it the largest bacteria ever till now.\n\n\n\nJewel Johnson\n\n\nJul 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion to keep a movie and a book list tracker\n\n\n\nNotion\n\n\nproductivity\n\n\n\nThis a tutorial on how you can use Notion to maintain reading and watch list tracker\n\n\n\nJewel Johnson\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe story behind one carat blog\n\n\n\nnews\n\n\n\nIn this article, I share my experiences on how this blog was made. Spoiler alter: it involved a lot of R package hitchhiking\n\n\n\nJewel Johnson\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion as your task manager\n\n\n\nPhD\n\n\nNotion\n\n\nproductivity\n\n\n\nIn this article I will showcase to you a template that I made, which can intelligently record and track tasks: you very own personal task manager with the help of Notion\n\n\n\nJewel Johnson\n\n\nJun 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion and Zotero to build a literature tracker\n\n\n\nPhD\n\n\nNotion\n\n\nZotero\n\n\nproductivity\n\n\n\nIn this tutorial we will learn how we can use Notion to make a literature database. With the help of the Notero plugin, our database will be synced with our Zotero‚Ä¶\n\n\n\nJewel Johnson\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion to keep a research diary\n\n\n\nPhD\n\n\nNotion\n\n\nproductivity\n\n\n\nThis a tutorial on how you can use Notion to maintain a research diary if you are a researcher or a PhD student\n\n\n\nJewel Johnson\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Notion story\n\n\n\nPhD\n\n\nNotion\n\n\nZotero\n\n\nproductivity\n\n\n\nNotion is a powerful productivity software that implements database creation in its core function. In this article, I will share my story on what led me to use Notion and‚Ä¶\n\n\n\nJewel Johnson\n\n\nJun 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe World Happiness Report 2022\n\n\n\nmaps\n\n\nleaflet\n\n\n\nPlotting an interactive map using the {leaflet} package in R\n\n\n\nJewel Johnson\n\n\nMay 20, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "one carat blog",
    "section": "",
    "text": "My name is Jewel Johnson and I am an aspiring ecologist who is interested in behaviour, vision and in the effect of human activities on animals (especially insects). I did my master‚Äôs in Biology from the Indian Institute of Science Education and Research, Thiruvananthapuram (IISER-TVM). My master‚Äôs thesis was on understanding the Visual ecology of the giant honeybee (Apis dorsata) under the guidance of Prof.¬†Hema Somanathan at IISER-TVM. Currently, I am looking for research labs to join for a PhD üòá\nI made this blog using the quarto package. My blog posts are generally focused on science communication and R programming. I also believe that teaching is the best way to learn anything, so I make extensive tutorials on the things I learn. You can find them all in the ‚ÄòTutorials‚Äô section in the navigation bar. Hope you like my blog ‚ò∫Ô∏è\nBlog history and updates"
  },
  {
    "objectID": "posts/notion_intro/index.html",
    "href": "posts/notion_intro/index.html",
    "title": "My Notion story",
    "section": "",
    "text": "Notion is a productivity software that can be used to create databases, manage tasks, take notes etc.\nDownload Notion and using the links given below, ‚ÄòDuplicate‚Äô the templates to your workspace\nMade five templates using Notion which features a literature tracker, research diary, task manager, book list tracker, movie list tracker and a finance tracker\n\nThose who want to download all the templates in one go, click here"
  },
  {
    "objectID": "posts/notion_intro/index.html#what-is-notion",
    "href": "posts/notion_intro/index.html#what-is-notion",
    "title": "My Notion story",
    "section": "\n2 What is Notion?",
    "text": "2 What is Notion?\nNotion is a productivity software which can be used to create databases, manage tasks, take notes etc. Its best known for its ability to make large collaborative databases where you can invite and work together with other people. In simple words, Notion can give you a personal Wiki experience by helping you manage your everyday activities.\n\n\n\nMy Notion dashboard\n\n\n\nHere is an introduction video from Notion themselves."
  },
  {
    "objectID": "posts/notion_intro/index.html#my-notion-story",
    "href": "posts/notion_intro/index.html#my-notion-story",
    "title": "My Notion story",
    "section": "\n3 My Notion story",
    "text": "3 My Notion story\nMy journey in discovering Notion came from my quest to find software that can track the research papers that I read. Naturally for this purpose, I began to use a reference manager software, Zotero, which is free to use and is open source. Zotero even comes in with a built-in pdf reader which you can use to highlight, comment and insert notes within a research paper This was very helpful as it enabled me to write my thoughts about the paper and summarize its results, which can be helpful if you are writing a research paper for yourself where you will be citing other papers. But soon I realized a big problem. The problem is that Zotero does not provide a clean user interface to show the summary notes I made on a research paper. What I was looking for was a tabular interface which had columns titled ‚Äòtitle of the paper‚Äô, which is followed by the ‚Äòsummary‚Äô column, where I list down a summary of the paper as bullet points. The rest of the columns can contain other relevant information about the paper.\n\n\nThe table interface I was looking for\n\n\n\n\n\n\n\n\n\nTitle\nAuthors\nDate added\nYear of publication\nSummary\nURL\n\n\nGlyphosate impairs collective thermoregulation in bumblebees\n(Weidenm√ºller et al., 2022)\n22-June-2022\n2022\n1. Glyphosate sugar water reduced life expectancy of worker bees2. Glyphosate affected bees invest less time in incubating the brood\nhttps://www.science.org/doi/10.1126/science.abf7482\n\n\n\nNow Zotero is not expected to have features like this simply because it is a reference manager and not a note-taking application. So I began looking for an application which can provide this exact feature. A simple Google search lead me to people suggesting Microsoft Excel and Google docs as their note-taking apps for summarizing the research papers they read. It sure works but is extremely cumbersome. You have to manually fill in all the columns and that is unnecessary work. Also, it‚Äôs not an elegant way of categorizing the data. I wanted something highly customizable and at the same time automated to an extent. Something similar to Zotero, where you can just simply add the pdf file of a research article and it automatically populates relevant details like Title, Authors, Year etc by grabbing information from the internet. And to my surprise, I came across a tweet on Twitter which had exactly what I was looking for. Thank you @thoughtsofaphd for that tweet.\n\n\n\nLiterature Tracker 2.0: Create a FREE customizable database in @airtable for keeping track of all the papers you download (and maybe readüòÇ). Please share if you find it helpful!‚ö°Ô∏èHere‚Äôs an easy step-by-step guide: https://t.co/jBQ0XeQahgcc @AcademicChatter pic.twitter.com/jLAkIlM2wQ\n\n‚Äî thoughtsofaphd (@thoughtsofaphd) January 7, 2022\n\nIn the tweet, I learned about a new software called Airtable and by following the guide given in the tweet, I made myself a literature tracker for managing the summaries of the papers I read.\n\n\n\nMy literature tracker template in Airtable\n\n\n\nEverything was going smoothly and I was in love with the new user interface Airtable provided. But I was still annoyed by the fact that I have to manually type in the basic information like ‚Äúname of the paper‚Äù, ‚Äúauthors name‚Äù etc. and quite frankly it was becoming quite tedious as the number of papers in my collection grew in number. Humans aren‚Äôt meant to write standard titles. If there was some way to connect Zotero to Airtable that would have made my life easy. So upon searching for a solution I came to know that there is an integration feature developed by Avana Vana. But upon reading about the feature on Github I quickly realized that it was beyond my current ability to comprehend what was happening and how to implement it. So I did what any honest person would have done after that. I gave up on Airtable and went to look for an alternative. And that‚Äôs how I came to meet Notion!\nAs I began learning about Notion I quickly realized the rich potential it offered. Initially, I just wanted a tabular database where I could efficiently write down the summaries of the papers that I read. But upon seeing how others use Notion in their daily life, I couldn‚Äôt help myself to try those features, expand on them and implement them in my day-to-day activities. So after putting in some hours in learning Notion, I was able to create a beautiful template which housed a database storing all the details of the papers I read. In addition to this template, I also made a few other ones focusing on various other day-to-day activities.\n\n\n\nMy literature tracker template in Notion\n\n\n\nThis new template is a significantly improved version of the Airtable template I had before. One of the best improvements was that this one is now automated. So no more wasting time by manually typing words in boring columns. With the help of the plugin Notero by David Vanoni, you can seamlessly integrate your Zotero literature collection with your workspace in Notion. This plugin was a game-changer for me. And thus began my Notion journey starting from this literature tracker template. In addition to this literature tracker template, I also made a few other ones focusing on various other day-to-day activities. If you are interested in how to use the template in Notion and how to get them, I have written separate articles, focusing on each of these templates. You can learn more about this at the end of this article."
  },
  {
    "objectID": "posts/notion_intro/index.html#do-you-need-notion",
    "href": "posts/notion_intro/index.html#do-you-need-notion",
    "title": "My Notion story",
    "section": "\n4 Do you need Notion?",
    "text": "4 Do you need Notion?\nConsider Notion as a LEGO set that you had when you were a child. You can build almost anything with Notion and it is tailored to give you a personal Wiki experience. You are mostly only limited by your creativity and needs. The learning curve for Notion is not a hard one and it‚Äôs pretty much self-intuitive. Whatever I was able to do until now required only a week of self-learning. With that said you can also get a wide variety of templates, mostly for free which can satisfy most of your needs. Most features of Notion are free for personal use but they also have different subscription-based plans for advanced power users. You can learn more about their plans here. Also, keep in mind that you need an active internet connection to use Notion.\nNow let us answer the elephant in the room. Is it secure? The answer is yes and no. Notion uses encryption to encrypt data when it is been sent and received between the user and Notion. But created databases that are stored in Notion‚Äôs cloud service are not encrypted. This means employees of Notion can access your data. Notion has a strict data security policy and claims that they will only access user data with prior permission from the users themselves. The reason why they don‚Äôt use end-to-end encryption is because it would make certain features like ‚Äòfull-text search‚Äô impossible to implement. As for Notion‚Äôs data security commitment, they are part of the Security First Initiative which pledges to put security first by sharing their security information proactively with their customers. So in my opinion I think it‚Äôs safe to use Notion unless you create tables containing your credit card info or account passwords. If you are a researcher, I would also recommend that you do not attach any sensitive research data to Notion. With over 30 million users and with strong ideals on user security and data protection, Notion is as good as any other productive software like Slack, Evernote etc. in terms of data security. So I don‚Äôt think you will face any problem when adopting Notion for your daily activities."
  },
  {
    "objectID": "posts/notion_intro/index.html#installing-notion",
    "href": "posts/notion_intro/index.html#installing-notion",
    "title": "My Notion story",
    "section": "\n5 Installing Notion",
    "text": "5 Installing Notion\nIf you are interested to try out Notion and seeing if it works for you, then please follow these steps.\n\nGo to Notion website and sign up for a free account. You can use your Gmail account for a quick sign-in.\nNotion can be used from your browser itself. But if you want to work from your desktop, please install the desktop app.\nNow you are ready to use Notion.\n\nIt can be pretty overwhelming at first, but as soon as you get familiar, you will learn to do a lot of cool new stuff with it. There are many YouTube videos and web articles that will walk you through the basics of Notion. A good place to start is by watching this YouTube video by Notion itself where they introduce the basics of Notion.\n\n\nNow my goal is to not make you watch multiple tutorial videos and bore yourself to death. Instead, I would like you to have a goal in mind. How I came to learn Notion is that, as I mentioned before, I wanted to have a ‚Äòliterature tracker‚Äô, so having a goal sets me up to see which features in Notion would help me build it. As Notion has a lot of features it can be too much on your plate in trying to learn about all these features. So proceed to go goal by goal. Rome wasn‚Äôt built in a single day! If you are learning any new skill it‚Äôs always helpful to visualize why you are learning this skill and what you want to use it for? A sense of purpose can make the learning process more enjoyable and satisfying in the end.\nNow I am aspiring to be a researcher, so naturally, that made me interested in using Notion for what would be my plausible PhD journey in the future. For this reason, I have made a couple of templates tailored to ease up my workflow when I start my PhD. At the time of writing this article, I am using Notion as a literature tracker, research diary (I will start using it once I get a PhD position), task manager, movie and book list tracker and as a simple finance tracker. If any of these templates interest you, then you are lucky as I have made all of them free for use! Furthermore, I have also written tutorials for some of these templates which you can read and get a better idea of how to use them."
  },
  {
    "objectID": "posts/notion_intro/index.html#tutorials-on-the-templates",
    "href": "posts/notion_intro/index.html#tutorials-on-the-templates",
    "title": "My Notion story",
    "section": "\n6 Tutorials on the templates",
    "text": "6 Tutorials on the templates\n\n\nUsing Notion as a literature tracker.\n\nUsing Notion to keep a research diary.\n\nUsing Notion as a task management software.\n\nUsing Notion as a reading list tracker and a movie list tracker.\n\nUsing Notion as a simple finance ledger. (I am still in the process of developing it, you can download the current version from the link)\n\nIf you want all these templates at once, then I have made a master template which has all of them together. You can get it here.\nI hope you find the templates useful and I wish you all the best in learning Notion. You can share your feedback and thoughts about the templates in the comment section below."
  },
  {
    "objectID": "posts/notion_intro/index.html#useful-references",
    "href": "posts/notion_intro/index.html#useful-references",
    "title": "My Notion story",
    "section": "Useful References",
    "text": "Useful References\n\nBeginners introduction to Notion. Source\n\nBasics of Notion. Source\n\nQuick tutorial on setting up Notion. Source\n\nNotion official website has detailed guides for very single feature in Notion. Source\n\n\nLast updated on\n\n\n[1] \"2022-07-01 19:51:49 IST\""
  },
  {
    "objectID": "posts/notion_literature/index.html",
    "href": "posts/notion_literature/index.html",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "",
    "text": "Use Notion to build a literature database that can integrate with directories in Zotero with the help of the Notero plugin\nInstall Notion, Zotero and the Notero plugin\nDuplicate the literature template to your Notion database"
  },
  {
    "objectID": "posts/notion_literature/index.html#tldr",
    "href": "posts/notion_literature/index.html#tldr",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "\n1 TL;DR",
    "text": "1 TL;DR\n\nUse Notion to build a literature database that can integrate with directories in Zotero with the help of the Notero plugin\nInstall Notion, Zotero and the Notero plugin\nDuplicate the literature template to your Notion database"
  },
  {
    "objectID": "posts/notion_literature/index.html#literature-tracker-using-notion-and-zotero",
    "href": "posts/notion_literature/index.html#literature-tracker-using-notion-and-zotero",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "\n2 Literature tracker using Notion and Zotero",
    "text": "2 Literature tracker using Notion and Zotero\nThis is a follow-up article from my earlier post on My Notion story. If you don‚Äôt know what Notion is or how to install it, please refer to my earlier article for the background information.\nIn this article, we will aim to build a literature tracker that looks similar to the one below. It will be automated via the Notero plugin integrating Zotero with Notion.\n\n\n\nMy literature tracker template in Notion\n\n\n\n\n2.1 Install Zotero\nThe first thing to do is to download and install Zotero, a free-to-use reference manager that you can use to categorize and manage your research article collection. You should also create an account in Zotero and log in using that account in your Zotero app.\n\n2.2 Installing Notero plugin\nNow we are going to install a plugin for Zotero, called Notero by David Vanoni, which acts as a link between Zotero and Notion. This is what syncs your Zotero library to your Notion database.\n\nGo to the Github page of Notero and download the notero-0.3.5.xpi file under the assets section. You can right-click on the file and save it by clicking on ‚Äòsave link as‚Äô\n\n\n\n\n\n\n\nVideo: Downloading the plugin (click here)\n\n\n\n\n\nVideo\n\n\n\n\nGo to the Zotero app and click on ‚ÄòTools‚Äô and then click ‚ÄòAdd-ons‚Äô. Then in the new window, click on the Settings button and then click on ‚ÄòInstall add-on from file‚Äô. Then browse to where your notero-0.3.5.xpi file is downloaded and then install it.\n\n\n\n\n\n\n\nVideo: Installing the plugin\n\n\n\n\n\nVideo\n\n\n\n\n2.3 Configure Notero plugin\n\nAfter installing Notero, you can find a new option called ‚ÄòNotero Preferences‚Äô under the ‚ÄòTools‚Äô section. To have the plugin work, you need to provide an ‚Äòintegration token‚Äô and a ‚Äòdatabase ID‚Äô.\nTo get the integration token, go to Notion integrations and click on ‚ÄòNew integration‚Äô. Give a suitable name (something like ‚ÄòNotero integration‚Äô for the ease of finding it) and then select your workspace in Notion. You can keep the rest of the options in their default setting and then click save. You will then get your integration token.\n\n\n\n\n\n\n\nVideo: Getting the integration token\n\n\n\n\n\nVideo\n\n\n\n\nGo back to your Notion app and then select the database which you want to integrate with the Notero plugin. If you are creating a database from scratch then make sure that the table that you are creating has the following named columns.\n\n\n\nImage from Notero Github page\n\n\nIf you want to use my template then click here and click on Duplicate. It contains most of the relevant columns and will save you time from creating it from scratch.\n\n\nDuplicating a template in Notion\n\n\n\nOnce you have the database that you want to integrate with Notero, click on the ‚ÄòShare‚Äô button and click on the ‚Äòcopy link‚Äô. Paste the link in a text editor and copy the first 36 characters after your workspace name. This is your ‚Äòdatabase ID‚Äô.\n\n\n\n\n\n\n\nThe format of the copied link would in this form\n\n\n\nhttps://www.notion.so/{workspace_name}/{database_id}?v={view_id}\nCopy the {database_id} part\n\n\n\n\n\n\n\n\nVideo: Getting the database ID\n\n\n\n\n\nVideo\n\n\n\n\nThen finally paste your ‚Äòintegration token‚Äô and the ‚Äòdatabase ID‚Äô into the ‚ÄòNotero Preferences‚Äô window under the ‚ÄòTools‚Äô section in Zotero. Then select the directory in Zotero that you want to integrate with your Notion database. Now you have successfully configured the Notero plugin.\n\n\n\nNotero Preferences window\n\n\nNow go to the associated database and click ‚ÄòShare‚Äô. Search for your integration and click invite. Any research article that is present in your directory associated with the Notero plugin will now automatically be synced to your associated Notion database. Notero plugin allows one-time sync between Zotero to Notion but not the other way around. So any changes that you do in the Notion database won‚Äôt be reflected in your Zotero database.\n\n\n\n\n\n\nVideo: Notero in action\n\n\n\n\n\nVideo\n\n\n\nThere is a reason why I said it‚Äôs a one-time sync, any modifications that you do for already existing files won‚Äôt be reflected in Notion. But this can be easily fixed by assigning the existing papers to a tag, you can also delete it if you dont want it to appear in the database.\n\n\n\n\n\n\nVideo: Updating existing files with a new tag\n\n\n\n\n\nVideo\n\n\n\nThere are some limitations on what the plugin is capable of doing, but it‚Äôs still better than nothing. You can learn more about the plugin from its Github repo page.\nSo there you have it, your very own literature tracker made using Notion and integrated with Zotero using the Notero plugin. I am so proud of you üëç\nThank you David Vanoni and the developers who contributed to developing the Notero plugin. You can share your feedback and thoughts about the templates in the comment section below.\nLast updated on\n\n\n[1] \"2022-08-05 11:14:02 IST\""
  },
  {
    "objectID": "posts/notion_movie_books/index.html",
    "href": "posts/notion_movie_books/index.html",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "",
    "text": "Use Notion to track your movie and book lists\nDuplicate the movie list tracker and the book list tracker templates to your Notion database and use them."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#using-notion-to-keep-a-movie-list-tracker",
    "href": "posts/notion_movie_books/index.html#using-notion-to-keep-a-movie-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n2 Using notion to keep a movie list tracker",
    "text": "2 Using notion to keep a movie list tracker\nThis was a fun little project where I made two templates that helps in maintaining reading and a watch list tracker for books and movies. I heard from my friends who are doing PhD that they spent a hefty amount of time binge-watching movies and series and most of them also read a lot of new books. So I thought why not make a template that will help them manage this. The templates are very simple and visually pleasing.\nThis might be my shortest article ever on this blog but I thought this post can act as a source to receive feedback and comments about the templates. The links for the templates are given below.\n\nMovie list tracker\nBook list tracker"
  },
  {
    "objectID": "posts/notion_movie_books/index.html#movie-list-tracker",
    "href": "posts/notion_movie_books/index.html#movie-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n3 Movie list tracker",
    "text": "3 Movie list tracker\n\n\n\nMovie list tracker\n\n\n\nYou can categorise the list to their ‚Äòwatch status‚Äô and also indicate if it‚Äôs a movie or a series. Furthermore, if watch a lot of series, then you can also add in the extra columns indicating the season and so on. In the poster column, I have embedded an online image link showcasing the movie or series poster. This is done so that the media collection view (gallery view) can use these images and show visually pleasing cards. You can get the poster image links for most of the movies and series from here. The progress section shows the data in a ‚Äòkanban‚Äô view to quickly visualize the watch list according to their status."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#book-list-tracker",
    "href": "posts/notion_movie_books/index.html#book-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n4 Book list tracker",
    "text": "4 Book list tracker\n\n\n\nMovie list tracker\n\n\n\nThe book list tracker is almost identical to the movie list one. To get the book cover image links for the database, you can visit here. The way you add entries is also pretty self intuitive so I urge you to explore it yourself."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#conclusion",
    "href": "posts/notion_movie_books/index.html#conclusion",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion\nI hope you find these templates useful. You can give your feedback and thoughts about the template in the comment section below this article.\nLast updated on\n\n\n[1] \"2022-07-01 19:52:02 IST\""
  },
  {
    "objectID": "posts/notion_research_diary/index.html",
    "href": "posts/notion_research_diary/index.html",
    "title": "Using Notion to keep a research diary",
    "section": "",
    "text": "Use Notion to write and maintain a research diary\nDuplicate the diary template and use it in Notion"
  },
  {
    "objectID": "posts/notion_research_diary/index.html#using-notion-to-keep-a-research-diary",
    "href": "posts/notion_research_diary/index.html#using-notion-to-keep-a-research-diary",
    "title": "Using Notion to keep a research diary",
    "section": "\n2 Using notion to keep a research diary",
    "text": "2 Using notion to keep a research diary\nIn this article, we will see how we can use Notion to keep a research diary. If you are a researcher or a PhD student, then a research diary would be a very important piece of intellectual work that will be staying with you for quite a few years. So it must be well written, well-formatted and efficiently organised. With the power of Notion, I will show you a template that I made that can be used in Notion and can act as an electronic research diary.\nYou can get the template by clicking here. Duplicate it into your Notion workspace.\n\n\n\nResearch diary database\n\n\n\nThe picture above shows the research diary database. Each row value created in the table will be each day‚Äôs entry. If you also have a literature tracker database in Notion workspace, then you can couple it with your diary entry and it will showcase any papers that you have read one that day.\nSo let us try writing an entry. First, make a blank row entry and then hover over the newly created empty row and click ‚ÄòOPEN‚Äô. This will open a new window. In that window click on ‚ÄòDiary entry template‚Äô. This will bring you a sample template. You can fill in relevant info or add in your subsection and modify the entry.\n\n\n\nResearch diary template\n\n\n\n\nThere are mainly five sections in the template. In the first section, you can outline what experiments or analyses or any other related work you did on that day. You can also attach any relevant pictures if it helps.\nIn the second section, you can outline the challenges you came across while implementing your work. It can be problems you encountered in your experiment or a bug in your code while doing data analysis etc.\nThe third section outlines your thoughts and ideas, it can either be possible solutions that you want to try out with regards to the challenges you faced or a new research question that you had while reading a paper.\nIn the fourth section of the template, I have attached my literature tracker database to the diary entry page. Now if I happened to read any research articles on that day, then by filtering the database to the diary entry date, I can have all the papers that I read on that date show up. This can be pretty convenient to track your reading goals. You can extend this idea and showcase any experiments that you have done on that day if you have an experiment database in your Notion database.\nThe fifth and last section of the template can have any other relevant work information which does not fall into the earlier section. It can be purchase reminders, suggestions by your supervisors or colleagues, deadlines for meetings or presentations etc.\n\nThis is a simple research diary template that I have created and I hope you find it useful. Suggestions and feedback to improve the diary template are most welcome. Please comment your thoughts below the article. Thanks!\n\n\n\n\n\n\nVideo: Making a diary entry (click here)\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/notion_research_diary/index.html#useful-references",
    "href": "posts/notion_research_diary/index.html#useful-references",
    "title": "Using Notion to keep a research diary",
    "section": "Useful References",
    "text": "Useful References\n\nThe research diary image in the thumbnail for this post belongs to Dr.¬†Julia Everitt (@juilaeverittdr). You can read more about research diary here.\n\nLast updated on\n\n\n[1] \"2022-07-03 10:28:17 IST\""
  },
  {
    "objectID": "posts/notion_task_manager/index.html",
    "href": "posts/notion_task_manager/index.html",
    "title": "Using Notion as your task manager",
    "section": "",
    "text": "Use Notion to manage your daily tasks\nFour templates to download; To-do-list, Completed task list, Task manager and Task dashboard\n\nThose who want to download all the templates in one go, click here\n\nDuplicate and use them in Notion"
  },
  {
    "objectID": "posts/notion_task_manager/index.html#using-notion-as-a-task-management-dashboard",
    "href": "posts/notion_task_manager/index.html#using-notion-as-a-task-management-dashboard",
    "title": "Using Notion as your task manager",
    "section": "\n2 Using Notion as a task management dashboard",
    "text": "2 Using Notion as a task management dashboard\nWith the ability to make databases, Notion becomes an excellent tool to manage task lists. In this article, I am going to give you a quick tour of how you can use Notion as a task management dashboard with the templates I made.\nThere are four templates to add to your Notion database and you need all four of them to function properly. If you don‚Äôt want to individually download each of them, then I have a single-page master template which features all the templates I made in one place. You can find it here.\n\n\nThe templates of interest for this article\n\n\n\n\nTo-do-list: Acts as the master database where all tasks are stored. By default only shows incomplete tasks.\n\nCompleted task list: Same template as above, but only tasks that are completed are shown. Act as an archive for completed tasks.\n\nTask manager: Task manager interface where each task is arranged and displayed categorically. By default only shows incomplete tasks.\n\nTask dashboard: A dashboard which informs you which tasks are due and how long are they due to completion."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#to-do-list",
    "href": "posts/notion_task_manager/index.html#to-do-list",
    "title": "Using Notion as your task manager",
    "section": "\n3 To-do-list",
    "text": "3 To-do-list\nFirst, let us see the To-do-list. Specifically, we will be seeing the ‚ÄòMaster table‚Äô view which lists out all the tasks which are yet to be completed.\n\n\n\nTo-do-list\n\n\n\nLike any basic to-do list, this list has task names and due date columns, along with many other columns. I have also tagged the tasks into their respective category and their priority. The ‚ÄòDate added‚Äô and ‚ÄòDue status is automatically calculated once you input a row value in the table. The ‚ÄôDue status‚Äô column is dependent on a formula which takes in the ‚ÄòDue date‚Äô value. The possible values for the ‚ÄòDue status‚Äô column are; task finished, overdue, due today, due tomorrow, someday and later. Here ‚Äòsomeday‚Äô label means that the task has no due date mentioned and the ‚Äòlater‚Äô label means that the task has at least 7 days before the due. The ‚Äòtask finished‚Äô is only given to tasks which have the ‚ÄòDone‚Äô column check box checked. The rest of the labels are what it says.\nYou can change every single aspect of this table to suit your needs. What might be tricky to change is the ‚ÄòDue status‚Äô column as it is governed by a formula which may look complex but is very easy.\n\n\n\n\n\n\nDue status formula\n\n\n\nif(prop(\"Done\") == false, \nif(formatDate(prop(\"Due date\"),\"L\") == formatDate(now(), \"L\"), \"Due today\",\nif(prop(\"Due date\") < now(), \"Overdue\",\nif(prop(\"Due date\") <= dateAdd(now(), 1, \"days\"), \"Due tomorrow\",\nif(empty(prop(\"Due date\")), \"Someday\", \"Later\")))), \"Task finished\")\n\n\nThe formula is a series of if statements. The syntax for the if statement in Notion is as follows;\nboolean ? value : value\nif(boolean, value, value)\nFrom the syntax, you can see that the if statement is dependent on a Boolean value.\nSo let us try to understand what this formula is trying to calculate, by taking each section separately.\n\n\n\n\n\n\nif(prop(‚ÄúDone‚Äù) == false\n\n\n\nThis means that if the ‚ÄòDone‚Äô column is unchecked then run the rest of the statements, if not, then it returns the label ‚ÄòTask finished‚Äô which you can see in the last part of the formula. The checkboxes in Notion follow Boolean values. Checked means true and unchecked means false.\n\n\n\n\n\n\n\n\nif(formatDate(prop(‚ÄúDue date‚Äù),‚ÄúL‚Äù) == formatDate(now(), ‚ÄúL‚Äù), ‚ÄúDue today‚Äù\n\n\n\nThis is yet another ‚Äòif‚Äô statement but this time we are reformatting the date. The reason why we are reformatting the date is that, in Notion, any date is followed by time, so 12-12-2022 is Notion is denoted as 12-12-2022 12:00 AM. So there can be some issues where ‚ÄòDue today‚Äô won‚Äôt be properly shown. So reformatting the date to ‚Äòday‚Äô units is easier to work with. The now() function returns the current date. Therefore this formula simply returns the label ‚ÄòDue today‚Äô if the due date matches the current date.\n\n\n\n\n\n\n\n\nif(prop(‚ÄúDue date‚Äù) < now(), ‚ÄúOverdue‚Äù\n\n\n\nThis formula returns an ‚ÄòOverdue‚Äô label if the due date is past the current date.\n\n\n\n\n\n\n\n\nif(prop(‚ÄúDue date‚Äù) <= dateAdd(now(), 1, ‚Äúdays‚Äù), ‚ÄúDue tomorrow‚Äù,\n\n\n\nThe dateAdd() function takes in the first value, which is the current date returned from the now() function, and then adds 1 unit of ‚Äúdays‚Äù to it. Therefore the formula boils down to checking if the due date is less than or equal to tomorrow and returns the ‚ÄòDue tomorrow‚Äô label if it does. We give less than or equal to sign because we want the counter-statement to be strictly greater than so that, only then does the rest of the if statement runs. Also, the less than condition can never be fulfilled here as the first statement checks if the due date matches the current day.\n\n\n\n\n\n\n\n\nif(empty(prop(‚ÄúDue date‚Äù)), ‚ÄúSomeday‚Äù, ‚ÄúLater‚Äù\n\n\n\nNow sometimes we don‚Äôt want to give the due date or we forgot to give them to a task. This is notified by the ‚ÄòSomeday‚Äô label. Now the ‚ÄòLater‚Äô label is given when the ‚ÄòDue tomorrow‚Äô condition fails, which means that the due date is greater than tomorrow.\n\n\nNow the reason why I went to explain this formula so elaborately is for you to appreciate the formula feature in Notion. Using clever little formulas like this you can efficiently automate many boring tasks. If it wasn‚Äôt for this formula, we would have to manually check between the current date to the due date. So something to keep in mind while designing Notion databases.\nNow Notion also features some powerful ways of visualizing tabular data via ‚Äòviews‚Äô. You can see that there is a ‚ÄòGroup table‚Äô, ‚ÄòPriority table‚Äô and a ‚ÄòStatus table‚Äô views in the database. These are nothing but the ‚ÄòMaster table‚Äô data grouped into different categories for easy visualization.\n\n\n\n\n\n\nVideo: Different views of the to-do list (click here)\n\n\n\n\n\nVideo\n\n\n\nOkay, now we have a fair idea of what is there in this database and what each of the columns does. Now the funny part is that we won‚Äôt be adding any tasks to this table using this page. Instead, we will be exclusively using the ‚ÄòTask manager‚Äô for it."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#task-manager",
    "href": "posts/notion_task_manager/index.html#task-manager",
    "title": "Using Notion as your task manager",
    "section": "\n4 Task manager",
    "text": "4 Task manager\n\n\n\n\n\n\nVideo: Task manager\n\n\n\n\n\nVideo\n\n\n\nThe task manager page is your go-to page to list out incomplete tasks. By default, the page has categorized all tasks into their respective category as subsections. The inbox section is where you add your task to the to-do list and as soon as you add the associated category, it vanishes from the inbox and goes to its category subsection. Thus you don‚Äôt have to worry about post categorizing into the subsection parts after you have added a task. And once you check the ‚ÄòDone‚Äô column for a task, it is then archived on the Completed task list page to preserve the history of completed tasks.\n\n\n\n\n\n\nVideo: Adding a task in the task manager\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/notion_task_manager/index.html#task-dashboard",
    "href": "posts/notion_task_manager/index.html#task-dashboard",
    "title": "Using Notion as your task manager",
    "section": "\n5 Task dashboard",
    "text": "5 Task dashboard\n\n\n\nTask dashboard\n\n\n\nTreat the task dashboard page as your command centre for all the tasks. Remember all those ‚Äòdue status‚Äô labels that we had in the to-do list database. With the help of Notion‚Äôs grouping feature for databases, we can easily visualize all the tasks according to their deadlines. In the ‚Äòtask list subsection,‚Äô I have also added table views showing tasks associated with a particular priority. The purpose of this page is to easily get a glance at tasks that close the due date."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#completed-tasks",
    "href": "posts/notion_task_manager/index.html#completed-tasks",
    "title": "Using Notion as your task manager",
    "section": "\n6 Completed tasks",
    "text": "6 Completed tasks\nThe completed tasks page acts as an archive showcasing all the completed tasks. As someone used to say to me, it brings great joy to tick off all those tasks from the to-do list after spending your time and energy completing them. The primary function of this page is to preserve and record task completion history. The database is identical to that of the earlier shown to-do-list database, as I have filtered it to only show row values where the ‚ÄòDone‚Äô column is checked."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#conclusion",
    "href": "posts/notion_task_manager/index.html#conclusion",
    "title": "Using Notion as your task manager",
    "section": "\n7 Conclusion",
    "text": "7 Conclusion\nI hope you have fun using this template. I enjoyed making this template and I aimed to make it simple and effective to use. To summarise, the to-do list is your main database, actually your only database. Everything else is just a modified version of this database where the info is shown in a different form using filtering and grouping features. After duplicating this template, you can tweak each of the parameters to suit your liking. You only need to change the parameters in the to-do-list database as the changes in it will be reflected in the rest of the pages. The only other thing that you will be needing to modify after you have changed the key parameters in the database are the headings, which are pretty straightforward to change. With that, I hope you find this template useful. Any feedback and suggestions are most welcome. You can comment them below the article.\nLast updated on\n\n\n[1] \"2022-07-03 10:26:59 IST\""
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html",
    "href": "posts/research_article_biggest_bacterium/index_post.html",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "",
    "text": "Summary of the study in (1)\n\n\nResearch article that is summarised in this post: Volland, J.-M., Gonzalez-Rizzo, S., Gros, O., Tyml, T., Ivanova, N., Schulz, F., Goudeau, D., Elisabeth, N. H., Nath, N., Udwary, D., Malmstrom, R. R., Guidi-Rontani, C., Bolte-Kluge, S., Davies, K. M., Jean, M. R., Mansot, J.-L., Mouncey, N. J., Angert, E. R., Woyke, T., & Date, S. V. (2022). A centimeter-long bacterium with DNA contained in metabolically active, membrane-bound organelles. Science, 376(6600), 1453‚Äì1458. https://doi.org/10.1126/science.abb3634."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#introduction",
    "href": "posts/research_article_biggest_bacterium/index_post.html#introduction",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n2 Introduction",
    "text": "2 Introduction\nWhen you think of bacteria, the first thing that might come to your mind is a microscope, given how tiny they are to see. Most of the bacteria and archaea are about ~ 2 \\mu m in size (1). The record for the smallest living organism is shared between the pathogenic bacteria, Mycoplasma pneumoniae and the Thermodiscus sp. belonging to Archae. Both can be as small as 0.2 \\mu m in size (2). The biggest bacteria was thought to be Thiomargarita namibiensis, which can be as big as 750 \\mu m in size (average size is 180 \\mu m) and can be seen with the naked eye (3). But the recent discovery of a new bacteria species has overtaken T. namibinesis to become the biggest bacteria ever. Moreover, it is about ~50 times bigger on average, compared to T. namibinesis and is seen in lengths of more than 9000 \\mu m, which is about 1cm! To put that in perspective, a housefly, which is a complex multi-cellular organism is on average only about 0.5 \\ to\\ 0.7cm long!\nMeet Thiomargarita magnifica, your newest species of bacteria, which as we know is the biggest bacteria to date. As reported by a group of researchers in a recent paper (1) in Science, there is more than just the size that makes this bacteria special. Thiomargarita magnifica is packed with so many complex features that it is blurring the lines between what we considered to be prokaryotes and eukaryotes."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#discovering-the-biggest-bacteria",
    "href": "posts/research_article_biggest_bacterium/index_post.html#discovering-the-biggest-bacteria",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n3 Discovering the biggest bacteria",
    "text": "3 Discovering the biggest bacteria\n\n\n\nFigure 1: Images (A) and (B) are taken from supplementary fig.¬†S1. in (1). (A) Shows Thiomargarita magnifica (white-filaments) attached to sunken leaves of Rhizophora mangle. (B) Buds forming on the apical pole the filament\n\n\n\nResearchers first discovered Thiomargarita magnifica attached to the sunken leaves of Rhizophora mangle (as seen in Figure¬†1) present in the shallow tropical marine mangroves from Guadeloupe, Lesser Antilles, a French overseas province. The bacteria appeared as a long-white filament and was first thought to be a fungus. It was only later through various techniques that the researchers concluded that this is not a fungus, but a new species of bacteria and rather a very long one that is. Since the bacteria is not yet culturable in laboratory conditions, under the nomenclature followed in microbiology, they are called Candidatus (Ca.) Thiomargarita magnifica."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#elucidating-the-biggest-bacteria",
    "href": "posts/research_article_biggest_bacterium/index_post.html#elucidating-the-biggest-bacteria",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n4 Elucidating the biggest bacteria",
    "text": "4 Elucidating the biggest bacteria\n\n4.1 The whole filament is a single bacterial cell\n\n\nFigure 2: Images (A), (B) and (C) are taken from supplementary fig.¬†S4. Image (D) is taken from Fig. 1. All images are from (1). In all the images the red outline represents the plasma membrane, arrowheads represent division septa and ‚ÄúV‚Äù in the image (D) represents a large central vacuole.(A) Cell membrane of Cyanobacterium Microcoleus vaginatus showing discontinuity throughout the filament because of the presence of division septa.(B) The membrane septum separating vacuolated cells of a Beggiatoa-like filament.(C) The membrane septum separating the large vacuolated cells of the Marithrix-like bacterial filaments.(D) Continuous cell membrane of with no division septa in (Ca.) T. magnifica.\n\n\n(Ca.) T. magnifica belongs to the family of large sulphur bacteria (LSB). LSBs are known to form long filaments of lengths up to 200 \\mu m (4‚Äì6). But these filaments are built up of individual LSB cells and therefore cannot be considered as a single living organism. Thus, the white filament was initially thought to be made of individual cells. To check this, researchers used dyes to visualize the entire plasma membrane of the filament. If it was made of individual cells, then the dye will show discontinuity in the plasma membrane structure indicating septa divisions. But to their surprise, they found no discontinuity and concluded that the whole filament is a single bacterial cell (as seen in Figure¬†2). Moreover, the filaments also had small buds forming from its apical pole and some of these filaments along with the buds were up to 2cm in size.\n\n4.2 Inside the biggest bacteria: Large central vacuole, sulphur granules and pepins\n\n\n\n\nFigure 3: Image (E) is taken from fig.¬†1. in (1). (E) Image shows a bud forming from the mother cell, where a Large central vacuole is present and is represented as ‚ÄúV‚Äù.\n\n\nInterestingly the whole filament was not filled with the cytoplasm rather, a large central vacuole was present in the middle (as seen in Figure¬†2 and Figure¬†3). This vacuole pushed the cellular cytoplasm to its periphery. The vacuole contributed to around 70% of the total cell volume. Therefore, although the cell is large, most of its volume is metabolically inactive. Apart from the vacuole, lucent vesicles were also found, which when further analyzed revealed to be filled with sulfur granules (see Figure¬†4). As Thiomargarita spp. are sulfur-oxidizing gammaproteobacteria (7), this was not a surprise.\nAdditionally, electron-rich membrane-bound compartments were found within the cytoplasm (see Figure¬†4), which was similar to structures previously reported in other LSBs. They were hypothesized to be compartments containing ribosomes and genetic material (8). To check if this holds, researchers used a stain to check where the genetic material was localized in (Ca.) T. magnifica. It was found that almost all of its DNA was concentrated inside these compartments. Further analysis also showed evidence of structures that were 10 to 20 nm in size, inside these compartments, similar to ribosomes. Researchers then used a technique to look for ribosomal RNAs and confirmed their presence inside these structures which indicated that those structures were ribosomes. Adding to this evidence, when translational activity was checked, they were consistently seen within most of the sites where ribosomal RNA was found.\n\n\nFigure 4: Image (G) is taken from fig.¬†1. in (1). (E) The arrowheads show two electron-rich membrane-bound compartments called pepins, the letter ‚ÄúS‚Äù represents sulphur granules and ‚ÄúV‚Äù represents a large central vacuole.\n\n\nThese membrane-bound structures containing DNA and ribosomes, which were not seen attached to the cell membrane are equivalent to the compartmentalization seen in eukaryotes, where DNA is present inside the nucleus and ribosomes are attached to the endoplasmic reticulum. Thus, this new membrane-bound organelle was named ‚Äòpepins‚Äô by the researchers, because of how they resemble pips or seeds in a watermelon. Overall, the cytoplasm of (Ca.) T. magnifica contained sulfur granules, pepins and various membrane structures forming a complex membrane network that spanned the whole cytoplasm.\n\n4.3 Localization of ATP synthase\nPeriplasm is a gel-like matrix present between the outer membrane and the inner membrane of a gram-negative bacteria\n\n\n\n\n\n(a) Image is taken from fig.¬†1. in: (9). Electron transport chain of E. coli\n\n\n\n\n\n\n\n\n(b) Image is taken from fig.¬†1. in: (10). Electron transport chain in a mitochondrial cell\n\n\n\n\nFigure 5: Electron transport chains of prokaryotes and eukaryotes\n\n\nAnother defining feature between eukaryotes and prokaryotes is the localization of ATP synthase. In eukaryotes, the ATP synthase is localized in the mitochondrial membrane where the electron transport chain (ETC) protein complexes are present (10). But for bacteria and archaea, mitochondria are absent and the ATP synthase is localized to the cell membrane along with the other protein complexes present in the ETC (9). Researchers checked within (Ca.) T. magnifica, where the ATP synthase is localized. Surprisingly ATP synthase was found in the membranes of pepins and the complex membrane structure, throughout the cytoplasm (see Figure¬†6). But ATP synthase was absent in the cellular membrane, this was similar to eukaryotes, where ATP synthase is only present in the mitochondria, which is present within the cytoplasm. These results together elevated pepins as not just a housing compartment for ribosomes and DNA, but also as a potential energy-producing organelle.\n\n\n\n\nFigure 6: Image (F) is taken from supplementary fig.¬†S20. in: (1). (E) The red colour indicates ATP synthase and blue colour dots highlight DNA which is used here as a proxy for where the pepins are situated. As you can see, the ATP synthase is expressed throughout the complex membrane network and the pepins present in the cytoplasm\n\n\nThe surprises do not stop there. Researchers further found that (Ca.) T. magnifica also showed extreme polyploidy, with around 740,000 genome copies for a 2 cm long cell. This is the highest number of genome copies ever reported for a single cell. The genome copies were very similar in their composition. A single genome copy was found to be 11.5 and 12.2 Mb in size, comparable to the popular Saccharomyces cerevisiae genome (12.1 Mb). (Ca.) T. magnifica genome had around 11,788 genes, more than three times the median gene count of prokaryotes (3935 genes) (11). Interestingly many of the common genes present within prokaryotes which coded for proteins that are responsible for cell division were absent here. But cell elongation protein genes were found to be intact and were found to be duplicated, forming multiple copies placed beside each other in the genome, which might suggest a possible reason why (Ca.) T. magnifica grow so long.\nThe researchers also found that, unlike most bacteria which reproduce via binary fission where cell volume doubles before cell division, in (Ca.) T. magnifica, they follow a dimorphic lifecycle; a bud-like cell stage and a filament-like cell stage. When they reproduce, the apical pole of the filament mother cell constricts and buds off to become a daughter cell (see Figure¬†1 and Figure¬†2). Interestingly, the genetic material is passed on to the daughter cell via pepins which is never seen before in bacteria."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#conclusion",
    "href": "posts/research_article_biggest_bacterium/index_post.html#conclusion",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion\nOverall (Ca.) T. magnifica turned out to be a very bizarre form of bacterial species that under the current understanding of cell morphology and energetics should not exist. So how come we find this bacteria in nature? Most bacteria are unable to grow to large sizes as they are met with various constraints related to energy and space. An overview of these constraints is given below.\n\nAs bacteria lack any mode of intracellular transporters to transport materials across the cytoplasm, they rely exclusively on diffusion. Chemical diffusion is extremely slow and is only effective in micro meter distances. So if the cell is too large, it will take a tremendous amount of time to disperse proteins and other materials effectively across the cell (2).\nWhen cell size increases, it reaches a point where the number of ribosomes needed to sustain the cell exceeds the available cell volume. This is called ‚Äòribosome catastrophe‚Äô and the upper limit of the cell volume for this catastrophe is 1.39¬±0.03 √ó 10^{‚àí15}m^3. The reported cytoplasmic volume of (Ca.) T. magnifica was around 5.91 √ó 10^{‚àí12} m^3, which is several orders greater than the ribosome catastrophe limit (12).\nPlasma membrane is the only location where ATP production can happen via the electron transport chain (ETC). And for bacteria and archaea, the only available membrane that they can use for ETC is their cellular membrane. Therefore as cell size increases, the surface area increase will quickly be outpaced by the volume increase owing to their differences in orders of increase. And as volume increase is followed by increased metabolic activity, energy production will become a bottleneck (12, 13).\n\nNow even if these constraints make perfect sense in the context of cell biology, there are always outliers in nature which defy them. One such outlies is this recently discovered bacterial species: (Ca.) T. magnifica.\nHow is (Ca.) T. magnifica overcoming these constraints? First of all, they have multiple copies of their genome compartmentalised along with ribosomes inside pepins. And these pepins are present across the cytoplasm. This might solve the diffusion limit problem because now the protein and other materials are almost always accessible across the cytoplasm at any time. Efficient compartmentalisation again might be the answer to surpassing the ribosome limit. In the case of the energy production limit attained due to surface area constraints, for (Ca.) T. magnifica, ATP synthase is expressed within the cytoplasm via pepin membranes and the complex membrane structure. Therefore, the available plasma membrane for energy production is much higher compared to other prokaryotes.\nIn the end, when we look at these features carefully, it is analogous to the various features present in eukaryotes. The polyploid genome of (Ca.) T. magnifica is equivalent to polypoid mtDNA present in eukaryotes. Compartmentalisation of DNA and ribosome via pepins is equivalent to compartmentalisation seen in the nucleus and the endoplasmic reticulum. And perhaps the icing on the cake is that (Ca.) T. magnifica transfers its genetic material via a membrane-bound organelle; pepins, something that is never seen before in prokaryotes. Therefore (Ca.) T. magnifica stands as one of the extreme cases of bacterial evolution which blurs our understanding of what constitutes a eukaryote and a prokaryote.\nFor more details about the study, please find the original paper (1).\nLast updated on\n\n\n[1] \"2022-07-22 15:07:24 IST\""
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html",
    "href": "posts/research_article_embryonic_learning/index_post.html",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "",
    "text": "Summary of the study (1)\n\n\nResearch article that is summarised in this post: Crowder, C., & Ward, J. (2022). Embryonic antipredator defenses and behavioral carryover effects in the fathead minnow (Pimephales promelas). Behavioral Ecology and Sociobiology, 76(2), 27. https://doi.org/10.1007/s00265-022-03136-2"
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#introduction",
    "href": "posts/research_article_embryonic_learning/index_post.html#introduction",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n2 Introduction",
    "text": "2 Introduction\nLearning is the process of acquiring information through experience. In nature, learning is ubiquitous in many different organisms and offers fitness advantages. Honeybees are known to learn floral features and utilize them during foraging (Review: 2), Wolf spiders (family Lycosidae) learn to avoid lesser quality prey when higher quality alternative prey is presented along with it (3), ants are known to learn and associate visual landmarks to their nest location (4). Learning is also particularly effective during different life stages. The experience acquired during the juvenile phase of an animal is known to affect their behaviour in adulthood. Baby Zebra finches (Taeniopygia guttata) are known to innately possess a ‚Äòtemplate song‚Äô which is used as a base to learn and mimic songs sung by adult conspecifics. If baby zebra finches are isolated and not allowed to mimic the song, then when they reach adulthood, they develop irregular and abnormally sound songs as compared to experienced adults (5). In the Tobacco hornworm moth, an aversive odour stimulus when learned during their larval stage persists when they metamorphose into a moth (6). In houseflies, adults develop a preference for olfactory cues that were exposed to them during their larval stage (7)."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#embryo-is-sensitive-to-external-cues",
    "href": "posts/research_article_embryonic_learning/index_post.html#embryo-is-sensitive-to-external-cues",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n3 Embryo is sensitive to external cues",
    "text": "3 Embryo is sensitive to external cues\nMost studies in the context of learning focused on fitness advantages to adults when learning was manifested during the juvenile phase. But some organisms, especially oviparous organisms undergo their embryonic stage outside of their mother‚Äôs body where the embryos are in direct contact with the surrounding environmental cues. They are also found to be highly sensitive to these cues. Perhaps the most famous evidence for this would be gender determination in reptiles, where the eggs respond to their surrounding ambient temperature. Here, gender is determined by temperature values after fertilization (Review: 8). In addition to sensing environmental cues, many studies have also reported evidence of ‚Äúembryonic learning‚Äù, where the embryos learn and form memories of surrounding environmental cues and utilize these memories in both the pre and post-embryonic period to modulate their behaviour. Some of the cases where the embryo is found to respond and/or learn external cues are given below;\n\nEggs of ringed salamanders (Ambystoma annulatum, Figure¬†1) when exposed to chemical cues from predators resulted in reduced activity and greater shelter-seeking behaviour in larvae (9).\nWood frog (Rana sylvatica, Figure¬†2) tadpoles learned to respond to chemical cues from unfamiliar predators when associated with conspecific alarm cues during the embryo stage (9).\nNaive cuttlefish (Sepia officinalis, Figure¬†3) prefer shrimps over crabs, but cuttlefish embryos which are visually conditioned with crabs preferred crabs over shrimps during the larval stage (10). Also, cuttlefish embryos that had experience with white crabs1 developed a preference for white crabs over black crabs2 during the larval stage (10). Moreover, cuttlefish larvae that received embryonic experience with white crabs exhibited prey generalisation where they prefer black crabs over shrimp (10).\nCuttlefish embryos (Sepia officinalis, Figure¬†3) are also seen to reduce their ventilation rate in the presence of predator indicative cues, possibly to remain cryptic. Furthermore, cuttlefish embryos are also found to associate non-predator cues with predator cues to invoke reduced ventilation behaviour which showcases evidence for associative learning in these embryos (11).\nRainbow trout embryos (Oncorhynchus mykiss, Figure¬†4) can learn both conspecific and heterospecific alarm cues and, rainbow trout fish with embryonic learning experiences had longer memory retention compared to the ones with no embryonic learning experience (12).\nBamboo sharks (Chiloscyllium punctatum, Figure¬†5), like all sharks, have electroreception which is normally used to locate prey. But unlike most other sharks where the embryo is developed within the mother‚Äôs body, in bamboo sharks egg sacks containing the embryo are oviposited on substrates which makes them vulnerable to predation (13). It is known that bamboo shark embryos can use their electrosensory system to innately detect electric fields indicative of predators and thereby cease their respiratory gill movements to become cryptic (14).\nPort Jackson shark (Heterodontus portusjacksoni, Figure¬†6) embryos are found to modulate their oxygen uptake rates against predator (Crested horn sharks; Heterodontus galeatus) and non-predator (Sand whiting; Sillago ciliata) olfactory cues with respect to their developmental stages. In earlier developmental stages, the embryos reduced oxygen uptake against non-predator odour cues but not against predator cues, indicating a cryptic response, possibly because of neophobia. At later developmental stages, the embryos increased oxygen uptake against predator cues and but not against non-predatory cues. This possibly indicates a fight-or-flight response where increased oxygen levels can contribute to enhanced capacity for aerobic or anaerobic activities that can aid in successful evasion by fleeing (15).\nCinnamon clownfish embryos (Amphiprion melanopus, Figure¬†7) are found to detect conspecific alarm cues and associate them with predator cues which resulted in increased heart rates (16). Increased heart rate is found to be positively correlated with anti-predator behaviours in fish.\n\nThese studies show that embryos can sense surrounding cues. The nature of surrounding cues also matters as some elicit innate behaviours and some don‚Äôt elicit any behaviour at all. For cues eliciting innate behaviours, some of the above-mentioned studies have shown that embryos can associate novel cues to these innately known cues and thereby change behaviour during the pre and/or post-embryonic stage.\n\n\n\n\n\n\nFigure 1: Image by Peter Paplanus from St.¬†Louis, Missouri - Ringed Salamander (Ambystoma annulatum)\n\n\n\n\n\n\nFigure 2: Image by Brian Gratwicke - Lithobates sylvaticus (Woodfrog). The wood frog is known as both Lithobates sylvaticus and Rana sylvatica\n\n\n\n\n\n\n\n\nFigure 3: Image by Diego Delso - Common cuttlefish (Sepia officinalis)\n\n\n\n\n\n\nFigure 4: Image by Mike Anderson - Female Rainbow trout (Oncorhynchus mykiss).\n\n\n\n\n\n\n\n\nFigure 5: Image by Zul M Rosle from Kuantan, Malaysia - Brownbanded bambooshark (Chiloscyllium punctatum) at the KLCC Aquaria.\n\n\n\n\n\n\nFigure 6: Image by Mark Norman/Museum Victoria - Port Jackson Shark (Heterodontus portusjacksoni) at Wilsons Promontory, Victoria.\n\n\n\n\n\n\n\n\nFigure 7: Image by Richard Ling - Red and Black Anemonefish (Amphiprion melanopus) in anemone (Entacmaea quadricolor). Steve‚Äôs Bommie, Ribbon Reefs, Great Barrier Reef 177094512."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#embryonic-response-to-predatory-cues",
    "href": "posts/research_article_embryonic_learning/index_post.html#embryonic-response-to-predatory-cues",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n4 Embryonic response to predatory cues",
    "text": "4 Embryonic response to predatory cues\nAs seen earlier, embryonic response to predators is evident and this might be because of strong predation pressures as most aquatic oviparous organisms are highly vulnerable to egg predators. While most studies have looked at the effect of embryonic response in juvenile and adult life stages, it might also have an immediate effect on the embryo itself [as seen in the Cuttlefish (11), in Bamboo shark (14), in Port Jackson shark (15) and Cinnamon clownfish (16)].\nSo meet fathead minnow fish (Pimephales promelas), who share the same fate of being embryonic prey as the above-mentioned ones. The Fathead minnow is a freshwater fish species native to North America. They live in a school of up to a hundred individuals (17) but during the onset of the breeding season. males depart from school and adopt a solitary lifestyle (18). Females of this species lay their eggs in sites guarded by a single male (19), which hatch into larvae after 5 days of post-fertilization. They are some of the well-studied model organisms which display prominent anti-predatory behaviours. They are perhaps best known for ‚ÄúSchreckstoff‚Äù (Paper is in German: 20), a chemical alarm signal, which is only produced when they suffer tissue damage, which occurs most often from predator attacks. Fathead minnow innately responds to these alarm cues and associates them with predator cues (21). With varying levels of these alarm cues, solitary individuals show a combination of anti-predatory behaviours that include, dashing, freezing, slowing and exploring (22). If chased by predators, they show an increased rate of shoaling3 and shooling4 and, also show increased shelter-seeking behaviour (23). Field studies have shown that they actively avoid areas containing heterospecific alarm cues (21). However, they are not reported to innately identify any predators (24).\nThis complex behaviour repertoire is only reported to be present in juveniles and adults (well no surprises there!). But fathead minnow starts their life cycle as eggs and faces high rates of predation (25). The embryonic stage is by far the worst period to get predated on as the embryos are heavily handicapped due to their immobility. Nevertheless, they are known to alter hatching times in response to predator cues (26). Moreover, when predator cues in combination with cues indicating embryo damage were introduced to embryos, the developed larvae from these embryos were small in size, indicating loss of developmental maturity (26). But in the presence of conspecific alarm cues, hatching times remain unchanged (27). Therefore these results suggest that fathead minnow embryos can sense environmental cues but clear-cut evidence was lacking. This is exactly what was explored by graduate student Christopher Crowder and Dr.¬†Jessica Ward at Ball State University, Indiana, USA. in their recent paper (1). The researchers asked whether fathead embryos can innately detect alarm cues just like their adult counterparts and if they do, then does that embryonic experience affect pre and/or post-embryonic behaviours?"
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#methods-and-results",
    "href": "posts/research_article_embryonic_learning/index_post.html#methods-and-results",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n5 Methods and results",
    "text": "5 Methods and results\nTo initiate embryonic experience, researchers grew the embryos in four different environments imparting ‚Äúpredator-risk‚Äù and ‚Äúnon-predator-risk‚Äù experiences.\n\n\nTable 1: Embryonic environment\n\n\n\n\n\nEnvironment\nCondition\n\n\n\nEnvironment 1\nEmbryos reared in a control condition (just water with no other cues)\n\n\nEnvironment 2\nEmbryos reared with predator cues\n\n\nEnvironment 3\nEmbryos reared with conspecific alarm cues\n\n\nEnvironment 4\nEmbryos reared with predator + conspecific alarm cues\n\n\n\n\nExcept for environments 1 and 2, all other environments indicated predator risk. Since the fatheaded minnow shows no innate behaviour against any predators, environment 2 (predator cue) should be similar to environment 1 (control). If associative learning occurs in fathead minnow embryos then the larvae which came from environment 4 should be able to associate the predator cue to the alarm cue, which can later be tested through behavioural assays. As adult fathead minnows are known to innately respond to conspecific alarm cues, environments 3 and 4 can be indicative of predation pressure and embryos should be able to detect them.\nAs fathead minnows show embryonic locomotor activity, it might make them conspicuous to egg predators. Interestingly, when locomotor activity was checked 5 days post fertilization, embryos from predator-risk environments (env. 3 and 4) showed lower motor activity, possibly indicating a cryptic response. This suggests that embryos can detect alarm cues like their adult counterparts.\nAfter testing for locomotor activity, the embryos were then grouped according to the environments they were raised in and were placed in separate control tanks devoid of alarm or predator cues. The embryos were allowed to hatch, and at 22 days post fertilization, the larvae were tested for predator avoidance behaviour and predator evasion behaviour. The idea was that larvae from predator-risk environments should show enhanced predator avoidance and evasion behaviours.\nTherefore, researchers tested the larvae individually on a test arena where swimming activity was checked before and after applying a stimulus. Four different stimuli were used which were synonymous with the environments. Here swimming activity was measured, which acted as a proxy to the ‚Äòfreezing‚Äô behaviour fathead minnow showed, which is a predator avoidance behaviour. Lower swimming activity corresponds to enhanced predator avoidance behaviour, as lower activity might make them more cryptic to predators.\nThe four stimuli that were used are;\n\n\nTable 2: Stimuli administered\n\nStimulus\nCondition\n\n\n\nStimulus 1\nControl (absence of predator or alarm cues)\n\n\nStimulus 2\nPredator cue\n\n\nStimulus 3\nConspecific alarm cue\n\n\nStimulus 4\nPredator + conspecific alarm cues\n\n\n\n\nLarvae from the predator risk environments (env. 3 and 4) should show enhanced freezing behaviour when predator risk stimuli (stimuli 3 and 4) are presented, compared to the control stimulus. Stimulus 2 should also elicit enhanced freezing behaviour if the embryo was able to associate the predator cue with the alarm cue (i.e.¬†associative learning occurred in embryos indicating evidence for embryonic learning)\nResearchers found that before stimulus use, larvae from predator-risk environments (env. 3 and 4) had decreased swimming activity. This is in conjunction with earlier mentioned studies, where it was shown that embryos which experience predator indicative cues showed an overall decrease in activity in post-embryonic stages. Upon stimulus usage, the same trend followed, larvae from predator-risk environments had an overall decreased swimming activity for all stimuli administration instances. Both these results indicate that the larvae from predator risk environments (env. 3 and 4) showed enhanced predator avoidance behaviour compared to larvae from non-predator risk environments (env. 1 and 2). In the case of larvae from environment 4 (predator + alarm cues), when the predator cue was administered, it showed reduced swimming activity compared to the control. But the difference was not statistically significant. The level of significance for this study was chosen to be \\alpha = 0.05 and the p-value for the difference between swimming activity when predator cue stimulus was used compared to control stimulus usage was 0.05 (same as the \\alpha value), as reported in the paper. But that does not rule out its biological significance and the researchers assert that associative learning has occurred. For larvae from environment 1 (control), except for the control stimulus, all other stimuli administration resulted in reduced swimming activity. This was a strange finding and the researchers suggested that it might be because of neophobia.\nThe next anti-predatory behaviour researchers looked at was predator evasion behaviour. As mentioned before, researchers hypothesized that larvae from predator-risk environments (env. 3 and 4) should show enhanced evasion behaviours. To quantify evasion behaviour, two parameters were measured; maximal body curvature and latency to respond to simulated predator attack. When a fish tries to escape from a predator, it bends its body into a ‚ÄúC‚Äù shape to generate momentum drag. The maximal body curvature is the angle formed when the ‚ÄúC‚Äù shape is formed in a fish. A shorter angle means that the body bend is greater indicating an enhanced escape behaviour. Overall, it was found that predator risk environments (env. 3 and 4) have resulted in reduced body curvature angle, indicating enhanced evasion behaviour in the larvae. But the evidence for associative learning was not present in this case, as for larvae from environment 4 (predator + alarm cues), upon predator cue administration, the maximal body curvature angle was not significantly different from the control. Latency to respond was found to be independent of embryonic environment and stimulus administration, as no change was observed between the groups. This indicates that post-embryonic behaviour can have varying levels of susceptibility to modification via embryonic learning."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#conclusion",
    "href": "posts/research_article_embryonic_learning/index_post.html#conclusion",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "\n6 Conclusion",
    "text": "6 Conclusion\nThe researchers showed, for the first time in fathead minnow embryos (Pimephales promelas), that they can detect conspecific alarm cues, moreover, they also showed that;\n\nIn addition to detecting conspecific alarm cues, the embryos, in response to its detection also modulated their embryonic locomotor activity to become cryptic, indicating an immediate adaptive response within the embryos\nEmbryonic experience overall enhanced predator avoidance and evasion behaviours\nEmbryonic associative learning only modulated predator avoidance and not evasion behaviour in the larvae, indicating a varying level of susceptibility to modification via embryonic learning\n\nThe evidence for associative learning in this study did not achieve statistical significance and was in a borderline area (p = 0.05, same as \\alpha = 0.05) but that does not mean there is no biological significance. As sample sizes were adequate, we can explore some possible future directions.\n\nThe predator cue used in this study came from a known egg predator of fathead minnow, the blue gill sunfish (Lepomis macrochirus). Using another predator cue in association with the conspecific alarm cue can mitigate any possible intrinsic effect the previous predator cue had in learning.\nAs fathead minnows were trained to associate predator cues to conspecific alarm cues throughout 5 days post fertilization as embryos and were only tested 22 days post fertilization as larvae. There was a significant time gap between training and testing which might have corroded its memory. But fathead minnows are known to form long-lasting associations between odour cues and other cues. In an earlier study (28), fathead minnow was shown to distinguish between familiar shoal mates and unfamiliar conspecifics, and they were able to maintain this discriminability even after 2 months of separation suggesting evidence for a long-lasting memory. I did not find any other studies which have exclusively looked at learning and memory in fathead minnow. An interesting experiment to follow up would be, training fathead minnow embryos to associate a predator cue to the conspecific alarm and testing this association at different ages after hatching. This can again check if embryonic associative learning occurred? if it occurred then how does the memory fare with increasing time interval between last association and larval age.\n\nIn conclusion, this study (1) showcased a whole new aspect of learning in fathead minnow (Pimephales promelas) which opens up new avenues of research questions given their peculiar behaviour and life history.\nLast updated on\n\n\n[1] \"2022-07-23 08:49:32 IST\""
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "The story behind one carat blog",
    "section": "",
    "text": "Cover photo of my old blog made using distill\n\n\n\n\n\n\nCover photo of my new blog made using quarto"
  },
  {
    "objectID": "posts/welcome/index.html#making-of-one-carat-blog",
    "href": "posts/welcome/index.html#making-of-one-carat-blog",
    "title": "The story behind one carat blog",
    "section": "\n2 Making of one carat blog",
    "text": "2 Making of one carat blog\nMy name is Jewel Johnson and I am the creator of this blog. The story about this blog started with my wish to have a writing space, to showcase the things that I learned. I find that having a blog to write, instills self-discipline in learning new things. So I initially began looking for ways to build a blog. I read many online articles which talked about using WordPress, Blogger, Weebly, Wix and so on. But almost all of these services were paid and I was not able to afford them. I began looking for free alternatives and I came to know about Medium. Now Medium is not precisely a blog but a publishing platform and it restricts readers behind a paywall if they want to read your posts. But you do get paid if anyone reads your articles. So I tried writing a few articles on R programming. Medium supports code blocks but it was extremely difficult to use them and it was never tailored for writing R codes. As I was interested in sharing some of the things I learned about R, I wanted to see what all articles were written about R programming on Medium. So I read some of them in Towards data science, which is part of Medium but specialized in articles related to data science. There were some good articles but to my surprise, most articles were downright copy-pasting what was there in the package tutorials or their related source webpages. And the articles were made with very low effort. The only thing I found nice was the stock images. So I ditched Medium for these reasons and went on to see if I can make a blog using purely R. And that‚Äôs when I came to know about the {Rmarkdown} package in R and Github pages. So long story short, I learned how to use this package and how to host a webpage for free on Github. And this led to the creation of my first blog. It was very basic and visually simple, but I was proud of it. So I began writing tutorials on data visualization using R and wrote my first article about the {ggplot2} package in R. I shared it with my friends and received good support from them. Then I wanted to improve my blog and I began searching for new ways to make the blog better, which led me to learn about the {distill} package in R.\nI was flabbergasted when I saw the blogs and websites made using the distill package. They were so much better than the ones made from {Rmarkdown}. I first learned about {distill} from Dr.¬†Ella Kaye‚Äôs blog post and Dr.¬†Lisa Lendway‚Äôs blog post. And after reading Prof.¬†Andreas Handel blog post on how to build a website using distill I was convinced to rebuild my blog. The YouTube video by Dr.¬†Lisa Lendway‚Äôs and the official tutorials given on the distill website also helped me very much. And the end product was this beautiful blog that I created. I even named my blog; 'one carat blog', a name resembling me.\nAnd all was well and I began concentrating on my blog. I went on to write a series of articles encompassing a complete tutorial on learning data visualization and data manipulation using R. It was well received and I got a lot of helpful comments about it. One thing that kind of annoyed me about the {distill} package was that the table of contents won‚Äôt float for long posts. This made the ‚Äòtable of contents‚Äô obsolete. Nevertheless, the package fulfilled most of what I wanted to have in a blog. This period was also the first time I began learning about HTML and CSS languages. Using that knowledge I began tweaking my blog and customizing it. I also wrote an article highlighting a few quality of life modifications one can implement in their distill blog and to my surprise, the post was adopted to the official distill website, which showcased useful tutorials related to distill blogging.\n\n\n\nGreat to see you‚Äôve added your blog to the distillery. You should also consider adding your excellent ‚ÄúQuality of life modifications for your distill websites‚Äù post to the Tips & Tricks section of the site https://t.co/SMOtCU0eM1\n\n‚Äî Ella Kaye (@ellamkaye) December 20, 2021\n\nI also received back from Dr.¬†Ella Kaye‚Äôs in Twitter and I was added to the distill club! I was very happy at that time.\n\n\n\nThis is fab! Welcome to #DistillClub pic.twitter.com/4jpeBe7Kis\n\n‚Äî Ella Kaye (@ellamkaye) December 20, 2021\n\nThis was also the time when I changed the About me page. Everyone who was having a distill blog was using a default template provided by the {postcards} package in R. But I wanted to have a unique ‚Äòabout me‚Äô page, which acted like my CV. So I used the modern resume theme by James Grant and made this cool looking CV.\nThen for about four months, I had the worst time in my life. I was feeling depressed and sick. Many things had happened and I could not do any blogging or anything for that matter. And then finally after sorting out my affairs, I went back to blogging. I opened Rstudio one day and I got a blank window. This was the first time I was seeing this bug, so when I searched about it, I learned that a new package for R called {quarto} was causing some issues with Rstudio. To my knowledge, it only affected Linux users. Then I searched about what is {quarto} package is about and I struck a gold mine.\nIt was everything {distill} had, but better. The best thing was that it natively supported a floating table of contents, something which was missing in {distill}. My first introduction to quarto came from Dr.¬†Alison Hill‚Äôs blog post where she gave a brief intro on what the quarto package is about. After reading the post I was in a dilemma as to whether I should migrate my blog to quarto or not. Then I found that most of the blogs featured on the distill official website had migrated to quarto. It was the new trend in town, and everyone was adapting it. So I finally made up my mind and decided to migrate, again, for the second time and it led to a few sleepless nights that I am not very proud of. The post from Dr.¬†Danielle Navarro was also very helpful and made the transition process much easier.\nSo with help of all these articles and many glasses of homemade wine, I finally migrated from {distill} to {quarto} and made this blog which you are browsing now. I had to convert all my .Rmd files to the new .qmd file that quarto supports. The YAML headers were a pain in the beep to change. But in the end, it was worth it. I changed the previous visitor counter to a new flag counter for this blog. I also designed a new icon and a new cover image. And as of writing this blog post, I also wrote a series of articles focusing on beginner-level statistics in my blog. And recently I have been working on a software called Notion and have made some cool productivity templates focusing on PhD students. I also have written some blog posts showcasing those templates, so make sure to check them out.\nTo conclude I hitchhiked from Rmarkdown to distill and now to quarto, the end product is this beautifully made blog that you are browsing. I hope you enjoy this blog and find the posts and tutorials helpful. If you find them useful, make sure to share them. If you have any suggestions or feedback for this blog, then please comment them in the comment section at the end of the blog. Thanks for visiting my blog and I hope you have a great time."
  },
  {
    "objectID": "posts/welcome/index.html#references",
    "href": "posts/welcome/index.html#references",
    "title": "The story behind one carat blog",
    "section": "References",
    "text": "References\nUseful Distill articles\n\nKaye (2021, May 8). ELLA KAYE: Welcome to my {distill} website!. Retrieved from https://ellakaye.rbind.io/posts/2021-05-08-welcome-to-my-distill-website/\nLendway (2020, Dec.¬†18). Lisa Lendway: Building a {distill} website. Retrieved from https://lisalendway.netlify.app/posts/2020-12-09-buildingdistill/\nCreate a GitHub website with distill in less than 30 minutes by Prof.¬†Andreas Handel.\nOfficial guide on creating a website using distill.\nBuilding a website using R {distill}. Youtube video by Dr.¬†Lisa Lendway.\n\nUseful Quarto articles\n\nWe don‚Äôt talk about Quarto by Dr.¬†Alison Hill.\nDanielle Navarro. 2022. ‚ÄúPorting a Distill Blog to Quarto.‚Äù April 20, 2022. https://blog.djnavarro.net/posts/2022-04-20_porting-to-quarto.\nCreating a blog with Quarto in 10 steps by Beatriz Milz\nCool quarto tips and tricks. Tweet by Albert Rapp\nOfficial guide by developers of Quarto\n\nLast updated on\n\n\n[1] \"2022-07-01 15:56:50 IST\""
  },
  {
    "objectID": "posts/word-happy-2022/index.html",
    "href": "posts/word-happy-2022/index.html",
    "title": "The World Happiness Report 2022",
    "section": "",
    "text": "World Happiness Report 2022 shows which are the happiest countries for the year 2022. By statistically analysing six key parameters, each country is given a score (which is called happiness score within the dataset). The higher the score, the happier the country is and vice versa. The six key parameters which are taken into analysis for determining the score are;\n\nGross domestic product per capita\nSocial support\nHealthy life expectancy\nFreedom to make your own life choices\nGenerosity of the general population\nPerceptions of internal and external corruption levels.\n\nFinland is ranked first among 149 countries with an overall score of 7.82. Despite COVID 19 wrecking havoc around the world, citizens of Finland have persevered through it and they have been maintaining first rank since 2016. Afghanistan is at the lowest rank with a score of 2.40. With complications from COVID 19 pandemic and the Taliban take over, Afghanistan is going through one of the worst humanitarian crisis in human history and the ranking reflects it."
  },
  {
    "objectID": "posts/word-happy-2022/index.html#getting-the-data",
    "href": "posts/word-happy-2022/index.html#getting-the-data",
    "title": "The World Happiness Report 2022",
    "section": "\n2 Getting the data",
    "text": "2 Getting the data\nIn this post we will do some exploratory data visualizations using data from The World Happiness Report 2022. You can download the .csv file from here."
  },
  {
    "objectID": "posts/word-happy-2022/index.html#plotting-a-world-map",
    "href": "posts/word-happy-2022/index.html#plotting-a-world-map",
    "title": "The World Happiness Report 2022",
    "section": "\n3 Plotting a world map",
    "text": "3 Plotting a world map\nWe will plot a world map with a scalable colour palette based on the ladder score where greater scores indicated happier countries and vice versa.\nIn short what we we will be doing is, we are going to join the World Happiness Report 2021 dataset with the map data and plot it using the ggplot2 package. The map_data() function helps us easily turn data from the {maps} package in to a data frame suitable for plotting with ggplot2.\n\n# required packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(viridis)\n\n# load the dataset which you have downloaded\n# please change the location to where your downloaded file is kept\nhap_pre <- read.csv(\"datasets/2022.csv\")\n\n# renaming column names of ease of use\ncolnames(hap_pre)[2] <- \"country\"\ncolnames(hap_pre)[3] <- \"score\"\nhap_pre <- hap_pre[-147,]\n\n# the score values are separated by commas\n# let us change that to dots\nhap_pre$score <- scan(text=hap_pre$score, dec=\",\", sep=\".\")\n\n# selecting country and score columns\nhap <- hap_pre %>% select(country,score)\n\n# removing special characters in df\nhap$country <- gsub(\"[[:punct:]]\",\"\",as.character(hap$country))\n\n# loading map\nmap_world <- map_data('world')\n# remove Antarctica\nmap_world <- map_world[!map_world$region ==\"Antarctica\",]\n\n# checking which country names are a mismatch between map data and the downloaded dataset\nanti_join(hap, map_world,  by = c(\"country\" = \"region\"))\n\n\n\n  \n\n\n\nYou can see that some country names are a mismatch with the map data. So we will attempt to fix it.\n\n# display all country names in the dataset\n# useful to locate correct country names\n#map_world %>% group_by(region) %>% summarise() %>% print(n = Inf)\n\n# correcting country names\n# here we are matching the country names of downloaded dataset with the map data\ncorrect_names <- c(\"United States\" = \"USA\",\n                   \"United Kingdom\" = \"UK\",\n                   \"Czechia\" = \"Czech Republic\",\n                   \"Taiwan Province of China\"  = \"Taiwan\",\n                   \"North Cyprus\"= \"Cyprus\",\n                   \"Congo\"= \"Republic of Congo\",\n                   \"Palestinian Territories\" = \"Palestine\",\n                   \"Eswatini Kingdom of\" = \"Swaziland\")\n\n# recoding country names \nhap2 <- hap %>% mutate(country = recode(country, !!!correct_names))\n\n# joining map and the data\nworld_hap <- left_join(map_world, hap2, by = c(\"region\" = \"country\"))\n\n# creating a function to add line in text, for the caption\naddline_format <- function(x,...){\n  gsub(',','\\n',x)}\n\n# plotting the world map\nggplot(world_hap, aes(long, lat)) + geom_polygon(aes(fill = score, group = group)) +\n  scale_fill_viridis(option = \"viridis\") + theme_void() +\n  theme(plot.background = element_rect(fill = \"aliceblue\"),\n        legend.position=\"bottom\") + \n  labs(title = \"Happiness scores of countries in 2022\",\n       subtitle = addline_format(\"Higher scores indicate happier countries and vice versa,Grey colour represents countries with no data\"),\n       fill = \"Score\",\n       caption = addline_format(\"Source: World Happiness Report 2022,Visualization by Jewel Johnson\"))"
  },
  {
    "objectID": "posts/word-happy-2022/index.html#plotting-an-interactive-world-map",
    "href": "posts/word-happy-2022/index.html#plotting-an-interactive-world-map",
    "title": "The World Happiness Report 2022",
    "section": "\n4 Plotting an interactive world map",
    "text": "4 Plotting an interactive world map\nLets plot an interactive world map with happiness score as a variable, where greater scores indicates happier countries and vice versa. We will be using the leaflet package in R for plotting the world map.\nWe have to download the world map data which comes as a .shp file.\nRun the codes given below to download the .shp file and load the .csv file required to plot the map. For plotting the interactive map, we will be using the sf package for reading the .shp file and the leaflet package for plotting the map.\n\n# loading the .csv file which was downloaded\n# please change the location to where your .csv file is kept\nhap_pre <- read.csv(\"datasets/2022.csv\")\n\n# renaming column names of ease of use\ncolnames(hap_pre)[1] <- \"rank\"\ncolnames(hap_pre)[2] <- \"country\"\ncolnames(hap_pre)[3] <- \"score\"\nhap_pre <- hap_pre[-147,]\n\n# the score values are separated by commas\n# let us change that to dots\nhap_pre$score <- scan(text=hap_pre$score, dec=\",\", sep=\".\")\n\n# selecting country and score columns\nhap <- hap_pre %>% select(rank,country,score)\n\n# removing special characters in df\nhap$country <- gsub(\"[[:punct:]]\",\"\",as.character(hap$country))\n\nNow that we have the dataset ready, let us download the .shp file which contains the world map.\n\n# downloading and loading the .shp file\n# please change the 'destfile' location to where your zip file is located\ndownload.file(\"http://thematicmapping.org/downloads/TM_WORLD_BORDERS_SIMPL-0.3.zip\",\n              destfile=\"shp/world_shape_file.zip\")\n\n# unzip the file into a directory. You can do it with R (as below).\n# the directory of my choice was folder named 'shp'\nunzip(\"shp/world_shape_file.zip\", exdir = \"shp/\")\n\nNow let us load the .shp file and plot the map.\n\n# Read the shape file with 'sf'\n#install.packages(\"sf\")\nlibrary(sf)\n\nworld_spdf <- st_read(paste0(getwd(),\"/shp/TM_WORLD_BORDERS_SIMPL-0.3.shp\"), stringsAsFactors = FALSE)\n\nReading layer `TM_WORLD_BORDERS_SIMPL-0.3' from data source \n  `C:\\Work\\Github\\one-carat-blog\\posts\\word-happy-2022\\shp\\TM_WORLD_BORDERS_SIMPL-0.3.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 246 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.57027\nGeodetic CRS:  WGS 84\n\n# checking which country names are a mismatch between map data and the downloaded dataset\n# this is an important check as we have to join the happiness dataset and .shp file with country names\nanti_join(hap, world_spdf,  by = c(\"country\" = \"NAME\"))\n\n\n\n  \n\n\n# correcting country names, note that some countries are not available in the .shp file\n\ncorrect_names <- c(\"Czechia\" = \"Czech Republic\",\n                   \"Taiwan Province of China\" = \"Taiwan\",\n                   \"South Korea\" = \"Korea, Republic of\",\n                   \"Moldova\"  = \"Republic of Moldova\",\n                   \"Belarus*\" = \"Belarus\",\n                   \"Vietnam\" = \"Viet Nam\",\n                   \"Hong Kong SAR of China\"= \"Hong Kong\",\n                   \"Libya\" = \"Libyan Arab Jamahiriya\",\n                   \"Ivory Coast\" = \"Cote d'Ivoire\",\n                   \"North Macedonia\" = \"The former Yugoslav Republic of Macedonia\",\n                   \"Laos\" = \"Lao People's Democratic Republic\",\n                   \"Iran\" = \"Iran (Islamic Republic of)\",\n                   \"Palestinian Territories*\" = \"Palestine\",\n                   \"Eswatini, Kingdom of*\" = \"Swaziland\",\n                   \"Myanmar\" = \"Burma\",\n                   \"Tanzania\" = \"United Republic of Tanzania\")\n\n# recoding country names \nhap2 <- hap %>% mutate(country = recode(country, !!!correct_names))\n\n# joining .shp file and the happiness data\nworld_hap <-  left_join(world_spdf, hap2, by = c(\"NAME\" = \"country\"))\n\n#install.packages(\"leaflet\")\nlibrary(leaflet)\nlibrary(viridis)\n\n# making colour palette for filling\nfill_col <- colorNumeric(palette=\"magma\", domain=world_hap$score, na.color=\"transparent\")\n\n# Prepare the text for tooltips:\ntext <- paste(\n  \"Country: \", world_hap$NAME,\"<br/>\", \n  \"Score: \", world_hap$score, \"<br/>\", \n  \"Rank: \", world_hap$rank, \n  sep=\"\") %>%\n  lapply(htmltools::HTML)\n\n# plotting interactive map\nleaflet(world_hap) %>% \n  addTiles()  %>% \n  setView( lat=10, lng=0 , zoom=2) %>%\n  addPolygons( \n    fillColor = ~fill_col(score), \n    stroke=TRUE, \n    fillOpacity = 0.9, \n    color= \"grey\", \n    weight=0.3,\n    label = text,\n    labelOptions = labelOptions( \n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"), \n      textsize = \"13px\", \n      direction = \"auto\"\n    )\n  ) %>%\n  addLegend( pal=fill_col, values=~score, opacity=0.7, title = \"Score\", position = \"bottomleft\" )"
  },
  {
    "objectID": "posts/word-happy-2022/index.html#summary",
    "href": "posts/word-happy-2022/index.html#summary",
    "title": "The World Happiness Report 2022",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope this post was helpful to you in understanding how to plot world maps in R. In short using ggplot2 we have first plot a static world map using the data from The World Happiness Report 2022, then similarly using the leaflet package we plotted an interactive world map."
  },
  {
    "objectID": "posts/word-happy-2022/index.html#references",
    "href": "posts/word-happy-2022/index.html#references",
    "title": "The World Happiness Report 2022",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nJoe Cheng, Bhaskar Karambelkar and Yihui Xie (2021). leaflet: Create Interactive Web Maps with the JavaScript ‚ÄòLeaflet‚Äô Library. R package version 2.0.4.1. https://CRAN.R-project.org/package=leaflet\nPebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009\nTutorial on plotting interactive maps in R.\nThe World Happiness Report\nSource for .csv file of World Happiness Score of countries 2022. Compiled by Mathurin Ach√© in Kaggle.com\n\nLast updated on\n\n\n[1] \"2022-08-04 21:19:52 IST\""
  },
  {
    "objectID": "tutorials/data_man/project5.html",
    "href": "tutorials/data_man/project5.html",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "",
    "text": "Raw data might not be always in a usable form for any form of analysis or visualization process. The tidyr package aims to help you in reshaping your data in a usable form. In short, it helps you to ‚Äòtidy‚Äô up your data using various tools. In this chapter, we will see how you can use the tidyr package to make your data tidy."
  },
  {
    "objectID": "tutorials/data_man/project5.html#what-is-tidy-data",
    "href": "tutorials/data_man/project5.html#what-is-tidy-data",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n2 What is tidy data?",
    "text": "2 What is tidy data?\nFirst, we need to understand what tidy data looks like. For that let us imagine a scenario where you are a doctor who is trying to find the best treatment for a disease. Now your colleagues have short-listed five different treatment methods and have reported their efficacy values when tested with five different patients. Now you are tasked with finding which of the five treatments is the best against the disease. You open your computer and you find the following data of the experiment.\n\n\n\n\n  \n\n\n\nThis is how often data is stored because it is easy to write it this way. In the first column, you can see the different treatments from one to five. And in the second column, you have the efficacy values of the treatments for patient 1 and it goes on for the other patients. Now, this is a good example of how a dataset should not look like! Surprised? Let us see what makes this dataset ‚Äòdirty‚Äô.\nYou can quickly notice that there is no mentioning of what these numerical values mean. Of course, we know that they are efficacy values for the different treatments. But for someone who only has this data as a reference, that person would not have a clue as to what these numbers mean. Also, note that each of the rows contains multiple observation values which is not a feature of tidy data. This kind of data format is called ‚Äòwide data‚Äô which we will talk more about later.\nWith that being said, tidy data will have;\n\nEach of its variables represented in its own column\nEach observation or a case in its own row.\nEach of the rows will contain only a single value.\n\nSo let us see how the ‚Äòtidier‚Äô version of this data would look like.\n\n\n\n\n  \n\n\n\nYou can see each of the columns represent only one type of variable. In the first column, you have the types of treatments, followed by patient IDs and their efficacy values for each treatment. Also, note that each row represents only one observation. So this kind of data format is what we strive to achieve by using the tidyr package and they are called as ‚Äòlong data‚Äô. So let us begin!"
  },
  {
    "objectID": "tutorials/data_man/project5.html#reshaping-dataset",
    "href": "tutorials/data_man/project5.html#reshaping-dataset",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n3 Reshaping dataset",
    "text": "3 Reshaping dataset\nThere are different sets of commands which you can utilize to reshape your data and make it tidy. Let us each of these commands in action. But first, make sure you have the tidyr package loaded.\n\nCode# load tidyr package\nlibrary(tidyr)\n\n\n\n3.1 pivot_longer()\nThe pivot_longer() command converts a ‚Äòwide data‚Äô to a ‚Äòlong data‚Äô. It does so by converting row names to a new column under a new variable name with its corresponding values moved into another column with another variable name. So let us see how it goes. We will take the earlier mentioned example and will see how to make it tidy. Now you don‚Äôt have to be concerned with the codes I have used to make the dummy data. Just have your focus on the pivot_longer() syntax.\n\nCodelibrary(tidyr)\n# making a dummy data\n# using sample function to pick random numbers in a sequence\npatient1 <- c(seq(1,5,1))\npatient2 <- c(seq(6,10,1))\npatient3 <- c(seq(11,15,1))\npatient4 <- c(seq(16,20,1))\npatient5 <- c(seq(21,25,1))\n\n# cbind simple combines the columns of same size\ntreatment_data <- cbind(patient1,patient2,patient3,patient4,patient5) \n\ntrt <- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\n\ntrt_data <- cbind(trt, treatment_data)\ntrt_data <- as.data.frame(trt_data) # making it a data frame\n\ntrt_data_tidy <- pivot_longer(trt_data,\n                              c(patient1,patient2,patient3,patient4,patient5), \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\ntrt_data_tidy\n\n\n\n  \n\n\n\nFurthermore, you don‚Äôt have to manually type in the column names as you can use colnames() to call the column names of the dataset. Another way of doing the same is by excluding the first column from the process. By doing so the command will automatically pivot all columns except the excluded ones, so in this way, we don‚Äôt need to manually specify the column names. The codes given below will give you the same result as before.\n\nShow the codelibrary(tidyr)\npatient1 <- c(seq(1,5,1))\npatient2 <- c(seq(6,10,1))\npatient3 <- c(seq(11,15,1))\npatient4 <- c(seq(16,20,1))\npatient5 <- c(seq(21,25,1))\ntreatment_data <- cbind(patient1,patient2,patient3,patient4,patient5) \ntreatment <- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\ntrt_data <- cbind(treatment, treatment_data)\ntrt_data <- as.data.frame(trt_data)\n# using colnames, [-1] is included to exclude the name of first column from the process\ntrt_data_tidy1 <- pivot_longer(trt_data,\n                              colnames(trt_data)[-1], \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\n\n# the same can be done by manually specifying which columns to exclude\n# this can be done by denoting the column name ('treatment' in this case) with '-' sign\ntrt_data_tidy2 <- pivot_longer(trt_data, names_to = \"patient_ID\",\n                               values_to = \"efficacy\", -treatment)\n# checking if both the tidy datasets are one and the same\ntrt_data_tidy1 == trt_data_tidy2\n\n\nThe syntax for pivot_longer() is given below with description\n\nCodepivot_longer(\"data\", c(\"colname1, colname2,.....\"), \n  names_to = \"name of the column where your row names are present\",\n  values_to = \"name of the column where your corresponding row values are present\")\n\n\nHere is a graphical representation\n\n\n3.2 pivot_wider()\nThe pivot_wider() does the exact opposite of what pivot_longer() does, which is to convert long data into wide data. We will use the previously given dummy data.\n\nCodelibrary(tidyr)\n# making a dummy data\n# using sample function to pick random numbers in a sequence\npatient1 <- c(seq(1,5,1))\npatient2 <- c(seq(6,10,1))\npatient3 <- c(seq(11,15,1))\npatient4 <- c(seq(16,20,1))\npatient5 <- c(seq(21,25,1))\n\n# cbind simple combines the columns of same size\ntreatment_data <- cbind(patient1,patient2,patient3,patient4,patient5) \n\ntrt <- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\n\ntrt_data <- cbind(trt, treatment_data)\ntrt_data <- as.data.frame(trt_data) # making it a data frame\n\ntrt_data_tidy <- pivot_longer(trt_data,\n                              c(patient1,patient2,patient3,patient4,patient5), \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\n\n# making the data wide\ntrt_data_wider <- pivot_wider(trt_data_tidy, names_from = \"patient_ID\",\n                              values_from = \"efficacy\")\n\n# paged_Table() for viewing the dataset as a table, \n# you can see that the dataset is same as before\ntrt_data_wider\n\n\n\n  \n\n\n\nThe syntax for pivot_wider() is given below with description\n\nCodepivot_longer(\"data\", \n  names_from = \"name of the column which contains your wide data columns\",\n  values_from = \"name of the column where your corresponding wide data column values are\")\n\n\nHere is a graphical representation"
  },
  {
    "objectID": "tutorials/data_man/project5.html#splitting-and-uniting-cells",
    "href": "tutorials/data_man/project5.html#splitting-and-uniting-cells",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n4 Splitting and uniting cells",
    "text": "4 Splitting and uniting cells\nThere can be an instance where you want to split or untie cells within your dataset. Let us look at some examples.\n\n4.1 unite()\nIn the data given below, let say we want to unite the century column and the year column together. This can be done using the unite() command. You can view the before and after instances in the tabs below.\n\n\nBefore\nAfter\n\n\n\n\nShow the codeevent <- c(letters[1:4])\ncentury <- c(rep(19:20, each = 2))\nyear <- c(seq(10,16,2))\ndata <- as.data.frame(cbind(event,century,year))\n\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nevent <- c(letters[1:4])\ncentury <- c(rep(19:20, each = 2))\nyear <- c(seq(10,16,2))\ndata <- as.data.frame(cbind(event,century,year))\n\n# uniting columns century and year\ndata_new <- unite(data, century, year, col = \"event_year\", sep = \"\")\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of unite() is as follows.\n\nCodeunite(\"dataset name\",\n      \"name of first column to unite, name of second column to unite,.......\",\n      col = \"name of the new column to which all the other column will unite together\",\n      sep = \"input any element as a separator between the joining column values\")\n# in this case we are not putting a sep value\n\n\n\n4.2 separate()\nIn the data given below, let say we want to split the ‚Äòarea_perimeter‚Äô column into two separate columns. This can be done using the separate() command. You can view the before and after instances in the tabs below. As always I will be making dummy data to work with.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n# viewing data as a table\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n\n# separating column values into two separate columns named area and perimeter respectively\ndata_new <- separate(data, area_perimetre, sep = \"_\",\n                     into = c(\"area\", \"perimetre\"))\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of separate() is as follows.\n\nCodeseparate(\"data name\",\n         \"column to separate into\",\n         sep = \"the separator element\",\n         into = c(\"col1\", \"col2\", \"........\")) # column names for the separated values\n\n\n\n4.3 separate_rows()\nSimilar to the above case, you can also separate column values into several rows.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n# viewing data as a table\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nshapes <- c(letters[1:4])\narea <- c(paste0(10:13, \"m^2\"))\nperimetre <- c(paste0(30:33, \"m\"))\nratio <-as.data.frame(cbind(shapes,area,perimetre))\ndata <- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n\n# separating column values into two several rows\ndata_new <- separate_rows(data, area_perimetre, sep = \"_\")\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of separate_rows() is as follows.\n\nCodeseparate_rows(\"data name\",\n         \"column to separate\",\n         sep = \"the separator element\")"
  },
  {
    "objectID": "tutorials/data_man/project5.html#expanding-and-completing-dataset",
    "href": "tutorials/data_man/project5.html#expanding-and-completing-dataset",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n5 Expanding and completing dataset",
    "text": "5 Expanding and completing dataset\nYou can expand your data to include all possible combinations of values of variables listed or complete the dataset with NA values for all possible combinations.\n\n5.1 expand()\nUsing the expand() command we can expand our data with missing combinations for the variables we specify.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# viewing data as a table\ndress_data\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# expanding dataset with brand and dress as variables\ndress_data_expand <- expand(dress_data, brand, dress)\n\n# viewing data as a table\ndress_data_expand\n\n\n\n  \n\n\n\n\n\n\nThe syntax of expand() is as follows.\n\nCodeexpand(\"data name\", \"column names which you want to expand separated by commas\")\n\n\n\n5.2 complete()\nThe complete() command functions similar to the expand() command, but it also fills in NA values for columns which we didn‚Äôt specify, The main reason to use this command would be to convert implicit NA values hidden in the dataset to explicit NA values which are expressed in the dataset. Given below is a comparison between the complete() and expand() commands.\n\n\nexpand()\ncomplete()\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# expanding dataset with brand and dress as variables\ndress_data_expand <- expand(dress_data, brand, dress)\n\n# viewing data as a table\ndress_data_expand\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\n# completing dataset with brand and dress as variables\n# the variable 'size' will be filled with NAs as we did not specify it\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# viewing data as a table\ndress_data_complete\n\n\n\n  \n\n\n\n\n\n\nThe syntax of complete() is as follows.\n\nCodecomplete(\"data name\", \"column names which you want to complete separated by commas\")"
  },
  {
    "objectID": "tutorials/data_man/project5.html#handling-nas-or-missing-values",
    "href": "tutorials/data_man/project5.html#handling-nas-or-missing-values",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n6 Handling NAs or missing values",
    "text": "6 Handling NAs or missing values\nMost data collection would often result in possible NA values. The tidyr package allows us to drop or convert NA values. We will reuse the earlier example. Below tabs show before and removing NA values.\n\n6.1 drop_na()\nUse drop_na() to remove NA value containing rows from the dataset.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# viewing data as a table\ndress_data_complete\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\n\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# dropping NA values\n\ndress_data_noNA <- drop_na(dress_data_complete)\n\n# viewing data as a table\ndress_data_noNA\n\n\n\n  \n\n\n\n\n\n\n\n6.2 fill()\nUse fill() to replace NA values by taking values from nearby cells. By default the NA values as replaced by whatever value that is above the cell containing the NA value. This can be changed by specifying the .direction value within fill()\n\nCodelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# direction 'downup' simultaneously fill both upwards and downwards NA containing cells\ndress_data_fill <- fill(dress_data_complete, size, .direction = \"downup\")\n\n# viewing data as a table\ndress_data_fill\n\n\n\n  \n\n\n\n\n6.3 replace_na()\nUse replace_na() command to replace NA values to whatever value specified.\n\nCodelibrary(tidyr)\n# dummy data\nbrand <- c(letters[1:4])\ndress <- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize <- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data <- as.data.frame(cbind(brand,dress,size))\ndress_data_complete <- complete(dress_data,brand,dress)\n\n# replace NA to unknown\n# specify the column which have NA inside the list()\n# then equate the value which would replace NAs\ndress_data_zero <- replace_na(dress_data_complete, list(size = \"unknown\"))\n\n# viewing data as a table\ndress_data_zero"
  },
  {
    "objectID": "tutorials/data_man/project5.html#summary",
    "href": "tutorials/data_man/project5.html#summary",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n7 Summary",
    "text": "7 Summary\nSo in this chapter, we learned what is tidy data and how we can make our data into tidy data. Making our data tidy is very important as it helps us to analyse and visualise the data in a very efficient manner. We also learned how to reshape our data, how to split or unite cells, how to complete and expand data and how to handle NA values. Hope this chapter was fruitful for you!"
  },
  {
    "objectID": "tutorials/data_man/project5.html#references",
    "href": "tutorials/data_man/project5.html#references",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n8 References",
    "text": "8 References\n\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr\n\nLast updated on\n\n\n[1] \"2022-08-04 01:23:14 IST\""
  },
  {
    "objectID": "tutorials/data_man/project6.html",
    "href": "tutorials/data_man/project6.html",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "",
    "text": "The dplyr package is a grammar of data manipulation just like how ggplot2 is the grammar of data visualization. It helps us to apply a wide variety of functions such as;\n\nSummarising the dataset\nApplying selections and orderings as a function of a variable\nCreating new variables as a function of existing variables\n\nWe will see in-depth how to manipulate our data like a boss!"
  },
  {
    "objectID": "tutorials/data_man/project6.html#the-pipe-operator",
    "href": "tutorials/data_man/project6.html#the-pipe-operator",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n2 The pipe operator %>%",
    "text": "2 The pipe operator %>%\nPerhaps the most amazing thing in making codes short and efficient is the pipe operator which is originally from the magrittr package which is made available for the dplyr package. The pipe operator helps you skip the intermediate steps of saving an object before you can use them in command. It does so by ‚Äòpiping‚Äô together results from the first object to the function ahead of the pipe operator. The command x %>% y %>% z can be read as ‚Äòtake the result of x and use it with function y and take that result and use it with function z‚Äô. This is the gist of what the pipe operator does. Allow me to demonstrate.\n\nlibrary(ggplot2)\n\n# dummy data\na <- c(sample(1:100, size = 50))\nb <- c(sample(1:100, size = 50))\ndata <- as.data.frame(cbind(a,b))\n\n# without %>%\ndata <- mutate(data, ab = a*b, twice_a = 2*a)\ndata_new <- filter(data, ab < 300, twice_a < 200)\nggplot(data_new, aes(ab, twice_a)) + geom_point()\n\n# with %>%\ndata %>% mutate(ab = a*b, twice_a = 2*a) %>% \n  filter(ab < 300, twice_a < 200) %>%\n  ggplot(aes(ab, twice_a)) + geom_point()\n\nAs you can see, with pipe operator %>%, we did not have to save any objects in the intermediate steps and also it improved the overall clarity of the code. I have used a few commands from the dplyr package in the example given above. So without further ado let us delve into the dplyr package. For this chapter, I will be using the penguin dataset from the popular {palmerpenguin} package as an example.\n\n# install palmerpenguins package\ninstall.packages(\"palmerpenguins\")\nlibrary(dplyr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "tutorials/data_man/project6.html#grouping-the-data",
    "href": "tutorials/data_man/project6.html#grouping-the-data",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n3 Grouping the data",
    "text": "3 Grouping the data\n\n3.1 group_by()\nThe command group_by() allows us to group the data via existing variables. It allows for a ‚Äòsplit-apply-combine‚Äô way of getting output. First, it will split the data or group the data with the levels in the variable, then apply the function of our choice and then finally combine the results to give us a tabular output. On its own the command doesn‚Äôt do anything, we use it in conjunction with other commands to get results based on the grouping we specify. The command ungroup() is used to ungroup the data."
  },
  {
    "objectID": "tutorials/data_man/project6.html#summarising-the-data",
    "href": "tutorials/data_man/project6.html#summarising-the-data",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n4 Summarising the data",
    "text": "4 Summarising the data\n\n4.1 summarise()\nThe summarise() command allows you to get the summary statistics of a variable or a column in the dataset. The result is given as tabular data. Many types of summary statistics can be obtained using the summarise() function. Some of them are given below. To calculate average values it is necessary to drop NA values from the dataset. Use drop_na() command from the tidyr package. The comments denote what each summary statistic is.\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nsummary_data <- penguins %>% drop_na() %>%\n  group_by(species) %>% # we are grouping/splitting the data according to species\n  summarise(avg_mass = mean(body_mass_g), # mean mass\n            median_mass = median(body_mass_g), # median mass\n            max_mass = max(body_mass_g), # max value of mass, can also use min()\n            standard_deviation_bill_length = sd(bill_length_mm), # standard deviation of bill_length\n            sum_mass = sum(flipper_length_mm), # sum\n            distinct_years = n_distinct(year), # distinct values in column year\n            no_of_non_NAs = sum(!is.na(year)), # gives no of non NAs, \n            length_rows = n(), # length of the rows\n            iqr_mass = IQR(body_mass_g), # inter quartile range of mass\n            median_absolute_deviation_mass = mad(body_mass_g), # median absolute deviation of mass\n            variance_mass = var(body_mass_g)) # variance\n# viewing summary as a table\nsummary_data\n\n\n\n  \n\n\n\nThe number of non NA values will be the same as that of n() result as we have used drop_na()command in the beginning.\nThe base function summary() in R also gives the whole summary statistics of a dataset.\n\nlibrary(palmerpenguins)\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\n4.2 when to use group_by()\nIt can be confusing to decide when to use the group_by() function. In short, you should use it whenever you want any function to act separately on different groups present in the dataset. Here is a graphical representation of how the summarise() function is used to calculate the mean values of a dataset. When used with group_by() it calculates mean values for the respective groups in the data, but when group_by() is not used, it will calculate the mean value of the entire dataset irrespective of the different groups present and outputs a single column.\n\n\n\n\n\n\n4.3 count()\nThe count() command is used to count the number of rows of a variable. Has the same function as that of n()\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\ncount <- penguins %>% group_by(species) %>%\n  count(island)\n\n# viewing count as a table\ncount"
  },
  {
    "objectID": "tutorials/data_man/project6.html#manipulating-cases-or-observations",
    "href": "tutorials/data_man/project6.html#manipulating-cases-or-observations",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n5 Manipulating cases or observations",
    "text": "5 Manipulating cases or observations\nThe following functions affect rows to give a subset of rows in a new table as output.\n\n5.1 filter()\nUse filter() to filter rows corresponding to a given logical criteria\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% filter(body_mass_g < 3000)\n\n\n\n  \n\n\n\n\n5.2 distinct()\nUse distinct() to remove rows with duplicate or same values.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% group_by(species) %>% distinct(body_mass_g)\n\n\n\n  \n\n\n\n\n5.3 slice()\nUse slice() to select rows by position.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# slice from first row to fifth row\npenguins %>% slice(1:5) \n\n\n\n  \n\n\n\n\n5.4 slice_sample()\nUse slice_sample() to randomly select rows from the dataset. Instead of (n = ) you can also provide the proportion value (between 0 and 1) using (prop = ). For e.g.¬†for a dataset with 10 rows, giving (prop = 0.5) will randomly sample 5 rows. Other related functions include;\n\n\npreserve : Values include TRUE to preserve grouping in a grouped dataset and FALSE to not preserve grouping while sampling.\n\nweight_by : Gives priority to a particular variable during sampling. An example is given below.\n\nreplace : Values include TRUE if you want sampling with replacement which can result in duplicate values, FALSE if you want sampling without replacement.\n\n\n\n(n = 4)\nweight_by\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# samples 4 rows randomly\npenguins %>% slice_sample(n = 4)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# sampling will favour rows with higher values of 'body_mass_g'\npenguins %>% drop_na() %>% slice_sample(n = 4, weight_by = body_mass_g)\n\n\n\n  \n\n\n\n\n\n\n\n5.5 slice_min() and slice_max()\nUse slice_min() to extract rows containing least values and use slice_max() to extract rows with greatest values. The function with_ties = FALSE is included to avoid tie values.\n\n\nslice_min()\nslice_max()\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_min(body_mass_g, n = 4, with_ties = FALSE)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_max(body_mass_g, n = 4,with_ties = FALSE)\n\n\n\n  \n\n\n\n\n\n\n\n5.6 slice_head() and slice_tail()\nUse slice_head() to extract first set of rows and use slice_tail() to extract last set of rows.\n\n\nslice_head()\nslice_tail()\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_head(n = 4)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% slice_tail(n = 4)\n\n\n\n  \n\n\n\n\n\n\n\n5.7 arrange()\nUse arrange() to arrange rows in a particular order.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# arranging rows in descending order of bill length\n# by default it arranges data by ascending order when no specifications are given\npenguins %>% arrange(desc(bill_length_mm))\n\n\n\n  \n\n\n\n\n5.8 add_row()\nUse add_row() to add rows to the dataset.\n\nlibrary(dplyr)\n\nName <- c(\"a\", \"b\")\nAge <- c(12,13)\ndata.frame(Name, Age) %>% add_row(Name = \"c\", Age = 15)"
  },
  {
    "objectID": "tutorials/data_man/project6.html#manipulating-variables-or-columns",
    "href": "tutorials/data_man/project6.html#manipulating-variables-or-columns",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n6 Manipulating variables or columns",
    "text": "6 Manipulating variables or columns\nThe following functions affect columns to give a subset of columns in a new table as output.\n\n6.1 pull()\nUse pull() to extract columns as a vector, by name or index. Only the first 10 results are shown for easy viewing.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% pull(body_mass_g)\n\n\n6.2 select()\nUse select() to extract columns as tables, by name or index.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% select(species, body_mass_g)\n\n\n\n  \n\n\n\n\n6.3 relocate()\nUse relocate() to move columns to new position. Results are not shown as these are trivial results.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# relocates 'species' column to last position\npenguins %>% relocate(species, .after = last_col())\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# relocates 'species' column before column 'year' and renames the column as 'penguins'\npenguins %>% relocate(penguins = species, .before = year)\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# you can also relocate columns based on their class\n# relocates all columns with 'character' class to last position\npenguins %>% relocate(where(is.character), .after = last_col())\n\n\n6.4 rename()\nUse rename() function to rename column names in the dataset.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# renames the column sex to gender\npenguins %>% rename(gender = sex)\n\n\n6.5 mutate()\nUse mutate() function to create new columns or variables.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% drop_na() %>% \n  group_by(species) %>%\n  mutate(mean_mass = mean(body_mass_g))\n\n\n\n  \n\n\n\n\n6.6 transmute()\nDoes the same function as mutate() but in the process will drop any other columns and give you a table with only the newly created columns.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% drop_na() %>% \n  group_by(species) %>%\n  transmute(mean_mass = mean(body_mass_g))\n\n\n\n  \n\n\n\n\n6.7 across()\nUse across() to summarise or mutate columns in the same way. First example shows across() used with summarise() function.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# summarise across columns body mass, bill length and bill depth\n# and calculate the mean values\n# since we are calculating mean values,\n# NAs are dropped using 'drop_na() function from 'tidyr' package\n\npenguins %>% drop_na() %>%\n  group_by(species) %>%\n  summarise(across(c(body_mass_g, bill_length_mm, bill_depth_mm), mean))\n\n\n\n  \n\n\n\nSecond example showing across() used with mutate() function. We can efficiently create new columns using mutate() and across() together. Suppose we want to multiply all numerical values in a dataset with 2 and create new columns of those values. This can be done using the code below.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# define the function\ntwo_times <- function(x) {\n  2*x\n} \n\n# .name will rename the new columns with 'twice` prefix combined with existing col names\npenguins %>% group_by(species) %>%\n  mutate(across(where(is.numeric), two_times, .names = \"two_times_{col}\"))\n\n\n\n  \n\n\n\nThe same code when used just with mutate() function will look like this\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# define the function\ntwo_times <- function(x) {\n  2*x\n}\n\n# using only 'mutate()' function\npenguins %>% group_by(species) %>%\n  mutate(twice_bill_lenght = two_times(bill_length_mm),\n         twice_body_mass = two_times(body_mass_g),\n         .....)\n\nSo in this code, I will have to manually type all the col names and apply the operation individually which is too much of a hassle. Now we can better appreciate how efficient it is in using mutate() and across() functions together.\n\n6.8 c_across()\nThe function c_across() is similar to the earlier mentioned across() function. But instead of doing a column-wise function, it applies function across columns in a row-wise manner. Now, most functions in R by default computes across columns, so to specify row-wise computation, we have to explicitly use the function rowwise() in conjunction with other functions. In the example below we will sum both bill and flipper lengths of the penguins in the penguins dataset and create a new column called ‚Äòsum_of_lengths‚Äô.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% drop_na() %>%\n  group_by(species) %>%\n  rowwise() %>%\n  transmute(sum_of_length = sum(c_across(c(bill_length_mm,flipper_length_mm))))"
  },
  {
    "objectID": "tutorials/data_man/project6.html#summary",
    "href": "tutorials/data_man/project6.html#summary",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n7 Summary",
    "text": "7 Summary\nThe dplyr package is the grammar of the data manipulation in R. It features well-made functions to help us summarise the data, group data by variables and manipulate columns and rows in the dataset. In this chapter, we learned in detail the different functions that help us manipulate data efficiently and have seen case examples also. In the next chapter, we will see the remaining set of functions in the dplyr package."
  },
  {
    "objectID": "tutorials/data_man/project6.html#references",
    "href": "tutorials/data_man/project6.html#references",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n8 References",
    "text": "8 References\n\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr Here is the link to the cheat sheet explaining each function in dplyr.\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr\n\nLast updated on\n\n\n[1] \"2022-08-04 19:47:20 IST\""
  },
  {
    "objectID": "tutorials/data_man/project7.html",
    "href": "tutorials/data_man/project7.html",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "",
    "text": "In the previous chapter we have seen quite a lot of functions from the dplyr package. In this chapter, we will see the rest of the functions where we learn how to handle row names, how to join columns and rows and different set operations in the dplyr package.\n\n# loading necessary packages\nlibrary(dplyr)\n\n\nTidy data does not use row names. So use rownames_to_column() command to convert row names to a new column to the data. The function column_to_rownames() does the exact opposite of rownames_to_column() as it converts a column into rownames but make sure that the column you are converting into rownames does not contain NA values.\n\n# mtcars dataset contains rownames\n# creates new column called car_names which contains row names\nmtcars %>% rownames_to_column(var = \"car_names\")\n\n# returns the original mtcars dataset\nmtcars %>% rownames_to_column(var = \"car_names\") %>%\n  column_to_rownames(var = \"car_names\")"
  },
  {
    "objectID": "tutorials/data_man/project7.html#combine-tablescolumns",
    "href": "tutorials/data_man/project7.html#combine-tablescolumns",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n2 Combine tables/columns",
    "text": "2 Combine tables/columns\n\n2.1 bind_cols()\nJoins columns with other columns. Similar function as that of cbind() from base R.\n\ndf1 <- tidytable::data.table(x = letters[1:5], y = c(1:5))\ndf2 <- tidytable::data.table(x = letters[3:7], y = c(6:10))\nbind_cols(df1,df2)\n\n#similar functionality\ncbind(df1,df2)\n\n\n2.2 bind_rows()\nJoins rows with other rows. Similar function as that of rbind() from base R.\n\ndf1 <- tidytable::data.table(x = letters[1:5], y = c(1:5))\ndf2 <- tidytable::data.table(x = letters[3:7], y = c(6:10))\nbind_rows(df1,df2)\n\n#similar functionality\nrbind(df1,df2)\n\nThe functions that are described below have the same functionality as that of bind_cols() but give you control over how the columns are joined."
  },
  {
    "objectID": "tutorials/data_man/project7.html#mutating-joins-and-filtering-joins",
    "href": "tutorials/data_man/project7.html#mutating-joins-and-filtering-joins",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n3 Mutating joins and filtering joins",
    "text": "3 Mutating joins and filtering joins\nMutating joins include left_join(), right_join(), inner_join() and full_join() and filtering joins include semi_join() and anti_join().\n\n\nleft_join()\nright_join()\ninner_join()\nfull_join()\nanti_join()\nsemi_join()\n\n\n\nIn the code below, matching variables of df2 are joined with df1. In the final data, you can see that only kevin and sam from df2 are matched with df1, and only those row values are joined with df1. For those variables which didn‚Äôt get a match, the row values for those are filled with NA. You can interpret the variables with NA values as; both john and chris are not present in df2.\nIf you are familiar with set theory in mathematics, what we are doing essentially is similar to (df1 \\cap df2) \\cup df1.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% left_join(df2)\n\n\n\n  \n\n\n\n\n\nSimilar to left_join() but here, you will be joining matching values from df1 to df2, the opposite of what we did earlier. As you can see only kevin and sam from the df1 is matched with df2, and only those row values are joined with df2. For the variables which didn‚Äôt get a match, the row values for those are filled with NA. You can interpret the variables with NA values as; bob is not present in df1.\nThis function, in the manner used here, is similar to (df1 \\cap df2) \\cup df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% right_join(df2)\n\n\n\n  \n\n\n\n\n\nThe function inner_join() compares both df1 and df2 variables and only joins rows with the same variables. Here only kevin and sam are common in both the dataframes so the row values of only those columns are joined and others are omitted.\nThis function is similar to df1 \\cap df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% inner_join(df2)\n\n\n\n  \n\n\n\n\n\nThe function full_join() compares both df1 and df2 variables and joins all possible matches while retaining both mistakes in df1 and df2 with NA values.\nThis function is similar to df1 \\cup df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% full_join(df2)\n\n\n\n  \n\n\n\n\n\nThis is an example of filtering join. The function anti_join() compares df1 variables to and df2 variables and only outputs those variables of df1 which didn‚Äôt get a match with df2.\nThis function, in the manner used here, is similar to df1 \\cap df2^c.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %>% anti_join(df2)\n\n\n\n  \n\n\n\n\n\nThis is an example of filtering join. The function semi_join() is similar to inner_join() but it only gives variables of df1 which has a match with df2.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\ndf1 %>% semi_join(df2)\n\n\n\n  \n\n\n\n\n\n\nHere is a nice graphical representation of the functions we just described now. Image source.\n\n\n\n\n\n(a) Mutating joins\n\n\n\n\n\n\n(b) Filtering joins\n\n\n\n\nFigure 1: Graphical abstract for joins. Image source: RPubs.com"
  },
  {
    "objectID": "tutorials/data_man/project7.html#additional-commands-for-joins",
    "href": "tutorials/data_man/project7.html#additional-commands-for-joins",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n4 Additional commands for joins",
    "text": "4 Additional commands for joins\nAdditionally, you can specify which common columns to match.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\n# match with column 'x'\ndf1 %>% left_join(df2, by = \"x\")\n\n\n\n  \n\n\ndf3 <- tidytable::data.table(a = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf4 <- tidytable::data.table(b = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\n# matching with column having different names, a and b in this case\ndf3 %>% left_join(df4, by = c(\"a\" = \"b\"))"
  },
  {
    "objectID": "tutorials/data_man/project7.html#set-operations",
    "href": "tutorials/data_man/project7.html#set-operations",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n5 Set operations",
    "text": "5 Set operations\nSimilar to the mutating join functions that we had seen, there are different functions related to set theory operations.\n\n5.1 intersect()\nOutputs common rows in the dataset.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nintersect(df1, df2)\n\n\n\n  \n\n\n\n\n5.2 setdiff()\nOutputs rows in first data frame but not in second data frame.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nsetdiff(df1, df2)\n\n\n\n  \n\n\n\n\n5.3 union()\nOutputs all the rows in both dataframes\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nunion(df1, df2)\n\n\n\n  \n\n\n\n\n5.4 setequal()\nChecks whether two datasets have same number of rows.\n\nlibrary(dplyr)\n\ndf1 <- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 <- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nsetequal(df1, df2)\n\n[1] FALSE"
  },
  {
    "objectID": "tutorials/data_man/project7.html#summary",
    "href": "tutorials/data_man/project7.html#summary",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n6 Summary",
    "text": "6 Summary\nIn this chapter, we have seen;\n\n\nHow to handle row names\nHow to combine columns and rows\nWhat are mutating and filtering joins and various set operations\n\n\nThus to conclude this chapter, we have now learned almost all functions in the dplyr package and have seen how to manipulate data efficiently. With the knowledge of the pipe operator that we have seen in chapter 1, we are now equipped to write codes compactly and more clearly. I hope this chapter was useful for you and I will see you next time."
  },
  {
    "objectID": "tutorials/data_man/project7.html#references",
    "href": "tutorials/data_man/project7.html#references",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n7 References",
    "text": "7 References\n\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr. Here is the link to the cheat sheet explaining each and every function in dplyr.\n\nLast updated on\n\n\n[1] \"2022-08-04 19:48:15 IST\""
  },
  {
    "objectID": "tutorials/data_viz/project1.html",
    "href": "tutorials/data_viz/project1.html",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "",
    "text": "This series of chapters are focused on people who have a basic understanding of R programming. You are expected to know how the basic syntaxes of R language work like how to assign values, how to load your data, the different operations of R, and so on. If you just started out on your R journey then this chapter and the rest will become a roadblock for you. But fear not, there are some amazing websites that teach you the very basics of R, and that too for free! For an interactive way of learning, I recommend DataCamp. It is an online platform for learning programming languages. They have both paid and free classes. Luckily for us, they are providing the introductory classes on R programming for free. This would be a great way to start your R journey. After you completed the course on data camp you can come back to this blog and you will find it very easy to comprehend and learn the various chapters that are available here. Coming from my own experience, it would be really helpful if you have your own data to work with. Rather than religiously following the steps in these chapters, I would recommend you have a goal in your mind before diving into the chapters. The goal should be to try incorporating the things you learned here into your own data. That would be the best way to learn anything in R. Hope you have a great time learning!"
  },
  {
    "objectID": "tutorials/data_viz/project1.html#introduction-to-ggplot2-package",
    "href": "tutorials/data_viz/project1.html#introduction-to-ggplot2-package",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n2 Introduction to ggplot2 package",
    "text": "2 Introduction to ggplot2 package\nIn this chapter we will be plotting different types of graphs using a package called ggplot2 in R. The ggplot2 package is based on ‚Äògrammar of graphics plot‚Äô which provides a systematic way of doing data visualizations in R. With a few lines of code you can plot a simple graph and by adding more layers onto it you can create complex yet elegant data visualizations.\nA ggplot2 graph is made up of three components.\n\n\nData: Data of your choice that you want to visually summarise.\n\nGeometry or geoms: Geometry dictates the type of graph that you want to plot and this information is conveyed to ggplot2 through the geom() command code. For e.g.¬†using the geom_boxplot() command, you can plot a box plot with your data. Likewise, there are many types of geometry that you can plot using the ggplot2 package.\n\nAesthetic mappings: Aesthetics define the different kinds of information that you want to include in the plot. One fo the most important aesthetic is in choosing which data values to plot on the x-axis and the y-axis. Another example is changing the colour of the data points, which can be used to differentiate two different categories in the data. The use of aesthetics depends on the geometry that you are using. We use the command aes() for adding different types of aesthetics to the plot. We will learn more about aes() in Chapter 2. For now, we will only see what kind of plots can be made using the ggplot2 package. We will learn how to tweak them in Chapter 2.\n\nThis tutorial is primarily focused on students who are beginners in R programming and wants to quickly plot their data without much of a hassle. So without further ado let‚Äôs plot some graphs!"
  },
  {
    "objectID": "tutorials/data_viz/project1.html#setting-up-the-prerequisites",
    "href": "tutorials/data_viz/project1.html#setting-up-the-prerequisites",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n3 Setting up the prerequisites",
    "text": "3 Setting up the prerequisites\nFirst, we need to install the ggplot2 package in R as it does not come in the standard distribution of R.\n\nTo install packages in R we use the command install.packages() and to load packages we use the command library(). Therefore to install and load ggplot2 package we use the following lines of command.\n\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nAll right we have the ggplot2 package loaded, now we just need some data to plot. Most R programming tutorials use the iris dataset as an example. But this tutorial won‚Äôt be like most tutorials. So let me introduce you to some lovely penguins from Palmer Station in Antarctica!\nFor this tutorial, we will be installing the palmerpenguins package which showcases body measurements taken from three different species of penguins from Antarctica. This package was made possible by the efforts of Dr.¬†Allison Horst. The penguin data was collected and made available by Dr.¬†Kristen Gorman and the Palmer Station, Antarctica LTER.\n\nInstall the palmerpenguins package and load it in R.\n\n\ninstall.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\nNow there are two datasets in this package. We will be using the penguins dataset which is a simplified version of the raw data present in the package.\n\nUse the command head() to display the first few values of penguins dataset to see how it looks like\n\n\nlibrary(palmerpenguins)\nhead(penguins)\n\n\n\n  \n\n\n\nWe can see that are 8 columns in the dataset representing different values. Now let us try plotting some graphs with this data.\n\n3.1 Bar graph\nSo we will try to plot a simple bar graph first. Bar graphs are used to represent categorical data where the height of the rectangular bar represents the value for that category. We will plot a bargraph representing frequency data for all three species of penguins.\n\nWe will be using the geom_bar() command to plot the bar graph. Let us also use the command theme_bw() for a nice looking theme.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, fill = species)) + \n  xlab(\"Species\") + ylab(\"Frequency\") + \n  ggtitle(\"Frequency of individuals for each species\") + \n  geom_bar() + theme_bw()\n\n\n\n\n\n3.2 Histogram\nHistograms are similar to bar graphs visually. But histograms are used to represent continuous data. Also the all the rectangular bars will have the same bin size or width.\n\nWe can plot a histogram using the command geom_histogram().\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, fill = species)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Frequency\") + \n  ggtitle(\"Frequency of individuals for respective body mass\") + \n  geom_histogram(bins = 25) + theme_bw()\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nThe warning message indicates that for two rows in the dataset, they have NA values or that they did not have any values present. This is true for real-life cases, as during data collection sometimes you will be unable to collect data due to various reasons. So this is perfectly fine.\n\n3.3 Line graph\nLine graph simply joins together data points to show overall distribution.\n\nUse the command geom_line() for plotting a line graph.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = bill_length_mm, \n                            y = bill_depth_mm, colour = species)) + \n  xlab(\"Bill length (mm)\") + ylab(\"Bill depth (mm)\") + \n  ggtitle(\"Bill length vs Bill depth\") + geom_line() + theme_bw()\n\n\n\n\n\n3.4 Scatter plot\nThe scatter plot simply denotes the data points in the dataset.\n\nUse the command geom_point() to plot a scatter plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm, \n                            shape = species, colour = species)) + \n  xlab(\"Body mass (g)\") + ylab(\"Flipper length (mm)\") + \n  ggtitle(\"Body mass vs Filpper length\") + geom_point(size = 2) + theme_bw()\n\n\n\n\n\n3.5 Density Plot\nDensity plots are similar to histograms but show it shows the overall distribution of the data in a finer way. This way we will get a bell-shaped curve if our data follows a normal distribution.\n\nUse the command geom_density() to a density plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, fill = species)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density() + theme_bw()\n\n\n\n\nSince we plotted for all three species the graph looks clustered. Let us try plotting the same graph for only gentoo penguins. We will use the dplyr package to filter() data for gentoo penguins alone. The dplyr package comes in-built with R so just load the dplyr package using the command library().\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins_gentoo <- penguins %>% filter(species == \"Gentoo\")\n\nggplot(data = penguins_gentoo, aes(x = body_mass_g)) + \n  xlab(\"Body Mass of Gentoo penguins (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Gentoo penguins\") + \n  geom_density(fill = \"red\") + theme_bw()\n\n\n\n\n\n3.6 Dot-plot\nDot-plot is similar to a density plot but it shows discretely each data point in the distribution.\n\nUse the command geom_dotplot() to plot a dot-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", binwidth = 100) + theme_bw()\n\n\n\n\n\n3.7 Rug-plot\nRug-plot is a simple way to visualize the distribution of data along the axis lines. It is often used in conjunction with other graphical representations.\n\nUse the command geom_rug() to plot a rug-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins_gentoo <- penguins %>% filter(species == \"Gentoo\")\n\nggplot(data = penguins_gentoo, aes(x = body_mass_g, y = flipper_length_mm)) + \n  xlab(\"Body Mass of Gentoo penguins (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Gentoo penguins\") + \n  geom_point(colour = \"darkred\") + geom_rug() + theme_bw()\n\n\n\n\n\n3.8 Box plot\nBox-plot is one of the better ways of showing data via quartiles. You can learn more about box plots here.\n\nUse the command geom_boxplot() to plot a box-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, colour = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + geom_boxplot() + \n  theme_bw()\n\n\n\n\n\n3.9 Violin plot\nViolin plot can be considered as the best of both a box-plot and a density plot. It shows the quartile values, like in a box-plot and also shows the distribution of the data, like in a density plot.\n\nUse the command geom_violin() in conjunction with geom_boxplot() to plot a violin plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_violin(aes(colour = species), trim = TRUE) + geom_boxplot(width = 0.2) +\n  theme_bw()"
  },
  {
    "objectID": "tutorials/data_viz/project1.html#saving-your-ggplot2-graphs",
    "href": "tutorials/data_viz/project1.html#saving-your-ggplot2-graphs",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n4 Saving your ggplot2 graphs",
    "text": "4 Saving your ggplot2 graphs\n\nUse the command ggsave() to save the graph locally. In the code below, ‚Äòmy_graph‚Äô is the ggplot element containing your graph. The plot will be saved in your working directory.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nmy_graph <- ggplot(data = penguins, aes(x = species, y = body_mass_g,\n                                    fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_violin(aes(colour = species), trim = TRUE) + \n  geom_boxplot(width = 0.2) +\n  theme_bw()\n\n#to save the plot\nggsave(my_graph, filename = \"your_graph_name.png\", width = 20, height = 20,\n       units = \"cm\")"
  },
  {
    "objectID": "tutorials/data_viz/project1.html#summary",
    "href": "tutorials/data_viz/project1.html#summary",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope this tutorial helped you to get familiarize with the ggplot2 commands. The best way to learn R is by actually doing it yourself. So try to recreate the examples given in this tutorial and then try to apply what you learned using the different datasets available in R. In chapter 2, we will learn how to customize the plots by tweaking the aesthetic mappings."
  },
  {
    "objectID": "tutorials/data_viz/project1.html#references",
    "href": "tutorials/data_viz/project1.html#references",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218.\n\nLast updated on\n\n\n[1] \"2022-08-04 19:49:06 IST\""
  },
  {
    "objectID": "tutorials/data_viz/project2.html",
    "href": "tutorials/data_viz/project2.html",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "",
    "text": "After going through Chapter 1 you would be now familiar with the different types of graphs that you can plot using ggplot2. So for this tutorial, we will be learning how to customize those ggplot graphs to our liking. We will learn how to tweak the aesthetics, how to change labels and how to modify and change the axes in a graph.\nSo let us plot a graph from scratch and learn how to use different aesthetics available."
  },
  {
    "objectID": "tutorials/data_viz/project2.html#setting-up-the-prerequisites",
    "href": "tutorials/data_viz/project2.html#setting-up-the-prerequisites",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n1 Setting up the prerequisites",
    "text": "1 Setting up the prerequisites\nFirst, we need to install the ggplot2 package in R as it does not come in the standard distribution of R. For the dataset, we will first download the Stat2Data package which houses a lot of cool datasets. For this tutorial let us use the Hawks dataset which showcases body measurements from three different species of Hawks. This data was collected by students and faculty at Cornell College in Mount Vernon and the dataset was made available by late Prof.¬†Bob Black at Cornell College.\n\nTo install packages in R we use the command install.packages() and to load packages we use the command library(). Therefore to install and load ggplot2 and {Stats2Data} packages we use the following lines of command. Call the Hawks data using the data() command.\n\n\n# Installng packages\ninstall.packages(\"ggplot2\")\ndevtools::install_github(\"statmanrobin/Stat2Data\")\n\n# Loading required packages\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\n# Loading the Hawks dataset\ndata(\"Hawks\")\n\n\nLet us look at how the dataset is structured. Use str() command\n\n\nlibrary(Stat2Data)\ndata(\"Hawks\")\n\nstr(Hawks)\n\n'data.frame':   908 obs. of  19 variables:\n $ Month       : int  9 9 9 9 9 9 9 9 9 9 ...\n $ Day         : int  19 22 23 23 27 28 28 29 29 30 ...\n $ Year        : int  1992 1992 1992 1992 1992 1992 1992 1992 1992 1992 ...\n $ CaptureTime : Factor w/ 308 levels \" \",\"1:15\",\"1:31\",..: 181 25 138 42 62 71 181 88 261 192 ...\n $ ReleaseTime : Factor w/ 60 levels \"\",\" \",\"10:20\",..: 1 2 2 2 2 2 2 2 2 2 ...\n $ BandNumber  : Factor w/ 907 levels \" \",\"1142-09240\",..: 856 857 858 809 437 280 859 860 861 281 ...\n $ Species     : Factor w/ 3 levels \"CH\",\"RT\",\"SS\": 2 2 2 1 3 2 2 2 2 2 ...\n $ Age         : Factor w/ 2 levels \"A\",\"I\": 2 2 2 2 2 2 2 1 1 2 ...\n $ Sex         : Factor w/ 3 levels \"\",\"F\",\"M\": 1 1 1 2 2 1 1 1 1 1 ...\n $ Wing        : num  385 376 381 265 205 412 370 375 412 405 ...\n $ Weight      : int  920 930 990 470 170 1090 960 855 1210 1120 ...\n $ Culmen      : num  25.7 NA 26.7 18.7 12.5 28.5 25.3 27.2 29.3 26 ...\n $ Hallux      : num  30.1 NA 31.3 23.5 14.3 32.2 30.1 30 31.3 30.2 ...\n $ Tail        : int  219 221 235 220 157 230 212 243 210 238 ...\n $ StandardTail: int  NA NA NA NA NA NA NA NA NA NA ...\n $ Tarsus      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ WingPitFat  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ KeelFat     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Crop        : num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nSo there is a lot of information in the dataset which we can use for plotting. So let us try plotting them."
  },
  {
    "objectID": "tutorials/data_viz/project2.html#building-a-plot",
    "href": "tutorials/data_viz/project2.html#building-a-plot",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n2 Building a plot",
    "text": "2 Building a plot\nOne thing to remember here is that how ggplot2 builds a graph is by adding layers. Let us start by plotting the basic layer first where the x-axis shows ‚Äòweight of the hawks‚Äô and the y-axis shows ‚Äòwingspan of the hawks‚Äô.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n\nggplot(data = Hawks, aes(x = Weight, y = Wing))\n\n\n\n\nWait a sec! Where are my data points? So right now if we look at the syntax of the ggplot code we can see that we have not told ggplot2 which geometry we want. Do we want a scatter plot or a histogram or any other type of graph? So let us plot a scatter plot first. Use geom_point() command. By adding geom_point() to the ggplot() command is equivalent to adding an extra layer to the already existing layer that we got previously. Let us also use theme_bw() for a nice looking theme.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + geom_point() + \n  theme_bw()\n\n\n\n\nWe got the graph! but we also got a warning message. The warning message tells us that the dataset which we had used to plot the graph had 11 rows of NA values and which could not be plotted into the graph. In real-life cases, we can have datasets with NA values due to various reasons, so this is fine.\nNow, this graph even though shows us data points we are not sure which point belongs to which species, as this dataset contains data for three species of Hawks. So let us try giving different colours to the points concerning the different species so that we are able to differentiate them.\n\n2.1 Changing colour\n\nTo change colour of the ‚Äòelement‚Äô as a function species, we have to add colour = Species within the aes() of the ggplot command. I use the general term ‚Äòelement‚Äô here to emphasize that the same change in aesthetics will work for most of other types of geometries in ggplot2 (something which you have seen extensively in Chapter 1. Like for a line graph, the ‚Äòelement‚Äô would be lines. Here we have a scatter plot, so the ‚Äòelement‚Äô would be points.\n\nAlso note that, in addition to colour, R also recognizes color and col wordings and they function the same as colour.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw()\n\n\n\n\nThe species abbreviations are the following: CH=Cooper‚Äôs, RT=Red-tailed, SS=Sharp-Shinned.\nNow, this graph is way better than the previous one.\n\n2.2 Changing point shape.\n\nNow instead of the colour let us change the shape of the point. Use shape() command in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, shape = Species)) + # instead of colour use shape.\n  geom_point() + theme_bw() \n\n\n\n\nNow we did change the shape of points but it is still hard to make out the difference. Let us try specifying colour along with the shape\n\nAdding both colour and shape in aesthetics\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, \n  colour = Species, shape = Species)) + geom_point() + theme_bw()\n\n\n\n\nThis plot is much better than the previous one.\nNow let us try specifying colour within the aes() of the geom()\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, shape = Species)) + \n  geom_point(aes(colour = Species)) + theme_bw()\n\n\n\n\nWe got the same graph as before! So what is the difference in specifying colour within aes() of ggplot() compared to the same but within geom_point(). Here Let us look at another example.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = \"red\")) + \n  geom_point() + theme_bw()\n\n\n\n\nI manually changed the colour of the points to red colour. Please not that you can also use hex codes to specify the colour attribute. Now let try specifying colour to the aes() within the geom()\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = \"red\")) + \n  geom_point(aes(colour = Species)) + theme_bw()\n\n\n\n\nYou can see that the red colour is overridden by other colours. So the aes() mapping (in this case colour) within geom_point() will override any aes() mapping within ggplot(). And whatever aes() mapping we give within ggplot() will be inherited by all other geom layers that are specified.\nLet us see another case.\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_point(colour = \"darkred\") + theme_bw()\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_point(aes(colour = \"darkred\")) + theme_bw()\n\n\n\n\nIf you compare both the codes, the only difference is that the colour = \"darkred\" command was outside aes() in the first code and inside aes() in the second code. So why didn‚Äôt the second graph have the same dark-red coloured points as the first one? The reason is that in the first code we are explicitly told to have all data points to be coloured dark-red but that is not the case with the second code. In the second code, since we have specified it inside aes(), ggplot is trying to look for a variable called ‚Äúdarkred‚Äù inside the dataset and colour it accordingly. This is why the legend that appears in the second graph has listed ‚Äúdarkred‚Äù as a category. And ggplot fails to find the variable called ‚Äúdarkred‚Äù but it still recognizes the colour command line and colour all the points in red. So the bottom line is that R has a pre-determined way of reading a code, so we users should well-understand what each line is expected to do and should not expect R to just fill it in accordingly to what we write.\nNow let us try a few other examples;\n\n2.3 Changing size\n\nUse size() in aes(). The shape aesthetic works best if the input variable is categorical.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Species, y = Hallux, size = Culmen)) + \n  geom_point() + theme_bw()\n\n\n\n\n\n2.4 Changing colour, shape and size manually\n\nUse scale_shape_manual() for changing shape, similarly scale_color_manual() for changing colour and scale_size_manual() for changing size of the element.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Hallux, colour = Species,\n                         shape = Species, size = Species)) + \n  geom_point() +\n  scale_shape_manual(values=c(1, 2, 3)) +\n  scale_color_manual(values=c('red','blue', 'green')) +\n  scale_size_manual(values=c(1,5,10)) + theme_bw()\n\n\n\n\n\n\n\n\n\n\n2.5 Changing the opcaity of the elements\n\nUse alpha() within the geom() with a numeric value to change the opacity of the elements. This is useful for visualizing large datasets such as this.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point(alpha = 1/5) + theme_bw()\n\n\n\n\nThe same commands also work for most of the other types of geom(). Now let us see a few other aesthetics in other types of geoms.\n\n2.6 Changing fill colour\n\nUse fill() in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, fill = Species)) +\n  geom_histogram(bins = 25) + theme_bw()\n\n\n\n\n\nUse scale_fill_manual() to manually change the colours.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, fill = Species)) +\n  geom_histogram(bins = 25) + theme_bw() + \n  scale_fill_manual(values = c(\"darkred\", \"darkblue\", \"darkgreen\"))\n\n\n\n\n\n2.7 Changing line type\n\nUse linetype in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, linetype = Species)) + \n  geom_line() + theme_bw()\n\n\n\n\n\nYou can manually change line types using scale_linetype_manual()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, linetype = Species)) + \n  geom_line() + \n  scale_linetype_manual(values= c(\"twodash\", \"longdash\", \"dotdash\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNow let us also see how to change the labels in a graph.\n\n2.8 Viewing datapoints as labels\n\nYou can plot data points as their values or as their labels using the geom_text() function.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Plotting the values\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_text(aes(label = Wing))\n\n\n\n# Plotting the values according to species text label\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_text(aes(label = Species))\n\n\n\n\n\n2.9 Changing labels in the axes\n\nUse xlab() to change x-axis title, ylab() to change y-axis title, ggtitle() with label and subtitle to add title and subtitle respectively.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + xlab(\"Weight (gm)\") + ylab(\"Wing (mm)\") +\n  ggtitle(label = \"Weight vs Wing span in three different species of Hawks\", \n          subtitle = \"CH=Cooper's, RT=Red-tailed, SS=Sharp-Shinned\")\n\n\n\n\n\nThe same result can be obtained by using labs() to specify each label in the graph. For renaming the legend title, the command will depend on what is there within the aes() or in other words what is the legend based on.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + labs(x = \"Weight (gm)\", y = \"Wing (mm)\", \n  title= \"Weight vs Wing span in three different species of Hawks\", \n  subtitle = \"CH=Cooper's, RT=Red-tailed, SS=Sharp-Shinned\",\n  caption = \"Source: Hawk dataset from Stat2Data r-package\", #caption for the graph\n  colour = \"Hawk Species\", # rename legend title\n  tag = \"A\") #figure tag\n\n\n\n\n\n2.10 Tweaking the axes\n\nUse xlim() and ylim() for limiting x and y axes respectively.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + xlim(c(0,1000)) + ylim(c(200,350))\n\n\n\n\n\nUse coord_cartesian() to zoom in on a particular area in the graph\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + coord_cartesian(xlim = c(0,1000),\n                                                   ylim = c(200,350))\n\n\n\n\n\nUse coord_flip() to flip the x and y axes.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + coord_flip()\n\n\n\n\n\nUse scale_x_continuous() for tweaking the x-axis. The same command work for the y-axis also. You can include label() inside the command to manually label the breaks of the axes.\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + scale_x_continuous(breaks = c(0,1000,2000))\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + \n  scale_x_continuous(breaks = c(0,1000,2000),label = c(\"low\", \"medium\", \"high\"))\n\n\n\n\n\nUse scale_y_reverse() to display the y values in the descending order. Same command applies to x-axis also.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + scale_y_reverse()"
  },
  {
    "objectID": "tutorials/data_viz/project2.html#typical-aesthetic-mappings",
    "href": "tutorials/data_viz/project2.html#typical-aesthetic-mappings",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n3 Typical aesthetic mappings",
    "text": "3 Typical aesthetic mappings\n\nTable of aesthetic mappings\n\nAesthetic\nDescription\n\n\n\nx\nX axis position\n\n\ny\nY axis position\n\n\nfill\nFill colour\n\n\ncolor\nColour points, outlines of other geoms\n\n\nsize\nArea or radius of points, thickness of the lines\n\n\nalpha\nTransparency\n\n\nlinetype\nLine dash pattern\n\n\nlabels\nText on a plot or axes\n\n\nshape\nShape\n\n\n\nwe are now familiar with all these different aesthetic mappings."
  },
  {
    "objectID": "tutorials/data_viz/project2.html#summary",
    "href": "tutorials/data_viz/project2.html#summary",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n4 Summary",
    "text": "4 Summary\nIn this tutorial, we learned how to modify aesthetic present for different geoms in ggplot2 Then we learned how to modify labels in a graph and finally, we learned how to modify and change the axes elements. This tutorial is in no way exhaustive of the different ways you can modify a graph as there many more methods which are not discussed here. Instead of trying to include everything, this tutorial tries to be a stepping stone to help students of R to learn the basics of tweaking a graph. Try to practice what is covered here using other datasets available in the r-package Stat2Data."
  },
  {
    "objectID": "tutorials/data_viz/project2.html#references",
    "href": "tutorials/data_viz/project2.html#references",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n5 References",
    "text": "5 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nAnn Cannon, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Allan Rossman and Jeffrey Witmer (2019). Stat2Data: Datasets for Stat2. R package version 2.0.0. https://CRAN.R-project.org/package=Stat2Data\n\nLast updated on\n\n\n[1] \"2022-08-04 19:50:12 IST\""
  },
  {
    "objectID": "tutorials/data_viz/project3.html",
    "href": "tutorials/data_viz/project3.html",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "",
    "text": "In this chapter, we will learn how to change the theme settings of a graph in ggplot2 The theme of a graph consists of non-data components present in your graph. This includes the different labels of the graph, fonts used, colour of axes, the background of the graph etc. By changing the theme we would not be changing or transforming how the data will look in the graph. Instead, we would only change the visual appearances in the graph and by doing so we can make it more aesthetically pleasing. Furthermore, we will see a few popular packages featuring ready to use themes. We will also learn about colour palettes and will see different packages associated with them."
  },
  {
    "objectID": "tutorials/data_viz/project3.html#complete-themes",
    "href": "tutorials/data_viz/project3.html#complete-themes",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n1 Complete themes",
    "text": "1 Complete themes\nThe ggplot2 package features ready to use themes called ‚Äòcomplete themes‚Äô. So before we begin customizing themes, let us plot a few graphs and see how these themes look like. For the plots, I have used the BirdNest dataset from the Stat2Data package in R. The BirdNest dataset contains nest and species characteristics of North American passerines. The data was collected by Amy R. Moore, as a student at Grinnell College in 1999.\n\nGetting the BirdNest dataset and viewing how the dataset is structured.\n\n\ninstall.packages(\"Stat2Data\") # for installing Stat2Data package\ninstall.packages(\"ggplot2\") # for installing ggplot2 package\n\n# load the packages\nlibrary(Stat2Data)\nlibrary(ggplot2)\n\ndata(\"BirdNest\") # loading the BirdNest dataset\nstr(BirdNest) # for viewing structure of the dataset\n\nSo we have plenty of variables to play with. The tabs shown below are named according to the theme used in the plots.\n\n\ntheme_gray()\ntheme_bw()\ntheme_linedraw()\ntheme_light()\ntheme_dark()\ntheme_minimal()\ntheme_classic()\ntheme_void()\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_gray()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_light()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_dark()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_minimal()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_classic()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_void()"
  },
  {
    "objectID": "tutorials/data_viz/project3.html#themes-from-ggthemes-package",
    "href": "tutorials/data_viz/project3.html#themes-from-ggthemes-package",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n2 Themes from ggthemes package",
    "text": "2 Themes from ggthemes package\nIf you want even more pre-built themes then you can try the ggthemes package. This package was developed by Dr.¬†Jeffrey B. Arnold.\n\n# install and load ggthemes package\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\nThe tabs shown below are named after the themes present in ggthemes package.\n\n\ntheme_base()\ntheme_calc()\ntheme_clean()\ntheme_economist()\ntheme_excel()\ntheme_excel_new()\ntheme_few()\ntheme_fivethirtyeight()\ntheme_foundation()\ntheme_gdocs()\ntheme_hc()\ntheme_igray()\ntheme_map()\ntheme_pander()\ntheme_par()\ntheme_solarized()\ntheme_solid()\ntheme_stata()\ntheme_tufte()\ntheme_wsj()\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_base()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_calc()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_clean()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_economist()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_excel()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_excel_new()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_few()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_foundation()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_gdocs()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_hc()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_igray()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_map()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_pander()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_par()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_solarized()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_solid()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_stata()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_tufte()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_wsj()"
  },
  {
    "objectID": "tutorials/data_viz/project3.html#changing-colour-palettes-in-ggplot2",
    "href": "tutorials/data_viz/project3.html#changing-colour-palettes-in-ggplot2",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n3 Changing colour palettes in ggplot2",
    "text": "3 Changing colour palettes in ggplot2\nApart from ready to use themes, there are also ready to use colour palettes which we can use. A colour palette contains a set of pre-defined colours which will be applied to the different geometries present in a graph.\nChoosing a good colour palette is important as it helps us to represent data in a better way and at the same time, it also makes the graph easier to read for people with colour blindness. Let us see a few popular colour palette packages used in R.\n\n3.1 viridis package\nviridis package is a popularly used colour palette in R. It is aesthetically pleasing and well designed to improve readability for colour blind people. The {virids} package was developed by Bob Rudis, Noam Ross and Simon Garnier. There are eight different colour scales present in this package. The name of the tab denotes the colour scale present in this package.\n\nlibrary(viridis)\n\n\n\nviridis\nmagma\nplasma\ninferno\ncividis\nmako\nrocket\nturbo\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"viridis\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"magma\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"plasma\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"inferno\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"cividis\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"mako\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"rocket\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"turbo\")\n\n\n\n\n\n\n\n\n3.2 wesanderson package\nIf you like your colours distinctive and narrative, just like how American film-maker Mr.¬†Wes Anderson would like it, then try the wesanderson package. Relive the The Grand Budapest Hotel moments through your graphs. The wesanderson package was developed by Karthik Ram. There are a total of 19 colour palettes present in this package. We will see a subset of them. All colour scales in this package are available here. The name of the tab denotes the colour scale used. The data used in this plot is the penguin dataset present in the package palmerpenguins.\n\n#install and load wesanderson and palmerpenguins package\ninstall.packages(\"wesanderson\")\ninstall.packages(\"palmerpenguins\")\nlibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\") #load the penguins dataset\n\n\n\nGrandBudapest1\nBottleRocket2\nRushmore1\nRoyal1\nZissou1\nDarjeeling2\nIsleofDogs1\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"GrandBudapest1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"BottleRocket2\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Rushmore1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Royal1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Zissou1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Darjeeling2\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"IsleofDogs1\", n = 3))\n\n\n\n\n\n\n\n\n3.3 ggsci package\nIf you want high-quality colour palettes reflecting scientific journal styles then you can try the ggsci package. The ggsci package was developed by Dr.¬†Nan Xiao and Dr.¬†Miaozhu Li. All colour scales in this package are available in package webpage. The name of the tab denotes the colour scale used.\n\n# load ggsci package\ninstall.packages(\"ggsci\")\nlibrary(ggsci)\n\n\n\nnpg\naaas\nnejm\nlancet\njama\njco\nucscgb\nd3\nlocuszoom\nigv\nuchicago\nstartrek\ntron\nfuturama\nrickandmorty\nsimpsons\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_npg()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_aaas()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_nejm()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_lancet()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_jama()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_jco()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_ucscgb()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_d3(palette = \"category10\")\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_locuszoom()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_igv()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_uchicago()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_startrek()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_tron()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_futurama()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_rickandmorty()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_simpsons()"
  },
  {
    "objectID": "tutorials/data_viz/project3.html#customizing-the-theme",
    "href": "tutorials/data_viz/project3.html#customizing-the-theme",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n4 Customizing the theme()",
    "text": "4 Customizing the theme()\nA ggplot theme is made up of different elements and it‚Äôs functions. For e.g.¬†plot.title() element allows you to modify the title of the graph using the element function element_text(). In this way, we can change the font size, font family, text colour etc. of the title of the plot. So let us begin customising our graph. We will be reusing the BirdNest dataset for the graphs.\n\n4.1 Customizing text elements using element_text()\nAll text elements can be customized using the element function element_text(). The syntax for element_text() is as follows\n\nelement_text(\n  family = NULL, #insert family font name, e.g. \"Times\"\n  face = NULL,  #font face (\"plain\", \"italic\", \"bold\", \"bold.italic\")\n  colour = NULL, #either from colours() or hex code inside \"\"\n  size = NULL, #text size (in pts)\n  hjust = NULL, #horizontal justification values 0 or 1\n  vjust = NULL, #vertical justification values 0 or 1\n  angle = NULL, #angle in degrees\n  lineheight = NULL, #distance between text and axis line\n  color = NULL, #same function as colour\n  margin = NULL,\n  debug = NULL,\n  inherit.blank = FALSE\n)\n\nWe can modify each of these parameters to improve our plots as shown below in ‚Äòbefore‚Äô and ‚Äòafter‚Äô the changes are made.\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(No.eggs, Totcare, colour = Nesttype)) + geom_point() +\n    labs(x= \"Number of eggs\", y= \"Total care time (days)\",\n       title= \"Relationship between number of eggs and total care time\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\",\n       colour = \"Nest type\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(No.eggs, Totcare, colour = Nesttype)) + geom_point() +\n    labs(x= \"Number of eggs\", y= \"Total care time (days)\",\n       title= \"Relationship between number of eggs and total care time\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\",\n       colour = \"Nest type\")\n\n#customizing text elements\np + theme(plot.title=element_text(size = 15,family = \"Comic Sans MS\",colour = \"darkred\",face = \"bold\"),\n          plot.subtitle=element_text(size = 10,family = \"Courier\",colour= \"blue\",face= \"italic\"),\n          plot.caption = element_text(size = 8,family = \"Times\",colour= \"green\",face=\"bold.italic\", hjust=0),\n          axis.text.x= element_text(size = 6,colour = \"magenta\", angle=20),\n          axis.text.y= element_text(size = 6,colour = \"darkblue\", angle=30),\n          axis.title.x = element_text(colour = \"orchid\"),\n          axis.title.y = element_text(colour = \"sienna\"),\n          legend.text = element_text(size = 8,colour = \"darkgreen\"),\n          legend.title = element_text(size = 10,colour = \"lightblue\",face = \"bold\"))\n\n\n\n\n\n\n\n\n4.2 Customizing line elements using element_line()\nLine elements include axes, grid lines, borders of the graph etc. All line elements can be customized using the element function element_line(). The syntax for element_line() is as follows\n\nelement_line(\n  colour = NULL, #either from colours() or hex code inside \"\"\n  size = NULL, #line size in mm units\n  linetype = NULL, # eg: dashed, dotted etc \n  lineend = NULL, #line end style (round, butt, square)\n  color = NULL, #same function as colour\n  arrow = NULL, #arrow specification\n  inherit.blank = FALSE\n)\n\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\n\n#customizing line elements\np + theme(panel.grid.major = element_line(colour = \"red\", size = 0.8, linetype = \"dashed\"),\n          panel.grid.minor = element_line(colour = \"blue\",linetype = \"twodash\"),\n          axis.line.x = element_line(colour = \"darkred\", arrow = arrow()),\n          axis.line.y = element_line(colour = \"darkblue\"),\n          axis.ticks = element_line(size = 5, colour = \"yellow\"),\n          axis.ticks.length.y=unit(0.5, \"cm\")) #ticks positioned 0.5cm away from y axis\n\n\n\n\n\n\n\n\n4.3 Customizing background elements using element_rect()\nBackground elements include plot, panel and legend backgrounds and their margins. All background elements can be customized using the element function element_rect(). The syntax for element_rect() is as follows\n\nelement_rect(\n  fill = NULL, #fills colour\n  colour = NULL, #colours the border\n  size = NULL, #changes border size in mm units\n  linetype = NULL, #changes border linetype\n  color = NULL,\n  inherit.blank = FALSE\n)\n\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np <- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\n\n#customizing line elements\np + theme(plot.background = element_rect(size = 5, colour = \"red\", fill = \"lightblue\"),\n          panel.background = element_rect(size = 3, colour = \"blue\", fill = \"lightyellow\", linetype = \"dotted\"),\n          legend.key = element_rect(fill = \"lightgreen\"),\n          legend.background = element_rect(fill = \"grey\"),\n          legend.key.size = unit(0.75, \"cm\"))"
  },
  {
    "objectID": "tutorials/data_viz/project3.html#summary",
    "href": "tutorials/data_viz/project3.html#summary",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope you are now able to customize the theme of a graph with ease. In this chapter, we learned about different theme elements and how to customize them. We also saw different packages in R which featured ready to use themes. We learned about colour palettes and got introduced to the popular colour packages available R. With that being said, always make sure that your graphs are colour-blind friendly."
  },
  {
    "objectID": "tutorials/data_viz/project3.html#references",
    "href": "tutorials/data_viz/project3.html#references",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nAnn Cannon, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Allan Rossman and Jeffrey Witmer (2019). Stat2Data: Datasets for Stat2. R package version 2.0.0. https://CRAN.R-project.org/package=Stat2Data\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nJeffrey B. Arnold (2021). ggthemes: Extra Themes, Scales and Geoms for ‚Äòggplot2‚Äô. R package version 4.2.4. https://CRAN.R-project.org/package=ggthemes\nSimon Garnier, Noam Ross, Robert Rudis, Ant√¥nio P. Camargo, Marco Sciaini, and C√©dric Scherer (2021). Rvision - Colorblind-Friendly Color Maps for R. R package version 0.6.2. You can read more here.\nKarthik Ram and Hadley Wickham (2018). wesanderson: A Wes Anderson Palette Generator. R package version 0.3.6. https://CRAN.R-project.org/package=wesanderson\nNan Xiao (2018). ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for ‚Äòggplot2‚Äô. R package version 2.9. https://CRAN.R-project.org/package=ggsci\n\nLast updated on\n\n\n[1] \"2022-05-26 07:01:56 IST\""
  },
  {
    "objectID": "tutorials/data_viz/project4.html",
    "href": "tutorials/data_viz/project4.html",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "",
    "text": "If you are a researcher who wants to have publication-ready plots but does not want to get hassled by the ggplot2 package, then let me introduce you to the ggpubr package. Using this package you can make publication grade plots without spending too much time modifying things. Even if you are a beginner in R programming and does not know how to use theggplot2 package, you will still be able to plot graphs using the ggpubr package because of how easy the syntax is. But having prior knowledge of the ggplot2 package will surely make things easier, and an experienced person will know that any plot which can be plotted using ggpubr can also be plotted using ggplot2. So let us start!\nFirst things first, install the ggpubr package and load it in the library.\n\ninstall.packages(\"ggpubr\")\nlibrary(ggpubr)\n\nLet us see what all plots can be plotted."
  },
  {
    "objectID": "tutorials/data_viz/project4.html#plots-in-ggpubr-package",
    "href": "tutorials/data_viz/project4.html#plots-in-ggpubr-package",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n2 Plots in ggpubr package",
    "text": "2 Plots in ggpubr package\n\n2.1 Balloon plot\nThe balloon plot is similar to bar plots as it is used to represent a large categorical dataset. The size and colour of the dot can be attributed to different values in the dataset.\n\nlibrary(viridis)\nlibrary(ggpubr)\n\nggballoonplot(mtcars, fill = \"value\") + \n  scale_fill_viridis(option = \"turbo\")\n\n\n\n\n\n2.2 Bar plot\nA simple bar graph which is used for representing categorical data. By using the add function inside the main plot function, you can easily display summary statistics like mean, median etc. and various types of errors like standard error, standard deviation and various others. You can view the whole list of features here.\n\n# install.packages(\"palmerpenguins\")\nlibrary(ggpubr)\nlibrary(palmerpenguins)\n\nggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\n\n\n\n\n2.3 Box plot\nStandard box plot graph. Like in the previous graph you can specify colour palettes from the scientific journal palettes featured in the ggsci R package.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\")\n\n\n\n\n\n2.4 Violin plot\nA simple violin plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggviolin(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          fill = \"species\",\n          palette = \"npg\",\n          add = \"boxplot\",\n          shape = \"species\")\n\n\n\n\n\n2.5 Density plot\nStandard density plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggdensity(penguins,\n          x = \"body_mass_g\",\n          color = \"species\",\n          rug = TRUE,\n          fill = \"species\",\n          add = \"mean\",\n          title = \"Mean body mass of penguins\",\n          xlab = \"Body mass (g)\",\n          palette = \"lancet\")\n\n\n\n\n\n2.6 Donut chart\nSimilar to a pie diagram. Also please note that you don‚Äôt have to explicitly mention x and y parameters in the command. You can simply just type the column names, the first column name will be shown on the x-axis and the second on the y axis.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(tidyr)\n\npenguins_freq <- penguins %>% drop_na() %>%\n  group_by(species) %>%\n  summarise(frequency = length(species))\n\nlabs <- paste0(penguins_freq$species, \" (\", round((penguins_freq$frequency/sum(penguins_freq$frequency))*100, digits = 0), \"%)\")\n\nggdonutchart(penguins_freq,\n             \"frequency\",\n             label = labs,\n             fill = \"species\",\n             palette = \"ucscgb\",\n             lab.pos = \"in\",\n             title = \"Frequency of penguins\")\n\n\n\n\n\n2.7 Pie chart\nSimple pie chart.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(tidyr)\n\npenguins_freq <- penguins %>% drop_na() %>%\n  group_by(species) %>%\n  summarise(frequency = length(species))\n\nlabs <- paste0(penguins_freq$species, \" (\", round((penguins_freq$frequency/sum(penguins_freq$frequency))*100, digits = 0), \"%)\")\n\nggpie(penguins_freq,\n             \"frequency\",\n             label = labs,\n             fill = \"species\",\n             palette = \"futurama\",\n             lab.pos = \"in\",\n             title = \"Frequency of penguins\")\n\n\n\n\n\n2.8 Dot chart\nThis is an upgrade from bar charts where the data is displayed with minimum clutter in the form of dots. This allows the readers to not get bothered about things like the slope of a line in case of line plots, or width of bars in case of bar charts or any other confusing aesthetics of a plot. You can read more about this graph here. It is also called ‚ÄúCleveland dot plots‚Äù named after the founder of this plot.\n\nlibrary(tibble)\nlibrary(ggpubr)\nlibrary(tidyr)\n\nmtcars %>% rownames_to_column(var = \"car_names\") %>% \n  mutate(cyl = as.factor(cyl)) %>%\n  ggdotchart(\"car_names\",\n             \"mpg\",\n             color = \"cyl\",\n             palette = \"aaas\",\n             sorting = \"ascending\",\n             rotate = TRUE,\n             y.text.col = TRUE,\n             dot.size = 2,\n             ylab = \"Miles per gallon of fuel\",\n             title = \"Mileage of different cars\",\n             ggtheme = theme_pubr()) + theme_cleveland() \n\n\n\n\n\n2.9 Dot plot\nSimple dot plot. Similar to a box plot. You can also overlay a box plot or a violin plot over the dot plot using the add function inside the main function.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggdotplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          fill = \"species\",\n          add = \"mean_sd\",\n          palette = \"locuszoom\")\n\n\n\n\n\n2.10 Histogram plot\nThe same function as that of a density plot but the data is represented in bars.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ngghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\n\n\n\n\n2.11 Line plot\nA simple line plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggline(penguins,\n      x = \"body_mass_g\",\n      y = \"bill_depth_mm\",\n      linetype = \"species\",\n      shape = \"species\",\n      color = \"species\",\n      title = \"Body mass vs Bill depth\",\n      xlab = \"Body mass (g)\",\n      ylab = \"Bill depth (mm)\",\n      palette = \"startrek\")\n\n\n\n\n\n2.12 Plotting paired data\nThis is essentially a box plot but for paired data. Widely used to represent treatment groups showing before and after results of the same sample. We will be using the Anorexia dataset from the {PairedData} package in R. It features weights of girls before and after treatment for Anorexia.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(ggpubr)\ndata(\"Anorexia\")\n\nAnorexia %>% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\")\n\n\n\n\n\n2.13 Quantile-Quantile plot\nQuantile-Quantile plot or QQ plot is useful in assessing the distribution of a data. A data having normal distribution will be shown as a straight line of the formula ‚Äòy=x‚Äô in the QQ plot. Points outside the confidence interval are outliers in the data.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% ggqqplot(\"body_mass_g\",\n                      color = \"species\",\n                      palette = \"aaas\",\n                      title = \"Quantile-Quantile plot\")\n\n\n\n\n\n2.14 Scatter plot\nA simple scatter plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% filter(species == \"Chinstrap\") %>%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            add = \"reg.line\",\n            add.params = list(color = \"darkred\", fill = \"yellow\"),\n            cor.coef = TRUE,\n            cor.method = \"pearson\",\n            conf.int = TRUE,\n            title = \"Body mass distribution of Chinstrap penguins\",\n            subtitle = \"Correlation method used was Pearson\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill lenght (mm)\")\n\n\n\n\nYou can also use scatter plot for data having different categories. Using ellipse=TRUE you can group data to its category.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5,\n            palette = \"d3\",\n            ellipse = TRUE, #adds an ellipse to group data of different category\n            title = \"Body mass vs Bill length\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\nYou can also label points in the scatter plot using the label function.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nmtcars %>% rownames_to_column(var = \"car_names\") %>% \n  mutate(cyl = as.factor(cyl)) %>%\n  ggscatter(\"wt\",\n             \"mpg\",\n             color = \"cyl\",\n             palette = \"nejm\",\n             xlab = \"Weight (1000 lbs)\",\n             ylab = \"Miles per gallon of fuel\",\n             title = \"Mileage vs Weight of different cars\",\n             label = \"car_names\",\n             repel = TRUE,\n             ggtheme = theme_pubr()) + theme_cleveland() \n\n\n\n\n\n2.15 Scatter plot with marginal histograms\nThis is plot is a combination of scatter plot and histograms.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.params = list(fill = \"species\", color = \"black\", size = 0.2),\n            title = \"Body mass distribution of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\nYou can also choose to show box plots.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_depth_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.plot = \"boxplot\",\n            title = \"Body mass vs Bill depth\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill depth (mm)\",\n            ggtheme = theme_bw())"
  },
  {
    "objectID": "tutorials/data_viz/project4.html#other-functions-in-ggpubr-package",
    "href": "tutorials/data_viz/project4.html#other-functions-in-ggpubr-package",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n3 Other functions in ggpubr package",
    "text": "3 Other functions in ggpubr package\n\n3.1 Statistical tests\nYou can do various statistical tests using the functions in the ggpubr package. We will be using the Anorexia dataset in the {PairedData} package in R. In the code given below, we are doing a Wilcoxon test to compare the mean weights of girls before treatment to the mean weights of girls post-treatment. Since the data is paired we will indicate it by the paired = TRUE function. A word of caution! Before starting to do statistical tests please ensure whether you can fulfil conditions for using parametric tests or not using or data. You can check whether your data is normally distributed using a QQ plot or by using any normality tests.\nPS: I use knitr::kable() just for illustrative purpose only. You can run the command inside the kable() argument and you will be fine.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndata(\"Anorexia\")\n\n# tidying the data\nAnorexia_new <- Anorexia %>% \n  pivot_longer(c(Prior, Post), names_to = \"condition\", values_to = \"weight\")\nknitr::kable(compare_means(weight ~ condition, Anorexia_new, paired = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\np\np.adj\np.format\np.signif\nmethod\n\n\nweight\nPrior\nPost\n0.0008392\n0.00084\n0.00084\n***\nWilcoxon\n\n\n\n\nYou can also do parametric tests like ANOVA and its non-parametric version; the Kruskal-Wallis test, which can be followed by multiple pairwise comparisons.\n\nknitr::kable(compare_means(body_mass_g ~ species, penguins, method = \"anova\"))\n\n\n\n.y.\np\np.adj\np.format\np.signif\nmethod\n\n\nbody_mass_g\n0\n0\n<2e-16\n****\nAnova\n\n\n\nknitr::kable(compare_means(body_mass_g ~ species, penguins, method = \"kruskal.test\"))\n\n\n\n.y.\np\np.adj\np.format\np.signif\nmethod\n\n\nbody_mass_g\n0\n0\n<2e-16\n****\nKruskal-Wallis\n\n\n\n\nNow doing pairwise comparisons\n\n# multiple pairwise comparisons\n# when there is more than two levels, the function automatically does pairwise comparisons\nknitr::kable(compare_means(body_mass_g ~ species, penguins))\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\np\np.adj\np.format\np.signif\nmethod\n\n\n\nbody_mass_g\nAdelie\nChinstrap\n0.4854773\n0.49\n0.49\nns\nWilcoxon\n\n\nbody_mass_g\nAdelie\nGentoo\n0.0000000\n0.00\n<2e-16\n****\nWilcoxon\n\n\nbody_mass_g\nChinstrap\nGentoo\n0.0000000\n0.00\n<2e-16\n****\nWilcoxon\n\n\n\n\n\n\n3.2 Descriptive statistics by groups\nUsing the function desc_statby() we can get the summary statistics of a dataset in the form of a data frame. Similar to the summary() function in base R.\n\nknitr::kable(desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nlength\nmin\nmax\nmedian\nmean\niqr\nmad\nsd\nse\nci\nrange\ncv\nvar\n\n\n\nAdelie\n151\n2850\n4775\n3700\n3700.662\n650.0\n444.780\n458.5661\n37.31758\n73.73601\n1925\n0.1239146\n210282.9\n\n\nChinstrap\n68\n2700\n4800\n3700\n3733.088\n462.5\n370.650\n384.3351\n46.60747\n93.02891\n2100\n0.1029537\n147713.5\n\n\nGentoo\n123\n3950\n6300\n5000\n5076.016\n800.0\n555.975\n504.1162\n45.45463\n89.98198\n2350\n0.0993134\n254133.2\n\n\n\n\n\nYou can also show the data as a table using the ggtexttable() function.\n\nsummary <- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short <- summary %>% dplyr::select(species, mean, median, se, sd)\nsummary_tbl <- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\nsummary_tbl\n\n\n\n\n\n3.3 Showing p-values and statistical results within plots\nUsing various functions you can show statistical outputs within the plots.\nUsing Wilcoxon test for paired data.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(ggpubr)\n\ndata(\"Anorexia\")\nAnorexia %>% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\") + stat_compare_means(paired = TRUE)\n\n\n\n\nUsing t-test\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% filter(species == \"Adelie\" & island == c(\"Biscoe\",\"Torgersen\")) %>%\nggboxplot(x = \"island\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Island\",\n          ylab = \"Bill depth (mm)\",\n          color = \"island\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"island\") + stat_compare_means(method = \"t.test\")\n\n\n\n\nUsing ANOVA test and t-test as post hoc test. For pairwise comparison, we have to manually list out the pairwise comparisons that we want.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\n# listing out pairwise comparisons \ncompare <- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\n\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(method = \"anova\", label.y = 25) + #anova test\n  stat_compare_means(comparisons = compare, method = \"t.test\") # post hoc test using t-test\n\n\n\n\nUsing Kruskal-Wallis test and Wilcoxon test as post hoc test.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ncompare <- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(label.y = 25) + #anova test\n  stat_compare_means(comparisons = compare) # post hoc test using t-test\n\n\n\n\nYou can also choose to show only asterisks as significance levels\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ncompare <- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(label.y = 25) +\n  stat_compare_means(comparisons = compare, label = \"p.signif\")\n\n\n\n\nFor illustrative purposes, I have used box plots for showing p-values and statistical test results, but you can do the same with most of the other types of graphs shown in this chapter.\n\n3.4 Faceting plots into grids\nYou can also facet different plots into grids using the function facet.by.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>% drop_na() %>%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5,\n            palette = \"d3\",\n            facet.by = c(\"island\", \"sex\"), # faceting graphs via island and sex categories\n            title = \"Body mass vs Bill length\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\n\n3.5 Adding paragraph\nYou can also add a paragraph beneath the plot of your interest using the ggparagraph() and ggarrange() functions.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\ndata(\"Anorexia\")\nlibrary(ggpubr)\n\ntext <- paste(\"The above dataset shows the weight (in lbs) of 17 girls before\",\n              \"and after they got treatment for anorexia.\", sep = \" \")\n\ntext_plot <- ggparagraph(text, face = \"bold\", size = 12)\n\nplot <- Anorexia %>% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\")\n\nggarrange(plot, text_plot,\n         ncol = 1, nrow = 2,\n         heights = c(1, 0.3))\n\n\n\n\n\n3.6 Having plots placed adjacent to each other\nYou can use the ggarrange() function to place different plots together.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nbar_plot <- ggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\nhistogram <- gghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\nsummary <- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short <- summary %>% dplyr::select(species, mean, median, se, sd)\nsummary_tbl <- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\n\n# arranging plots together\nggarrange(bar_plot, histogram,\n         ncol = 2, nrow = 2, labels = c(\"A\", \"B\"),\n         heights = c(1, 0.3))\n\n\n\n\nIf you are arranging three graphs it is better to use the grid.arrange() function from the {gridExtra} package in R.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nbar_plot <- ggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\nhistogram <- gghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\nsummary <- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short <- summary %>% dplyr::select(species, mean, median, se, sd)\nsummary_tbl <- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\n\n# arranging three plots together\nlayout_matrix <- matrix(c(1, 1, 2, 2, 4, 3, 3, 4), nrow = 2, byrow = TRUE)\nlibrary(gridExtra)\ngrid.arrange(bar_plot, histogram, summary_tbl, layout_matrix = layout_matrix)"
  },
  {
    "objectID": "tutorials/data_viz/project4.html#saving-your-plot",
    "href": "tutorials/data_viz/project4.html#saving-your-plot",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n4 Saving your plot",
    "text": "4 Saving your plot\nUsing the function ggexport() you can save your plot. Tweak width and height accordingly and also change the resolution to fit your needs.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %>%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.params = list(fill = \"species\", color = \"black\", size = 0.2),\n            title = \"Body mass distribution of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\") %>%\n  ggexport(filename = \"my_plot.png\", width = 800, height = 600, res = 150)"
  },
  {
    "objectID": "tutorials/data_viz/project4.html#summary",
    "href": "tutorials/data_viz/project4.html#summary",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n5 Summary",
    "text": "5 Summary\nIn this chapter we learned how to plot publication standard graphs using the ggpubr package in R. Even with little to no experience in using the ggplot2 package in R, one can plot graphs with ease using the ggpubr package. To quickly recap, from this chapter we saw;\n\nHow to plot around 15 different types of graphs\nHow to facet plots\nHow to do basic statistical tests and visualize them within graphs\nHow to add paragraph text under the graphs\nHow to group different graphs into one single file\n\nI hope this chapter was useful to you. Check out the other chapter for more beginner content."
  },
  {
    "objectID": "tutorials/data_viz/project4.html#references",
    "href": "tutorials/data_viz/project4.html#references",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n6 References",
    "text": "6 References\n\nAlboukadel Kassambara (2020). ggpubr: ‚Äòggplot2‚Äô Based Publication Ready Plots. R package version 0.4.0. https://CRAN.R-project.org/package=ggpubr\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nStephane Champely (2018). PairedData: Paired Data Analysis. R package version 1.1.1. https://CRAN.R-project.org/package=PairedData\nWilliam S. Cleveland & Robert McGill (1984) Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods, Journal of the American Statistical Association, 79:387, 531-554, DOI: 10.1080/01621459.1984.10478080\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nSimon Garnier, Noam Ross, Robert Rudis, Ant√¥nio P. Camargo, Marco Sciaini, and C√©dric Scherer (2021). Rvision - Colorblind-Friendly Color Maps for R. R package version 0.6.2.\n\nLast updated on\n\n\n[1] \"2022-08-04 19:53:06 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project1.html",
    "href": "tutorials/stat_basic/project1.html",
    "title": "Chapter 1: Introduction to statistics with R",
    "section": "",
    "text": "Image by Isaac Smith on Unsplash"
  },
  {
    "objectID": "tutorials/stat_basic/project1.html#introduction-to-statistics",
    "href": "tutorials/stat_basic/project1.html#introduction-to-statistics",
    "title": "Chapter 1: Introduction to statistics with R",
    "section": "\n1 Introduction to statistics",
    "text": "1 Introduction to statistics\nThis article, along with the related articles to this topic, is a culmination of knowledge that I gained during my time in IISER-TVM, when I took the BIO4203 course named Biological Data Analysis offered in 2019. The course was taught to me by Dr.¬†Ravi Maruthachalam who is an excellent teacher and an inspiring researcher. The course helped me immensely by allowing me to understand the core basics of statistics and more importantly, remove any misconceptions about it. I hope I can deliver whatever I learned effectively through the series of articles that will be available regarding this topic, starting with this one.\n\n\n\n\n\n\nWord of caution\n\n\n\nIf you notice any typos or errors in any of the pages, you can request an edit of the page which will be shown at end of the table of contents on the right side. Thank you.\n\n\nThe topics I want to cover include;\n\nIntroduction to statistics\nDistribution and variance\nStandard error and standard deviation\np-value and level of significance\nType I error and type II error\nConfidence interval and effect size\nParametric test and assumptions\nNon-parametric test and assumptions\nTests for multiple groups of data\n\nWith that being said, the articles will be focused on enriching the intuition behind the concepts rather than the pure math behind them. In addition, I will also be explaining how one can do the same tests using R. The articles will be supplemented by published research papers related to the topic."
  },
  {
    "objectID": "tutorials/stat_basic/project1.html#descriptive-statistics",
    "href": "tutorials/stat_basic/project1.html#descriptive-statistics",
    "title": "Chapter 1: Introduction to statistics with R",
    "section": "\n2 Descriptive statistics",
    "text": "2 Descriptive statistics\nApplied statistics is broadly categorized into descriptive statistics and inferential statistics. Descriptive statistics deals with the data at hand. It uses different ways to summarise and inspect the data in hand and does not go beyond the data. If the data is for a population then the characteristics used are called parameters. Some of those descriptive parameters include the mean, median, and mode which are used to elucidate the central tendency of the data. For measuring variability, there is a range, quartiles, and standard deviation which we will see later in greater detail."
  },
  {
    "objectID": "tutorials/stat_basic/project1.html#inferential-statistics",
    "href": "tutorials/stat_basic/project1.html#inferential-statistics",
    "title": "Chapter 1: Introduction to statistics with R",
    "section": "\n3 Inferential statistics",
    "text": "3 Inferential statistics\nInferential statistics are used to understand patterns of a population using sample data from it. Since collecting data for a population is tedious, we take a sample from that population, which acts as a true representative of the population. Through sampling methods and hypothesis testing, patterns seen within the sample can be extrapolated to the whole population. The characteristics used for sample data are called statistics. Some of the inferential statistics include sample mean, standard deviation, etc.\nLast updated on\n\n\n[1] \"2022-08-04 01:35:17 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project2.html",
    "href": "tutorials/stat_basic/project2.html",
    "title": "Chapter 2: Distribution and variance",
    "section": "",
    "text": "Normality is vulnerable against outliers"
  },
  {
    "objectID": "tutorials/stat_basic/project2.html#distribution-and-variance",
    "href": "tutorials/stat_basic/project2.html#distribution-and-variance",
    "title": "Chapter 2: Distribution and variance",
    "section": "\n1 Distribution and variance",
    "text": "1 Distribution and variance\nLet us look at the penguins dataset from the palmerpenguins package in R. We will plot the distribution curve for ‚Äúbody mass‚Äù of the ‚ÄúChinstrap‚Äù species of penguins.\n\n#install.packages(\"palmerpenguins\")\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %>% filter(species == \"Chinstrap\") %>% ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density(fill = \"darkred\") + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Chinstrap\") %>% nrow())) +\n  theme_bw()\n\n\n\n\nAs you can see, the data distribution closely resembles a ‚Äúbell-shaped‚Äù curve. This kind of distribution is known as a normal distribution or a Gaussian distribution. On closer look, you can also see that the area under the curve is almost symmetrical to both sides and there is only a single peak present. We can also learn about the variance of the data by looking at the width of the curve. A wider curve means variance is higher and a narrower curve means variance in the data is smaller. Also for a normal distribution, there are no visible outliers present in the plot. Outliers are those data points which fall out of the normal trend in the data. The peak represents the mean of the data.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Create function to get mode\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\npen_avg <- penguins %>% filter(species == \"Chinstrap\") %>% summarise(mean = mean(body_mass_g),\n                                                          median = median(body_mass_g),\n                                                          mode = getmode(body_mass_g))\n\npenguins %>% filter(species == \"Chinstrap\") %>% \n  ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density(fill = \"grey\") + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Chinstrap\") %>% nrow())) +\n  geom_vline(aes(xintercept = pen_avg$mean, colour = \"red\")) +\n  geom_text(aes(x=pen_avg$mean, label=\"mean\", y=4e-04), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = pen_avg$median, colour = \"blue\")) +\n  geom_text(aes(x=pen_avg$median, label=\"median\", y=4e-04), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = pen_avg$mode, colour = \"green\")) +\n  geom_text(aes(x=pen_avg$mode, label=\"mode\", y=4e-04), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\nFor a normal distribution the mean, median and mode are all the same value and are in the middle of the curve. From the plot above you can see that all the averages are almost the same. In general all three modes of averages tries to represent the centre of the dataset for any distribution. With that said some mode of average is preferred over others in terms of the nature of the distribution, we will see what it means later on.\nNow let us look the same distribution but for ‚ÄúGentoo‚Äù species of penguins.\n\n#install.packages(\"palmerpenguins\")\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %>% filter(species == \"Gentoo\") %>% ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density(fill = \"darkblue\") + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Gentoo\") %>% nrow())) +\n  theme_bw()\n\n\n\n\nUnlike the previous curve this one has two peaks. This kind of curve is an example for a ‚Äúbimodal distribution‚Äù. Now what would have caused these two peaks to form in this one but not in the first one? So we can guess that there is some hidden variable that is affecting the body mass distribution of Gentoo penguins. What about the ‚Äòsex‚Äô of the penguins? Will that have any effect? Let us try plotting the same plot but in terms of the sex of the penguins.\n\n#install.packages(\"palmerpenguins\")\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %>% filter(species == \"Gentoo\") %>% drop_na() %>%\n  ggplot(aes(x = body_mass_g, fill = sex)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density() + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Gentoo\") %>% nrow())) +\n  theme_bw()\n\n\n\n\nHa! We were spot on! So it looks like male Gentoos are generally larger in comparison to females.\nNow let us consider the plot given below.\n\n#install.packages(\"palmerpenguins\")\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\ndiamonds %>% filter(cut == \"Fair\") %>%\n  ggplot(aes(x = carat)) + \n  xlab(\"carats\") + ylab(\"Density\") + ggtitle(\"Diamond carat distribution\") + \n  geom_density(fill = \"gold\") + \n  labs(subtitle = paste0(\"N=\",diamonds %>% filter(cut == \"Fair\") %>% nrow())) +\n  theme_bw()\n\n\n\n\nThis is an example of a ‚Äúskewed distribution‚Äù. Moreover, this is a ‚Äúright-skewed distribution‚Äù, why is it called right-skewed? Look at the same plot below.\n\n#install.packages(\"palmerpenguins\")\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Create function to get mode\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\ndiamonds %>% filter(cut == \"Fair\") %>%\n  ggplot(aes(x = carat)) + \n  xlab(\"carats\") + ylab(\"Density\") + ggtitle(\"Diamond carat distribution\") + \n  geom_density(fill = \"gold\") + \n  labs(subtitle = paste0(\"N=\",diamonds %>% filter(cut == \"Fair\") %>% nrow())) +\n  geom_vline(aes(xintercept = mean(diamonds$carat), colour = \"red\")) +\n  geom_text(aes(x=mean(diamonds$carat), label=\"mean\", y=1), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = median(diamonds$carat), colour = \"blue\")) +\n  geom_text(aes(x=median(diamonds$carat), label=\"median\", y=1), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = getmode(diamonds$carat), colour = \"green\")) +\n  geom_text(aes(x=getmode(diamonds$carat), label=\"mode\", y=1), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\nI have plotted the mean, median, and mode values for the plot and you can see that the mean value is more than the mode value. This is why the curve is indicated as right-skewed, as the mean is skewed towards the right side of the mode value. If it was the other way around then the curve will be called ‚Äúleft-skewed‚Äù. Also in this case, if we had used the mean value to represent the dataset it would have been a poor choice and you see that the median is a better choice compared to the mean. Therefore when the dataset is skewed, the median is the better choice to represent the dataset. The mode will be best suited for datasets with a lot of repeated values.\n\n\nFig1: Types of distributions. Credits: biologyforlife.com"
  },
  {
    "objectID": "tutorials/stat_basic/project2.html#variance",
    "href": "tutorials/stat_basic/project2.html#variance",
    "title": "Chapter 2: Distribution and variance",
    "section": "\n2 Variance",
    "text": "2 Variance\nThe variance of data is defined as the average squared difference from the mean of the data. It tells us how far each of our data points is from the mean value.\nThe formula for finding the variance is as follows;\ns^{2} = \\frac{\\sum (x_{i} - \\bar{x})^{2}}{n - 1}\nWhere s^{2} is sample variance, x_{i} is your data point, \\bar{x} is mean and n is the sample size.\nFor a sufficiently large value of n, the equation for variance will become;\n\\sigma^{2} = \\frac{\\sum (x_{i} - \\bar{x})^{2}}{n} where \\sigma^{2} is called population variance.\nHere \\sigma is called the standard deviation of the data which we will see in the next chapter.\nWhen we compare both equations, you can see that the sample variance has n-1 in the denominator when compared to just n in population variance. The n-1 used in the formula is called the Bessel's correction which aims to improve the accuracy in determining the sample variance as close as the population variance."
  },
  {
    "objectID": "tutorials/stat_basic/project2.html#references",
    "href": "tutorials/stat_basic/project2.html#references",
    "title": "Chapter 2: Distribution and variance",
    "section": "\n3 References",
    "text": "3 References\n\nNotes on skewed data distributions.\nMore notes on variance.\nMore notes on Basel correction. 1 and 2\n\nLast updated on\n\n\n[1] \"2022-08-04 22:03:55 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project3.html",
    "href": "tutorials/stat_basic/project3.html",
    "title": "Chapter 3: Standard error and standard deviation",
    "section": "",
    "text": "Image by Michal Matlon on Unsplash"
  },
  {
    "objectID": "tutorials/stat_basic/project3.html#standard-error",
    "href": "tutorials/stat_basic/project3.html#standard-error",
    "title": "Chapter 3: Standard error and standard deviation",
    "section": "\n1 Standard error",
    "text": "1 Standard error\nThe standard error or more accurately called the standard error of mean is a measure of spread that calculates how far the sample mean is different from the population mean. It calculates the error in a sample statistic compared to the corresponding population parameter.\nThe formula to calculate the standard error of mean is;\n\\sigma_{\\bar{X}} = \\frac{s}{\\sqrt{N}}\nwhere \\sigma_{\\bar{X}} is standard error of mean, s is sample standard deviation and N is sample size.\nAs you can see from the formula, if we increase our sampling size, the standard error of the mean decreases, and therefore our sample mean will closely resemble the population mean."
  },
  {
    "objectID": "tutorials/stat_basic/project3.html#standard-deviation",
    "href": "tutorials/stat_basic/project3.html#standard-deviation",
    "title": "Chapter 3: Standard error and standard deviation",
    "section": "\n2 Standard deviation",
    "text": "2 Standard deviation\nThe equivalent of standard error, but for a sample in a population. Simply put, it tells us how much spread is there within a data, more precisely speaking, how much each of our data points are spread around the mean.\nThe formula to calculate the standard deviation of the mean is;\ns = \\sqrt{s^{2}} = \\sqrt{\\frac{\\sum (x_{i} - \\bar{x})^{2}}{N - 1}} where s is standard deviation, \\sqrt{s^{2}} is variance, x_{i} is your data point, \\bar{x} is sample mean and N is sample size.\nStandard deviation is generally constant across the population and is larger than the standard error of the mean. Because of this sometimes researchers cheat out by using standard error of the mean in the results, rather than using standard deviation, which leads to a false idea that their measurements are accurate."
  },
  {
    "objectID": "tutorials/stat_basic/project3.html#standard-normal-distribution",
    "href": "tutorials/stat_basic/project3.html#standard-normal-distribution",
    "title": "Chapter 3: Standard error and standard deviation",
    "section": "\n3 Standard normal distribution",
    "text": "3 Standard normal distribution\nA normal distribution with mean(\\mu) = 1 and variance(\\sigma^{2}) = 1 is called as a standard normal distribution or a z-distribution. With the help of standard deviation, we can split the area under the curve of standard normal distribution into three parts. This partitioning of the area is known as the 68‚Äì95‚Äì99.7 rule. What it means is that;\n\n68% of the data will lie within ¬±\\sigma from \\mu\n\n95% of the data will lie within ¬±2\\sigma from \\mu\n\n99.7% of the data will lie within ¬±3\\sigma from \\mu\n\n\n\n\nFig1: 68‚Äì95‚Äì99.7 rule. Credits: www.nagwa.com\n\n\nAs previously mentioned, the width of the curve tells us about the variance/standard deviation of the data.\n\n\nFig2: Area under the curve for different standard deviations. Credits: www.varsitytutors.com"
  },
  {
    "objectID": "tutorials/stat_basic/project3.html#references",
    "href": "tutorials/stat_basic/project3.html#references",
    "title": "Chapter 3: Standard error and standard deviation",
    "section": "\n4 References",
    "text": "4 References\n\nLivingston EH. The mean and standard deviation: what does it all mean? J Surg Res. 2004 Jun 15;119(2):117-23. doi: 10.1016/j.jss.2004.02.008. PMID: 15145692. Source\n\n\nLast updated on\n\n\n[1] \"2022-08-04 01:35:42 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project4.html",
    "href": "tutorials/stat_basic/project4.html",
    "title": "Chapter 4: p-value and level of significance",
    "section": "",
    "text": "Comic from XKCD"
  },
  {
    "objectID": "tutorials/stat_basic/project4.html#definiton-of-p-value",
    "href": "tutorials/stat_basic/project4.html#definiton-of-p-value",
    "title": "Chapter 4: p-value and level of significance",
    "section": "\n1 Definiton of p-value",
    "text": "1 Definiton of p-value\n‚ÄúWhat is p-value?‚Äù This was the first question that came from my instructor while I was taking the course on statistics. The question was very simple, we just have to give the definition of p-value. Yet, I was completely dumbfounded. I have seen it in almost all the scientific papers that I have read and I know that it is a result obtained after hypothesis testing. And if the value is less 0.05 then the we accept the alternate hypothesis and reject the null hypothesis. I thought that was it, until I found out about the real deal about p-value.\nA study published on 2007 surveyed medical researchers and asked them ‚Äúwhat is interpretation on p > 0.05 ?‚Äù and thankfully they had the choice to select the answer from four choices. The choices were;\n\nThe chances are greater than 1 in 20 that a difference would be found again if the study were repeated.\nThe probability is less than 1 in 20 that a difference this large could occur by chance alone.\nThe probability is greater than 1 in 20 that a difference this large could occur by chance alone.\nThe chance is 95% that the study is correct.\n\nWhat do you think is the answer if you could answer now?\nAnyway, around 60% of the participants of the survey selected choice 3 and 100% of them were wrong.\nYes, all of them answered wrong! which means there was no right answer among the choices to begin, yes betrayal indeed! The closest correct answer would be choice 3, but the correct definition for p-value is as follows.\n\n\n\n\n\n\nDefinition of p-value\n\n\n\nThe probability to get the observed results, plus more extreme results, if the null hypothesis were true"
  },
  {
    "objectID": "tutorials/stat_basic/project4.html#misconceptions-about-p-value",
    "href": "tutorials/stat_basic/project4.html#misconceptions-about-p-value",
    "title": "Chapter 4: p-value and level of significance",
    "section": "\n2 Misconceptions about p-value",
    "text": "2 Misconceptions about p-value\nGoodman S. A dirty dozen: twelve p-value misconceptions. Semin Hematol. 2008 Jul;45(3):135-40. doi: 10.1053/j.seminhematol.2008.04.003. Erratum in: Semin Hematol. 2011 Oct;48(4):302. PMID: 18582619. Link\nNow even after knowing this definition, sometimes it might not be enough. So let us try to see some of the common misconceptions regarding p-value which is beautifully written in the paper ‚ÄòA dirty dozen: twelve p-value misconceptions.‚Äô by Goodman S. I will be summarising the content of the paper.\n\n\n\n\n\n\nMisconception 1\n\n\n\n\nIf p = 0.05, the null hypothesis has only a 5% chance of being true.\n\nIf p = 0.05, by definition it means that there is a 5% chance that we get the observed result plus more extreme results, given that the null hypothesis is true. Therefore by definition, we never say anything about the chance of the null hypothesis being true.\n\n\n\n\n\n\n\n\nMisconception 2\n\n\n\n\nA non significant difference (eg, p > 0.05) means there is no difference between groups.\n\np-value as we now know is a probability value. Therefore we cannot conclude anything with absolute certainty like mentioned above saying that ‚Äòthere is no difference between the groups‚Äô. The takeaway message is that the result that, ‚Äúthere is no difference between the groups‚Äù is statistically significant within the level of significance we have chosen and falls within the range of results we should expect.\n\n\n\n\n\n\n\n\nMisconception 3\n\n\n\n\nStatistical significance means biological significance.\n\nThis does not need to be always the case. A drug effect experimented with within a large sample size can yield significant p-values but still be not good enough to be used for a cure. This is because the p-value does not tell you anything about the magnitude of the effect. Effect size, which is independent of sample size is a better measure to determine biological significance. We will see effect sizes in detail later.\n\n\n\n\n\n\n\n\nMisconception 4\n\n\n\n\nStudies with p values on opposite sides of 0.05 are conflicting.\n\nA better way to see if two results are conflicting is by comparing the confidence intervals of the data. If they do not overlap and have little overlap, then they can be considered conflicting with each other. Two studies looking at the same result can have conflicting p-values because of the differences in sample sizes.\n\n\n\n\n\n\n\n\nMisconception 5\n\n\n\n\nStudies with the same p value provide the same evidence against the null hypothesis.\n\nTwo different results can be statistically significant and have same the p-value. But it is not necessary that they yield the same evidence against the null hypothesis. Consider two studies, where each of them test a novel drug against a disease. The null hypothesis for these studies would be that the cure rate of the drugs would be zero. Let us imagine that the first study had a cure rate of 5% and the second study have 15% cure rate and both studies have the same p-value of 0.05. Now here, both studies are able to reject the null hypothesis but, they don‚Äôt show the same evidence against the null hypothesis, as the second study shows higher cure rate compared to first. As mentioned before the p-value does not convey any information about the magnitude of effect in question.\n\n\n\n\n\n\n\n\nMisconception 6\n\n\n\n\np = 0.05 means that we have observed data that would occur only 5% of the time under the null hypothesis.\n\nFrom the definition of the p-value, we know that this statement is incomplete. If \\alpha = 0.05 and p = 0.05, then what it means is that this is the highest probability value at which the observed results or more extreme results occur, given that the null hypothesis is true. If the above statement also mentioned about the occurrence of more extreme results than the observed data then it would be a correct statement.\n\n\n\n\n\n\n\n\nMisconception 7\n\n\n\n\np = 0.05 and p ‚â§ 0.05 mean the same thing.\n\nNow, this would be the most confusing one out of all the misconceptions that are described in this post. In the paper that I referenced for this article, they don‚Äôt give any explanation other than saying to explain this difference one would need to use the ‚ÄúBayesian evidence metric‚Äù. So after reading about it this is my understanding of it.\nWe can calculate the ‚Äúlikelihood‚Äù for a hypothesis(H) for a given data(x) which is expressed as L(H|x). The expression is the same as that of a conditional probability, but since we are dealing with hypotheses we use the word likelihood and if it was results, we associate it with probability. Also, L(H|x) is directly proportional to P(x|H), where P(x|H) is the probability of obtaining data(x) given hypothesis(H) is true. Now for null(Ho) and alternate hypothesis (Ha), we have;\n\nL(Ho|x) \\propto P(x|Ho)\nL(Ha|x) \\propto P(x|Ha)\n\nNow likelihood ratio (LR) is calculated as;\nLR = L(Ha|x) / L(Ho|x) = P(x|Ha) / P(x|Ho)\nHere P(x|H) can be probabilities or probability densities.\nNow let us try to understand the difference between p = 0.05 and p ‚â§ 0.05.\n\n\nFig1: Probability distribution curves for Ha and Ho. Credits: Goodman, S.N. (1993)\n\n\nConsider the figure given above.\nFor p = 0.05, we will have;\nLR = P(x|Ha) / P(x|Ho) = P(A)/P(o)\nwhere P(A) is the probability at B for curve Ha and P(o) is the probability at A for curve Ho.\nand for p ‚â§ 0.05, we will have;\nLR = P(x|Ha) / P(x|Ho) = Shaded\\;area/Striped\\;area\nwhere the shaded area is the probability density for p ‚â§ 0.05 in curve Ha and the striped area is the probability density for p ‚â§ 0.05 in curve Ho.\nNow for p values between 0.05 to 0.001;\nP(A)/P(o) < Shaded\\;area/Striped\\;area\nwhich means, for p = 0.05 we have less evidence for the alternate hypothesis as opposed to when p ‚â§ 0.05. Therefore p = 0.05 and p ‚â§ 0.05 are different from each other.\n\n\n\n\n\n\n\n\nMisconception 8\n\n\n\n\np values are properly written as inequalities (eg, ‚ÄúP ‚â§ 0.02‚Äù when P = 0.015)\n\nWhile showing p-values in results, it is always a thumb rule to denote the actual value rather than denoting them as an inequality. For example, for \\alpha=0.05, a result displaying p = 0.04 is far more useful, when compared to displaying it as p ‚â§ 0.05 as we know that the p-value is barely on the edge of the level of significance.\n\n\n\n\n\n\n\n\nMisconception 9\n\n\n\n\np = 0.05 means that if you reject the null hypothesis, the probability of a type I error is only 5%.\n\nType I error also called false-positive is when we accept that there is some effect when in reality there is none. Now the definition of p-value is that it is the probability of obtaining the observed effect plus more extreme effects, given that the null hypothesis is true. Therefore for type, I error to happen, the null hypothesis has to be true. So if we reject the null hypothesis, then the chance of type I error is zero.\n\n\n\n\n\n\n\n\nMisconception 10\n\n\n\n\nWith p = 0.05 threshold for significance(\\alpha), the chance of a type I error will be 5%.\n\nIf our p = \\alpha = 0.05, then we reject the null hypothesis. In doing so, we are allowing ourselves the chance of error that there is a 5% chance that the null hypothesis is true. Do not get confused by definitions of p-value and \\alpha. Having p = 0.05 and setting \\alpha = 0.05 are two different things. Therefore \\alpha can also be termed as the probability that a type I error occurs. So when does this statement becomes a misconception?\nFor multiple pairwise comparisons, this concept is not true. For \\alpha = 0.05, the new \\alpha for n tests is equal to;\n\\alpha' = 1 - (1-\\alpha)^n = 1 - (1-0.05)^n\nTherefore even if we had set \\alpha = 0.05 prior, after multiple comparisons the resultant \\alpha' will be a greater value. This is also called as multiple comparison problem and it is corrected through methods called as multiple comparison corrections. You can learn more about this in Chapter 9."
  },
  {
    "objectID": "tutorials/stat_basic/project4.html#references",
    "href": "tutorials/stat_basic/project4.html#references",
    "title": "Chapter 4: p-value and level of significance",
    "section": "\n3 References",
    "text": "3 References\n\nGoodman S. A dirty dozen: twelve p-value misconceptions. Semin Hematol. 2008 Jul;45(3):135-40. doi: 10.1053/j.seminhematol.2008.04.003. Erratum in: Semin Hematol. 2011 Oct;48(4):302. PMID: 18582619. Link\nGao, J. P-values ‚Äì a chronic conundrum. BMC Med Res Methodol 20, 167 (2020). https://doi.org/10.1186/s12874-020-01051-6 Link\nGoodman SN. p values, hypothesis tests, and likelihood: implications for epidemiology of a neglected historical debate. Am J Epidemiol. 1993 Mar 1;137(5):485-96; discussion 497-501. doi: 10.1093/oxfordjournals.aje.a116700. PMID: 8465801. Link\nLang JM, Rothman KJ, Cann CI. That confounded P-value. Epidemiology. 1998 Jan;9(1):7-8. doi: 10.1097/00001648-199801000-00004. PMID: 9430261. Link\n\nLast updated on\n\n\n[1] \"2022-08-04 01:35:47 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project5.html",
    "href": "tutorials/stat_basic/project5.html",
    "title": "Chapter 5: Type I and Type II errors",
    "section": "",
    "text": "Type I and Type II error in its simplicity"
  },
  {
    "objectID": "tutorials/stat_basic/project5.html#type-i-error",
    "href": "tutorials/stat_basic/project5.html#type-i-error",
    "title": "Chapter 5: Type I and Type II errors",
    "section": "\n1 Type I error",
    "text": "1 Type I error\nType I error happens when we fail to reject the null hypothesis when it is true. It is also called ‚Äòfalse-positive‚Äô. The chance of a type I error occurring is determined by the level of significance(\\alpha) that we set before our experiments. Generally, \\alpha is set at a value of 0.05. As previously mentioned, at this value, there is a 5% chance that the observed results or more extreme results to occur, given that the null hypothesis is true. At lower values of \\alpha, the chance of type I error to occur reduces.\n\n\nFig1: Type I error. Credits: www.scribbr.com"
  },
  {
    "objectID": "tutorials/stat_basic/project5.html#type-ii-error",
    "href": "tutorials/stat_basic/project5.html#type-ii-error",
    "title": "Chapter 5: Type I and Type II errors",
    "section": "\n2 Type II error",
    "text": "2 Type II error\nType II error happens when we fail to not reject the null hypothesis when it is false. It is also called ‚Äòfalse-negative‚Äô. The chance of type II error occurring is inversely proportional to the statistical power of the study. The statistical power of a study depends on many factors. Two of those factors are the effect size and the sample size. The effect size describes how strong the relationship is between the variables that we are experimenting with. So studies having smaller effect sizes will lead to higher chances of committing type II error when the sample size is small. Since we cannot do anything about the effect size of a study all we can do is increase the sample size of the study. Another factor that affects the chance of causing type II error is the level of significance(\\alpha) we set. At lower levels of \\alpha, there is less chance of type II error.\n\n\nFig1: Type II error. Credits: www.scribbr.com\n\n\nIn the later chapters, we will see how to calculate the effect size of a study."
  },
  {
    "objectID": "tutorials/stat_basic/project5.html#references",
    "href": "tutorials/stat_basic/project5.html#references",
    "title": "Chapter 5: Type I and Type II errors",
    "section": "\n3 References",
    "text": "3 References\n\nIoannidis JPA (2005) Why Most Published Research Findings Are False. PLOS Medicine 2(8): e124. https://doi.org/10.1371/journal.pmed.0020124 Link\n\n\nLast updated on\n\n\n[1] \"2022-08-04 01:35:52 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project6.html",
    "href": "tutorials/stat_basic/project6.html",
    "title": "Chapter 6: Confidence interval and effect size",
    "section": "",
    "text": "What is the correct definition Will?"
  },
  {
    "objectID": "tutorials/stat_basic/project6.html#confidence-interval-ci",
    "href": "tutorials/stat_basic/project6.html#confidence-interval-ci",
    "title": "Chapter 6: Confidence interval and effect size",
    "section": "\n1 Confidence interval (CI)",
    "text": "1 Confidence interval (CI)\nConfidence interval(CI) can be thought of as a range of values derived from a point estimate (for eg: sample mean) which has a likelihood of having the true point estimate of the population. The likelihood is set by the user and generally, they are set at 95%.\n\n\n\n\n\n\nDefinition of 95% CI\n\n\n\nA 95% confidence interval means that if the experiment were to be repeated multiple times, 95% of the calculated intervals from the experiments would contain the true point estimate value\n\n\nThe CI is descriptive statistics which is used for population data but can also be used for sample data.\nThe formula for finding CI is;\nCI = \\mu ¬± z*S.E = \\mu ¬± z*(\\sigma/\\sqrt n) Where \\mu is sample mean, S.E is standard error of means, z is z-value, \\sigma is standard deviation and n is the sample size.\nFor calculating 95% CI the corresponding z-value is 1.96. Normally for sufficiently large value sample sizes this formula works fine, but for sample sizes < 30, instead of the z-score we use the t-statistic which is obtained from the t-distribution. Also in this case, instead of n in the denominator, we use degrees of freedom which n-1. The formula then becomes;\nCI = \\mu ¬± t*(\\sigma/\\sqrt{n-1}) The t-statistic will depend on the degrees of freedom and the confidence level.\nThe confidence interval can be more useful when compared to p-value as they convey the precision of our estimates. The width of the CI is affected by the sample size and standard deviation, therefore CI with higher sample sizes is better in representing the population estimate. Generally, the standard deviation used for calculating CI should be the population standard deviation which unfortunately is most times an unknown value. Therefore for the calculations, it is better to use the SD from the control group as they resemble SD value close to the population value as they don‚Äôt undergo treatment. If both the control group and the treatment group have equal or close to equal variances, then we can use ‚Äúpooled SD‚Äù for calculating the CI. Pooled SD is the average of both the treatment and the control groups SD."
  },
  {
    "objectID": "tutorials/stat_basic/project6.html#effect-size",
    "href": "tutorials/stat_basic/project6.html#effect-size",
    "title": "Chapter 6: Confidence interval and effect size",
    "section": "\n2 Effect size",
    "text": "2 Effect size\nAs previously mentioned, the p-value does not convey any information about the magnitude of an effect. The CI does a fair job of giving us a range of values but it is dependent on the sample size. This is where effect size comes in. The effect size of a study tells us the magnitude of the difference between the two groups. There are many ways effect size can be calculated as shown in the table below.\n\n\nFig1: Effect sizes. Credits: Sullivan, G. M., & Feinn, R. (2012)"
  },
  {
    "objectID": "tutorials/stat_basic/project6.html#references",
    "href": "tutorials/stat_basic/project6.html#references",
    "title": "Chapter 6: Confidence interval and effect size",
    "section": "\n3 References",
    "text": "3 References\n\nLee D. K. (2016). Alternatives to P value: confidence interval and effect size. Korean journal of anesthesiology, 69(6), 555‚Äì562. https://doi.org/10.4097/kjae.2016.69.6.555 Link\nHazra A. (2017). Using the confidence interval confidently. Journal of thoracic disease, 9(10), 4125‚Äì4130. https://doi.org/10.21037/jtd.2017.09.14 Link\nSullivan, G. M., & Feinn, R. (2012). Using Effect Size-or Why the P Value Is Not Enough. Journal of graduate medical education, 4(3), 279‚Äì282. https://doi.org/10.4300/JGME-D-12-00156.1 Link\nIalongo C. (2016). Understanding the effect size and its measures. Biochemia medica, 26(2), 150‚Äì163. https://doi.org/10.11613/BM.2016.015 Link\nNakagawa S, Cuthill IC. Effect size, confidence interval and statistical significance: a practical guide for biologists. Biol Rev Camb Philos Soc. 2007 Nov;82(4):591-605. doi: 10.1111/j.1469-185X.2007.00027.x. Erratum in: Biol Rev Camb Philos Soc. 2009 Aug;84(3):515. PMID: 17944619. Link\n\nLast updated on\n\n\n[1] \"2022-08-04 01:35:57 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project7.html#parametric-test-assumptions",
    "href": "tutorials/stat_basic/project7.html#parametric-test-assumptions",
    "title": "Chapter 7: Parametric tests and assumptions",
    "section": "\n1 Parametric test assumptions",
    "text": "1 Parametric test assumptions\nParametric tests can only be used in datasets having two groups, which fulfill the following conditions;\n\nThe data should follow a normal distribution\nThe groups should have equal variances (homoscedasticity)\nThe dependent variable should be of an ordinal scale, which means it can be arranged in a defined order\nThe two groups should be independent of each other\n\nNow we will learn how to do some of the parametric tests in R. For this tutorial, we will be using the penguins dataset from the palmerpenguins package in R. Before we begin trying out the test, we will first have to check if the data is normally distributed. Using various normality tests, we can check if our data is normally distributed. On the other hand, we can also check if our data is normally distributed by simply plotting the data or by using a qq-plot.\n\n1.1 Density plot, qq-plot and residual plot\nNow we will plot a histogram of body mass in Chinstrap penguins.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\n#install.packages(\"ggpubr\")\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(ggpubr)\n\npenguins %>% filter(species == \"Chinstrap\") %>% ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Frequency\") + ggtitle(\"Body mass distribution\") + \n   geom_histogram(bins = 25) + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Chinstrap\") %>% nrow())) +\n  theme_bw()\n\n\n\n\nThe plot is similar to a normal distribution. We can also see the same by plotting a qq-plot\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\n#install.packages(\"ggpubr\")\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(ggpubr)\n\npenguins %>% filter(species == \"Chinstrap\") %>% \n  ggqqplot(x = \"body_mass_g\", color = \"species\")\n\n\n\n\nIf the qq-plot shows data points aligning within the reference line, then we can conclude that the data points follow a normal distribution.\nWe can also test the same by plotting a residual plot which shows the distribution of residual distances of the data points. Residual distance shows how far each point is from the mean value.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\n#install.packages(\"ggpubr\")\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(ggpubr)\n\npenguins %>% filter(species == \"Chinstrap\") %>% ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Residual\") + ggtitle(\"Body mass distribution\") + \n   geom_density() + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Chinstrap\") %>% nrow())) +\n  theme_bw()\n\n\n\n\nAs we found earlier, the plot resembles a normal distribution.\nNow let us do some normality tests to check if the data we have is following a normal distribution. In normality tests, the null hypothesis is that the distribution is normal and the alternate hypothesis is that it is not.\n\n1.2 Shapiro-Wilk‚Äôs test\nWe will be doing the Kolmogorov-Smirnov (K-S) normality test and Shapiro-Wilk‚Äôs test. The Kolmogorov‚ÄìSmirnov test is best for sample sizes ‚â• 50 and Shapiro-Wilk‚Äôs test is best for < 50.\n\npen_chin <- penguins %>% filter(species == \"Chinstrap\")\nshapiro.test(pen_chin$body_mass_g)\n\n\n    Shapiro-Wilk normality test\n\ndata:  pen_chin$body_mass_g\nW = 0.98449, p-value = 0.5605\n\n\nFrom the output, the p-value > 0.05 implies that the distribution of the data is not significantly different from a normal distribution. In other words, we can assume normality.\n\n1.3 Kolmogorov-Smirnov (K-S) normality test\nThe Kolmogorov-Smirnov test requires a continuous distribution to function properly. So let us do the test on dummy data.\n\nset.seed(123)\n\nx <- rnorm(50)\n\nks.test(x,\"pnorm\")\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  x\nD = 0.073034, p-value = 0.9347\nalternative hypothesis: two-sided\n\n\nAs p > 0.05, we can confirm that the data is normally distributed."
  },
  {
    "objectID": "tutorials/stat_basic/project7.html#parametric-test",
    "href": "tutorials/stat_basic/project7.html#parametric-test",
    "title": "Chapter 7: Parametric tests and assumptions",
    "section": "\n2 Parametric test",
    "text": "2 Parametric test\n\n2.1 one sample t-test\nThe student‚Äôs t-test is a very popular parametric. In the following example, we will be doing a one-sample t-test. We will be checking if the body masses of Gentoo penguins are significantly different from 4500g.\n\npen_gen <- penguins %>% filter(species==\"Gentoo\")\n# one sample t-test\nt.test(pen_gen$body_mass_g, mu = 4500)\n\n\n    One Sample t-test\n\ndata:  pen_gen$body_mass_g\nt = 12.672, df = 122, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 4500\n95 percent confidence interval:\n 4986.034 5165.998\nsample estimates:\nmean of x \n 5076.016 \n\n\nSo the body masses of Gentoo are significantly large than 4500g.\n\n2.2 two sample t-test\nIf we are comparing the means of two groups with equal variance, then we do a two-sample t-test. We will compare the body weights of Gentoo to Chinstrap species and see if their mean body masses differ significantly.\n\npen_gen <- penguins %>% filter(species==\"Gentoo\")\npen_chin <- penguins %>% filter(species==\"Chinstrap\")\n# two sample t-test\nt.test(pen_gen$body_mass_g, pen_chin$body_mass_g, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  pen_gen$body_mass_g and pen_chin$body_mass_g\nt = 19.103, df = 189, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1204.258 1481.598\nsample estimates:\nmean of x mean of y \n 5076.016  3733.088 \n\n\nThe test results show that the body masses of these two species of penguins differ significantly from each other.\n\n2.3 welsh‚Äôs t-test\nNow, normally we need to have the variances of two groups the same to perform a two-sample t-test. If the variances are not the same, then we could still do a t-test. For Welch‚Äôs t-test, variance doesn‚Äôt need to be the same for the two groups in question.\n\npen_gen <- penguins %>% filter(species==\"Gentoo\")\npen_chin <- penguins %>% filter(species==\"Chinstrap\")\n# welsh's t-test\nt.test(pen_gen$body_mass_g, pen_chin$body_mass_g)\n\n\n    Welch Two Sample t-test\n\ndata:  pen_gen$body_mass_g and pen_chin$body_mass_g\nt = 20.628, df = 170.4, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1214.416 1471.440\nsample estimates:\nmean of x mean of y \n 5076.016  3733.088 \n\n\n\n2.4 paired t-test\nNow if we have paired data, then to compare between two groups, we can do a paired t-test. Paired data means that the data in the two groups came from the same source. Here we will be comparing Gentoo penguin bill lengths versus bill depth. Here both the bill length and bill depth are measured from the same set of individuals.\n\npen_gen <- penguins %>% filter(species==\"Gentoo\")\n# paired t-test\nt.test(pen_gen$bill_length_mm, pen_gen$bill_depth_mm, paired = T)\n\n\n    Paired t-test\n\ndata:  pen_gen$bill_length_mm and pen_gen$bill_depth_mm\nt = 140.73, df = 122, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 32.06527 32.98026\nsample estimates:\nmean difference \n       32.52276"
  },
  {
    "objectID": "tutorials/stat_basic/project7.html#references",
    "href": "tutorials/stat_basic/project7.html#references",
    "title": "Chapter 7: Parametric tests and assumptions",
    "section": "\n3 References",
    "text": "3 References\n\nGhasemi, A., & Zahediasl, S. (2012). Normality tests for statistical analysis: a guide for non-statisticians. International journal of endocrinology and metabolism, 10(2), 486‚Äì489. https://doi.org/10.5812/ijem.3505 Link\nLivingston, E. H. (2004). Who was student and why do we care so much about his T-test?1. Journal of Surgical Research, 118(1), 58‚Äì65. https://doi.org/10.1016/j.jss.2004.02.003 Link\n\nLast updated on\n\n\n[1] \"2022-08-04 01:36:07 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project8.html",
    "href": "tutorials/stat_basic/project8.html",
    "title": "Chapter 8: Non-parametric test and assumptions",
    "section": "",
    "text": "Which is more robust between paramteric and non-paramteric?\n\n\n\n\nImage source: Twitter"
  },
  {
    "objectID": "tutorials/stat_basic/project8.html#non-parametric-test-and-assumptions",
    "href": "tutorials/stat_basic/project8.html#non-parametric-test-and-assumptions",
    "title": "Chapter 8: Non-parametric test and assumptions",
    "section": "\n1 Non-parametric test and assumptions",
    "text": "1 Non-parametric test and assumptions\nNon-parametric tests are used when the assumptions for parametric tests fail;\n\nThe data does not normal distribution\nThe groups should have un-equal variances (heteroskedasticity)\nThe two groups are not independent of each other"
  },
  {
    "objectID": "tutorials/stat_basic/project8.html#mannwhitney-u-test",
    "href": "tutorials/stat_basic/project8.html#mannwhitney-u-test",
    "title": "Chapter 8: Non-parametric test and assumptions",
    "section": "\n2 Mann‚ÄìWhitney U test",
    "text": "2 Mann‚ÄìWhitney U test\n\n2.1 one sample mann-whitney u test\nThe Mann‚ÄìWhitney U test also known as the Mann-Whitney-Wilcoxon test is the equivalent of one sample and two-sample t-test for non-normal data. Like last time, in this tutorial also, we will be using the penguins dataset from the palmerpenguins package in R. For the sake of the tutorial, we will assume that the assumptions for parametric tests are violated here.\nWe will we checking if the median bill length of Gentoo penguins is significantly different from 50mm.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\npen_gen <- penguins %>% filter(species==\"Gentoo\")\n# one sample mann-whitney test\nwilcox.test(pen_gen$bill_length_mm, mu = 50)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  pen_gen$bill_length_mm\nV = 816.5, p-value = 2.859e-13\nalternative hypothesis: true location is not equal to 50\n\n\n\n2.2 two sample mann-whitney u test\nWe will we checking if bill lengths of Gentoo penguins differ significantly from the bill length of Adelie. This will be a two sample test.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\npen_gen <- penguins %>% filter(species==\"Gentoo\")\npen_ad <- penguins %>% filter(species==\"Adelie\")\n# two sample mann-whitney test\nwilcox.test(pen_gen$bill_length_mm, pen_ad$bill_length_mm)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  pen_gen$bill_length_mm and pen_ad$bill_length_mm\nW = 18349, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n2.3 wilcoxon signed rank test\nThe Wilcoxon Signed-Rank test is the equivalent of paired t-test.\nWe will be checking if there is a significant difference between the bill length and flipper length in Chinstrap penguins.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\npen_chin <- penguins %>% filter(species==\"Chinstrap\")\n# wilcoxon singed rank test\nwilcox.test(pen_chin$bill_length_mm, pen_chin$flipper_length_mm, paired = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  pen_chin$bill_length_mm and pen_chin$flipper_length_mm\nV = 0, p-value = 7.801e-13\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "tutorials/stat_basic/project8.html#references",
    "href": "tutorials/stat_basic/project8.html#references",
    "title": "Chapter 8: Non-parametric test and assumptions",
    "section": "\n3 References",
    "text": "3 References\n\nNeuh√§user M. (2011) Wilcoxon‚ÄìMann‚ÄìWhitney Test. In: Lovric M. (eds) International Encyclopedia of Statistical Science. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-04898-2_615 Link\n\n\nLast updated on\n\n\n[1] \"2022-08-04 01:36:20 IST\""
  },
  {
    "objectID": "tutorials/stat_basic/project9.html",
    "href": "tutorials/stat_basic/project9.html",
    "title": "Chapter 9: Tests for multiple groups of data",
    "section": "",
    "text": "It always has been"
  },
  {
    "objectID": "tutorials/stat_basic/project9.html#multiple-pairwise-comparisons-problem",
    "href": "tutorials/stat_basic/project9.html#multiple-pairwise-comparisons-problem",
    "title": "Chapter 9: Tests for multiple groups of data",
    "section": "\n1 Multiple pairwise comparisons problem",
    "text": "1 Multiple pairwise comparisons problem\nImagine you are rolling dice. Then the probability to get 6 as an outcome is;\n\\begin{align*}\nP(probability\\ to\\ get\\ 6) &= 1/6 \\\\\n&= 1- P(probability\\ to\\ not\\ get\\ 6) \\\\\n&= 1- 5/6 \\\\\nTherefore;\\ P(probability\\ to\\ not\\ get\\ 6) &= 1- 1/6\n\\end{align*}\nLet‚Äôs roll the dice for the second time, the what is the probability to get at least one 6 in these two attempts.\n\\begin{align*}\nP(probability\\ to\\ get\\ at\\ least\\ one\\ 6\\ in\\ two\\ rolls) &= 1/6 + 1/6\\\\\n&= 1- P(probability\\ to\\ not\\ get\\ 6\\ in\\ two\\ rolls) \\\\\n&= 1 - (1 - 1/6)^2\n\\end{align*}\nTherefore for n rolls, we have the probability to get at least one 6 as;\n\\begin{align*}\nP(probability\\ to\\ get\\ at\\ least\\ one\\ 6\\ in\\ n\\ rolls) &= 1/6 + 1/6 + \\dots \\\\\n&= 1- P(probability\\ to\\ not\\ get\\ 6\\ in\\ n\\ rolls) \\\\\n&= 1 - (1 - 1/6)^n\n\\end{align*}\nTherefore as we keep increasing the rolls, we are bound to get a 6 in one of these rolls. This means the probability to get 6 increases as the number of rolls increases.\nNow with the same notion as earlier, imagine the following situation.\nImagine you have set \\alpha = 0.05, then the probability to have a type I error will be 0.05, and the probability of not having a type I error will be 0.95.\n\\begin{align*}\nP(Probability\\ to\\ have\\ type\\ I\\ error)&= 1 - P(Probability\\ to\\ not\\ have\\ type\\ I\\ error) \\\\\n&= 1- 0.95 \\\\\nTherefore;\\ P(Probability\\ to\\ not\\ have\\ type\\ I\\ error) &= 1 - 0.05\n\\end{align*}\nNow if we do the same test again, the probability to get at least a single type I error is;\n\\begin{align*}\nP(Probability\\ to\\ have\\ at\\ least\\ one\\ type\\ I\\ error\\ in\\ 2\\ tests)&= 1 - P(Probability\\ to\\ have\\ no\\ type\\ I\\ error\\ in\\ 2\\ tests) \\\\\n&= 1- (1-0.05)^2\n\\end{align*}\nTherefore if we perform n pairwise comparisons, then the probability to have at least one type I error in n tests is;\n\\begin{align*}\nP(Probability\\ to\\ have\\ at\\ least\\ one\\ type\\ I\\ error\\ in\\ n\\ tests)&= 1 - P(Probability\\ to\\ have\\ no\\ type\\ I\\ error\\ in\\ n\\ tests) \\\\\n&= 1- (1-0.05)^n\n\\end{align*}\nLike for the coin toss problem we get an increased probability to get a type I error, even when we had set the initial \\alpha to be 0.05. As an example, if we do multiple t-tests for different groups in our data, considering multiple hypotheses, we have a high chance of getting a type I error. This is known as a multiple pairwise comparison problem. This is avoided by implementing countermeasures called multiple comparison corrections such as Tukey Honest Significant Differences and the Bonferroni method."
  },
  {
    "objectID": "tutorials/stat_basic/project9.html#parametric-multiple-comparison-test",
    "href": "tutorials/stat_basic/project9.html#parametric-multiple-comparison-test",
    "title": "Chapter 9: Tests for multiple groups of data",
    "section": "\n2 Parametric multiple comparison test",
    "text": "2 Parametric multiple comparison test\nIf the data follows the assumptions for parametric tests, then for comparing multiple independent groups of data, we can ANOVA. A significant p-value from ANOVA indicates some of the group differences are significantly different from each other, but it does not tell which of these groups are different from each other. To know which groups are different from each other, we perform a post hoc test like a t-test, Tukey‚Äôs test etc."
  },
  {
    "objectID": "tutorials/stat_basic/project9.html#anova",
    "href": "tutorials/stat_basic/project9.html#anova",
    "title": "Chapter 9: Tests for multiple groups of data",
    "section": "\n3 ANOVA",
    "text": "3 ANOVA\n\n3.1 one way ANOVA\nNow let us try doing ANOVA in R. Like before, we will be using the penguins dataset from the palmerpenguins package in R. We will compare the body masses of all three species of penguins in the dataset and see if any two of them are significantly different from each other. In this case, we will be doing a one-way ANOVA, where species is our independent variable and body mass is the independent variable.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass <- penguins[,c(1,6)]\n# one way ANOVA test\nanova1 <- aov(body_mass_g ~ species, data = pen_mass)\n# summarising test result\nsummary(anova1)\n\n             Df    Sum Sq  Mean Sq F value Pr(>F)    \nspecies       2 146864214 73432107   343.6 <2e-16 ***\nResiduals   339  72443483   213698                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n\n\nThe column Pr(>F) corresponds to the p-value of the test and from the result we find that the test returned a statistically significant p-value. Now to see which species have different body masses from each other, we run a post-hoc test which in this case, we will run a Tukey HSD test.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass <- penguins[,c(1,6)]\n# one way ANOVA test\nanova1 <- aov(body_mass_g ~ species, data = pen_mass)\n# tukey HSD test\nTukeyHSD(anova1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = body_mass_g ~ species, data = pen_mass)\n\n$species\n                       diff       lwr       upr     p adj\nChinstrap-Adelie   32.42598 -126.5002  191.3522 0.8806666\nGentoo-Adelie    1375.35401 1243.1786 1507.5294 0.0000000\nGentoo-Chinstrap 1342.92802 1178.4810 1507.3750 0.0000000\n\n\nFrom the post-hoc test, we can see that body masses between Gentoo-Adelie and Gentoo-Chinstrap are significantly different from each other.\n\n3.2 two way ANOVA\nNow if we have more than one independent variable, then we can do a two-way ANOVA. In the test below we have both species and island as the independent variables.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass_land <- penguins[,c(1,2,6)]\n# two way ANOVA test\nanova2 <- aov(body_mass_g ~ species + island, data = pen_mass_land)\n# summarising test result\nsummary(anova2)\n\n             Df    Sum Sq  Mean Sq F value Pr(>F)    \nspecies       2 146864214 73432107 341.663 <2e-16 ***\nisland        2     13655     6827   0.032  0.969    \nResiduals   337  72429829   214925                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n\n\nHere the results suggests that, body masses are different between species but there is no significant difference in body mass across the different islands. The follow up post-hoc test would look like this.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass_land <- penguins[,c(1,2,6)]\n# two way ANOVA test\nanova2 <- aov(body_mass_g ~ species + island, data = pen_mass_land)\n# tukey HSD test\nTukeyHSD(anova2)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = body_mass_g ~ species + island, data = pen_mass_land)\n\n$species\n                       diff       lwr       upr     p adj\nChinstrap-Adelie   32.42598 -126.9602  191.8122 0.8813058\nGentoo-Adelie    1375.35401 1242.7960 1507.9120 0.0000000\nGentoo-Chinstrap 1342.92802 1178.0050 1507.8511 0.0000000\n\n$island\n                      diff       lwr      upr     p adj\nDream-Biscoe     -7.911442 -137.2861 121.4632 0.9886403\nTorgersen-Biscoe  3.339873 -171.2651 177.9448 0.9988827\nTorgersen-Dream  11.251314 -170.2981 192.8007 0.9883345\n\n\nWe got tot the same inference as last time.\n\n3.3 two way ANOVA with dependent variable\nNow suppose that there is a synergistic effect between species and sex variable. Then in that case they are not independent with each other, nevertheless we can incorporate this factor by changing the addition sign + between the variables to an asterisk sign *.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass_land <- penguins[,c(1,6,7)]\n# two way ANOVA test with dependent variables\n# species and sex variables are dependent in nature\nanova_2 <- aov(body_mass_g ~ species * sex, data = pen_mass_land)\n# summarising test result\nsummary(anova_2)\n\n             Df    Sum Sq  Mean Sq F value   Pr(>F)    \nspecies       2 145190219 72595110 758.358  < 2e-16 ***\nsex           1  37090262 37090262 387.460  < 2e-16 ***\nspecies:sex   2   1676557   838278   8.757 0.000197 ***\nResiduals   327  31302628    95727                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n11 observations deleted due to missingness\n\n\nFrom the result we can see that both species and sex have a significant effect on body mass and also the interaction between species and sex is also a significant effect.\nThe follow up post hoc test result will be;\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass_land <- penguins[,c(1,6,7)]\n# two way ANOVA test with dependent variables\n# species and sex variables are dependent in nature\nanova_2 <- aov(body_mass_g ~ species * sex, data = pen_mass_land)\n# tukey HSD test\nTukeyHSD(anova_2)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = body_mass_g ~ species * sex, data = pen_mass_land)\n\n$species\n                       diff       lwr       upr     p adj\nChinstrap-Adelie   26.92385  -80.0258  133.8735 0.8241288\nGentoo-Adelie    1386.27259 1296.3070 1476.2382 0.0000000\nGentoo-Chinstrap 1359.34874 1248.6108 1470.0866 0.0000000\n\n$sex\n                diff      lwr      upr p adj\nmale-female 667.4577 600.7462 734.1692     0\n\n$`species:sex`\n                                     diff       lwr       upr     p adj\nChinstrap:female-Adelie:female   158.3703  -25.7874  342.5279 0.1376213\nGentoo:female-Adelie:female     1310.9058 1154.8934 1466.9181 0.0000000\nAdelie:male-Adelie:female        674.6575  527.8486  821.4664 0.0000000\nChinstrap:male-Adelie:female     570.1350  385.9773  754.2926 0.0000000\nGentoo:male-Adelie:female       2116.0004 1962.1408 2269.8601 0.0000000\nGentoo:female-Chinstrap:female  1152.5355  960.9603 1344.1107 0.0000000\nAdelie:male-Chinstrap:female     516.2873  332.1296  700.4449 0.0000000\nChinstrap:male-Chinstrap:female  411.7647  196.6479  626.8815 0.0000012\nGentoo:male-Chinstrap:female    1957.6302 1767.8040 2147.4564 0.0000000\nAdelie:male-Gentoo:female       -636.2482 -792.2606 -480.2359 0.0000000\nChinstrap:male-Gentoo:female    -740.7708 -932.3460 -549.1956 0.0000000\nGentoo:male-Gentoo:female        805.0947  642.4300  967.7594 0.0000000\nChinstrap:male-Adelie:male      -104.5226 -288.6802   79.6351 0.5812048\nGentoo:male-Adelie:male         1441.3429 1287.4832 1595.2026 0.0000000\nGentoo:male-Chinstrap:male      1545.8655 1356.0392 1735.6917 0.0000000\n\n\nFrom the pairwise comparisons we can see that male and female penguins have significantly different body masses and also the Chinstrap penguins with respect to sex, have similar body mass to Adelie penguins.\n\n3.4 ANOVA test for unbalanced designs\nNow if the dataset has unequal sample sizes for each groups, then we can use the ANOVA test for unbalanced designs. To use the test we would require the car package in R.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\n#install.packages(‚Äúcar‚Äù)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(car)\n\n# selecting species and mass columns\npen_mass_land <- penguins[,c(1,6,7)]\n# two way ANOVA test with dependent variables\n# species and sex variables are dependent in nature\nanova_2 <- aov(body_mass_g ~ species * sex, data = pen_mass_land)\nAnova(anova_2, type = \"III\")\n\n\n\n  \n\n\n\n\n3.5 MANOVA\nIf you have multiple dependent variables, then you can use MANOVA. Here our dependent variables are body mass and bill depth and independent variable is species. We want to see if body mass and bill depth is associated with different species of penguins.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\n#install.packages(‚Äúcar‚Äù)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(car)\n\n# selecting species and mass columns\npen_mass_bill <- penguins[,c(1,4,6)]\nmass_bill <- cbind(penguins$bill_depth_mm, penguins$body_mass_g)\n# two way ANOVA test with dependent variables\n# species and sex variables are dependent in nature\nanova_3<- manova(mass_bill ~ species, data = pen_mass_bill)\nsummary(anova_3)\n\n           Df  Pillai approx F num Df den Df    Pr(>F)    \nspecies     2 0.91732   143.61      4    678 < 2.2e-16 ***\nResiduals 339                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the result, we can see that, species statistically significant association with both combined body mass and bill depth variables."
  },
  {
    "objectID": "tutorials/stat_basic/project9.html#kruskal-wallis-test",
    "href": "tutorials/stat_basic/project9.html#kruskal-wallis-test",
    "title": "Chapter 9: Tests for multiple groups of data",
    "section": "\n4 Kruskal-Wallis Test",
    "text": "4 Kruskal-Wallis Test\nThe Kruskal-Wallis test is the non-parametric equivalent of ANOVA.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass <- penguins[,c(1,6)]\n# Kruskal-Wallis Test\nkruskal.test(body_mass_g ~ species, data = pen_mass)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  body_mass_g by species\nKruskal-Wallis chi-squared = 217.6, df = 2, p-value < 2.2e-16\n\n\nFor post-hoc test, we can use the non-parametric version of pairwise comparisons tests coupled with p-value correction. Here we will be using the Wilcoxon rank sum test.\n\n#install.packages(\"palmerpenguins\")\n#install.packages(\"tidyverse\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# selecting species and mass columns\npen_mass <- penguins[,c(1,6)]\n# wilcoxon rank sum test\npairwise.wilcox.test(pen_mass$body_mass_g, pen_mass$species, p.adjust.method = \"BH\")\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  pen_mass$body_mass_g and pen_mass$species \n\n          Adelie Chinstrap\nChinstrap 0.49   -        \nGentoo    <2e-16 <2e-16   \n\nP value adjustment method: BH \n\n\nHere we can see that, Gentoo penguins have significantly different body masses compared to both Adelie and Chinstrap species."
  },
  {
    "objectID": "tutorials/stat_basic/project9.html#references",
    "href": "tutorials/stat_basic/project9.html#references",
    "title": "Chapter 9: Tests for multiple groups of data",
    "section": "\n5 References",
    "text": "5 References\n\nCardinal, R.N., & Aitken, M.R.F. (2005). ANOVA for the Behavioral Sciences Researcher (1st ed.). Psychology Press. https://doi.org/10.4324/9780203763933 Link\nKeselman HJ, Huberty CJ, Lix LM, et al.¬†Statistical Practices of Educational Researchers: An Analysis of their ANOVA, MANOVA, and ANCOVA Analyses. Review of Educational Research. 1998;68(3):350-386. doi:10.3102/00346543068003350 Link\n\nLast updated on\n\n\n[1] \"2022-08-04 01:36:32 IST\""
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html",
    "href": "tutorials/stat_model/intro_stat_model.html",
    "title": "Introduction to statistical modelling in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() => null)"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#definition-of-statistical-model",
    "href": "tutorials/stat_model/intro_stat_model.html#definition-of-statistical-model",
    "title": "Introduction to statistical modelling in R",
    "section": "\n1 Definition of statistical model",
    "text": "1 Definition of statistical model\nBefore we get to know the definition of what a statistical model is, let us first try to understand what a model means. All of us have an intuitive idea of what a ‚Äòmodel‚Äô is. A model can be a fashion model posing in pretty clothes and makeup or it can be a blueprint which represents the overall layout of a building. In all these understandings of a model, we can summarise that it is representing a relationship.\nA statistical model also belongs to the same suite but is made using data. It is also representing ‚Äòrelationships‚Äô, but between variables within the data. Understanding a statistical model can lead us to understand how the variables within a dataset are related to each other.\nLet us consider the graph given below;\n\n\n\n\n\nThe x-axis shows speed and the y-axis shows stopping distance. From the graph, you can see that, as speed increases, the stopping distance also increases. Therefore we can say that the stopping distance linearly increases with speed. To showcase this linear relationship, we can plot a regression line (green line) as shown below.\n\n\n\n\n\nThe regression line can be extended to values outside the dataset and doing so would enable us to predict values outside our dataset, like predicting what the stopping distance will be if the speed was 35 mph. You can also see that many points are away from the regression line and do not lie on it, therefore the linear relationship we predicted for the variables is not a perfect one. Suppose instead of a linear relationship, we fit a polynomial regression line.\n\n\n\n\n\nIn this graph, especially at the extreme ends, most of the points are closer to the regression line than in the earlier case. Therefore, the residual distance, which is the distance between the actual data points and the fitted values by regression lines would be smaller indicating a better fit for the polynomial regression. Therefore a good fit can more accurately represent the relationship between the variables and therefore would have better predictive power. But how do we exactly compare the linear regression and the polynomial regression ‚Äòmodels‚Äô? Is there a metric which would allow this comparison easier? We will learn all about this later in this article.\nGoing back to the linear regression plot, suppose, instead of speed, we use the colour of the car and see if it affects the stopping distance at a constant speed. For obvious reasons, there should be no effect of colour on the stopping distances (duh!). Nevertheless, now we also know that some variables form relationships and some don‚Äôt.\nIn short, what we essentially were doing was linear modelling, a type of statistical modelling. And this exercise helped us realise that a model can inform us about;\n\nThe relationship between the variables in the data\nWhich variables form relationships\nPredict values outside our dataset\nModel comparisons (linear vs polynomial)\n\nThe textbook definition for statistical modelling is;\n\nA statistical model is a set of probability distributions P(S) on a sample space (S) (1).\n\nIn our previous modelling exercise, we saw how speed affects stopping distance. Here the variable ‚Äòspeed‚Äô is called the parameter or otherwise, better called the explanatory variable, which tries to explain the variations seen in the response variable, which is the stopping distance. The model we created was not perfect as there were many data points which did not lie on the regression line. One potential source of this error can arrive from the selection of the explanatory variables. The dataset had stopping distances of a particular vehicle at different speeds. The stopping distance can also be affected by the road terrain (friction) and the efficiency of the brakes, all of which were missing from the dataset. Therefore these parameters that we have not accounted for can also potentially affect our model. So why weren‚Äôt those included in the dataset? Often it is time-consuming to keep note of every parameter that can affect the variable we are interested in and therefore, they are often excluded from data collection. Thus, the model created using this data would only be a close approximation of what is happening in the real world. This particular drawback is emphasised in the following aphorism;\n\n‚ÄúAll models are wrong, but some are useful‚Äù - George Box (2)\n\nThus even though our model is not the perfect one1, if it reliably predicts values then the model is considered useful. Thus a statistical model is stochastic.\nFrom a mathematical point of view, an ideal model would contain all the parameters that affect the response variable. Thus the sample space (S) will contain all possible combinations of [explanatory variables, stopping distance] values. Therefore, the set of probability distributions corresponding to all these combinations will be P(S). But in real life, we cannot afford to measure all the parameters concerning the response variable and are only interested in a small set of variables. Thus we often have a subset of the total possible combinations of [explanatory variables, stopping distance] values (\\theta), and the corresponding probability distribution for these combinations will be P(\\theta). The model thus created with a small set of parameters is called a parametrised statistical model.\nFor all such models; P(\\theta) \\subset P(S).\nThis definition is not exactly needed to do modelling but it‚Äôs good to know the actual mathematical definition of what a statistical model is.\nNow that we have a rough idea of what a model is, let us know learn how to build models in R."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#building-a-model",
    "href": "tutorials/stat_model/intro_stat_model.html#building-a-model",
    "title": "Introduction to statistical modelling in R",
    "section": "\n2 Building a model",
    "text": "2 Building a model\nTo make life easier, please install the following packages before hand (Not required but can be very helpful if you are following this article code by code).\n\n# Install packages\ninstall.packages('cherryblossom')\ninstall.packages('rpart')\ninstall.packages('datasets')\ninstall.packages('NHANES')\ninstall.packages('rpart.plot')\ninstall.packages('mosaicModel')\ninstall.packages('devtools')\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\n\nThe pipeline for building a model is as follows;\n\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\ngraph LR\n  A(Collect data) --> B(Select explanatory and response variables) --> C(Select model architecture) --> D(Build the model)\n  subgraph Pipeline for making a statistical model\n  A\n  B\n  C\n  D\n  end\n\n\n\n\n\n\n\n\nThe first step in building a model is by acquiring data. Then we need to select the appropriate response variable (dependent variable) and the explanatory variables (independent variable). In a cancer drug trial experiment data, tumour size can be the response variable and the drug type and patient age can be the explanatory variables. After selecting the variables, the model architecture is chosen. In our earlier example, we used a linear model to explain the changes seen in stopping distances with speed. Choosing a model architecture depends on the nature of the data. For now, we will mostly be using the linear model lm(). But throughout the tutorial, we will also see other model architectures. The final step is to build the model, which is done by the computer.\nThe syntax for building a model in R is as follows;\n\nfunction(response ~ explanaotry1 + explanatory2, data = dataset_name)\n\n\n2.1 Linear model\nLet us try to plot some models using the linear model architecture using the lm()function in R. We will be using the run17 dataset from the cherryblossom package in R. The run17 dataset contains details for all 19,961 runners in the 2017 Cherry Blossom Run, which is an annual road race that takes place in Washington, DC, USA. Also, the Cherry Blossom Run has two events; a 10 Mile marathon and a 5 Km run. For now, we will be concerned with participants that participated in the 10 Mile marathon only.\nIn the dataset, we are interested to check whether the net time to complete the 10 Mile marathon (net_sec) is affected by the age and sex of the participant. So for making the model, we use net_sec as the response variable and, age and sex as the explanatory variables.\nWe can use the summary() function to summarise the model. For now, we will not worry about the summary details. Then, using the ggplot2 package, we will plot the model using the stat_smooth() function. Please not that there is another function geom_smooth() which is an alias of stat_smooth(). Both do the same thing.\n\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Checking headers in the data\nhead(run17)\n\n\n\n  \n\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_1 <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Get the summary of the model\n# For now don't worry about the details\nsummary(model_1)\n\n\nCall:\nlm(formula = net_sec ~ age + sex, data = run17_marathon)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2814.89  -672.65   -41.43   625.34  3112.81 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5475.8377    25.8841  211.55   <2e-16 ***\nage           17.9157     0.6789   26.39   <2e-16 ***\nsexM        -674.3765    14.8290  -45.48   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 947.5 on 17439 degrees of freedom\nMultiple R-squared:  0.1229,    Adjusted R-squared:  0.1228 \nF-statistic:  1222 on 2 and 17439 DF,  p-value: < 2.2e-16\n\n# Plotting the model using ggplot2\n# Use stat_smooth() function and specify \"lm\"\n\nrun17_marathon %>% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex\") + \n  theme_bw()\n\n\n\n\nFrom the graph, it seems that female participants take more time to complete the run compared to their male counterparts.\n\n2.2 Logistic model\nNow let us try to see whether the participant‚Äôs choice in choosing between the events is affected by their age and sex. We can hypothesize that older participants of both sexes would prefer the 5 km run over 10 Mile marathon. Thus, we use the variable event as the response variable and, age and sex as the explanatory variables. We are more interested to see the effect of age as compared to sex, therefore the variable sex is considered a covariate. We will learn later what covariate mean\nWe will use the lm() function to make a linear model. Here we will convert the event variable to boolean values for it to work with the formula. The corresponding values are; 1 = 10 Mile event and 0 = 5 Km event.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Converting event variable values into boolean \nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Building the linear model\nmodel_2 <- lm(event ~ age + sex, data = run17_boolean)\n\n# Plotting the model using ggplot2\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n\nFrom the graph, it seems like, for male participants, age doesn‚Äôt affect their event choice selection. But for females, compared to males, age does seem to affect it. Therefore older female participants prefer 5 km runs as compared to older male participants.\nLet us look at the dataset more closely, especially the response variable.\n\n# Checking the data type of the response variable\nclass(run17$event)\n\n[1] \"character\"\n\n# Printing the unique strings from the response variable\nunique(run17$event)\n\n[1] \"10 Mile\" \"5K\"     \n\n\nOur response variable is dichotomous and thus not continuous. The lm() function we used works best for continuous numerical data. So the model architecture we used was not an appropriate one here. So in these situations, we can use the logistic modelling architecture. Both logistic modelling and linear modelling are part of generalised linear modelling. We can use the glm() function in R and specify family = binomial to have a logistic model.\nThe syntax for a logistic model in R is;\n\nglm(response_variable ~ explanatory_variable, data = dataset_name, family = \"binomial\")\n\nNow let us try the glm() function and make a logistic model for the earlier case.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Converting event variable values into boolean \nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Building the logistic model\nmodel_3 <- glm(event ~ sex + age, data = run17_boolean, family = \"binomial\")\n\n# Plotting the model using ggplot2\n# Use stat_smooth() function and specify \"glm\"\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()\n\n\n\n\nThe graph looks very similar to the earlier one. For easy comparison, both the graphs are shown next to each other below. Please use the slider to compare between the graphs.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cherryblossom)\n\nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Linear model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n# Logistic model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cherryblossom)\n\nrun17_boolean <- run17\nrun17_boolean$event <- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Linear model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n# Logistic model\nrun17_boolean %>% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#model-evaluation",
    "href": "tutorials/stat_model/intro_stat_model.html#model-evaluation",
    "title": "Introduction to statistical modelling in R",
    "section": "\n3 Model evaluation",
    "text": "3 Model evaluation\nAfter building the model, we can evaluate it by providing new inputs to the model to get corresponding predicted output values. The predicted output values can then be cross-checked against the original output values and see how far off they are in terms of prediction (i.e.¬†the prediction error). Using the predict() function, we can predict for values outside the dataset or evaluate the model using the prediction error.\n\n3.1 Prediciting values\nWe will go back to the first example where we checked if the net time to complete the 10 Mile marathon (net_sec) is affected by the age and sex of the participants. We will create dummy data of participants with random age and sex values and use the model to predict their net time to complete the race.\n\nlibrary(cherryblossom)\n\n# Creating dummy data of different ages\nmale <- data.frame(\"age\" = c(seq(1, 100, 2)),\n                   \"sex\" = c(replicate(50, \"M\")))\nfemale <- data.frame(\"age\" = c(seq(1, 100, 2)),\n                   \"sex\" = c(replicate(50, \"F\")))\n\ndummy_data <- rbind(male, female)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building the linear model\nmodel_lm <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Predicting values\ndummy_data$net_sec <- predict(model_lm, newdata = dummy_data)\n\n# Plotting the predicted values\ndummy_data %>% ggplot(aes(age, net_sec, col = sex)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 100, by = 5)) +\n  labs(x = \"Age\",\n       y = \"Time to complete the marathon (sec)\",\n       title = \"Predicted values\") + \n  theme_bw()\n\n\n\n\nIn the plot, we can see that we have babies as young as 1 year old, who have been predicted to have completed the 10 Mile marathon faster that their older participants. So what on earth did we do to get these results?\n\n\nMy reaction after the model predictions\n\n\nLet us look the linear model plot that we had earlier.\n\n\n\n\n\nYou can see that there is a general trend of increasing ‚Äònet time to complete the race‚Äô as the ‚Äòage‚Äô value increases and decreasing net time as age decreases. Therefore our linear model favours reduced net times for lesser values of age, which is what we see as predicted values.\nSo our model has its limitations in predicting values for a certain range of ageüòÖ.\nThe moral of the story here is that models trained on data can be a bit wild when evaluated outside the range of the data. So we have to be mindful of its prediction abilities, otherwise, we can end up with superhuman babies who can run marathons faster than anyone.\n\n\nIdeal relationship between age and the time to complete the marathon\n\n\n\n3.2 Evaluating a model\nInstead of predicting new values outside the dataset, we can use the same dataset used for model training to predict values. Utilizing these predicted values, we can compare them back to the original values and calculate the prediction error. This is one way to compare different models and see which models predict values closer to the original values in the dataset.\nWe will use our earlier made linear model, where we looked at whether the total time to complete the race is affected by age and sex. We will also make a new model using the ‚Äòrecursive partitioning model architecture‚Äô using the same dataset so that we can have a model to compare with. Using the rpart() function in the rpart package in R, we can build a recursive partitioning model. The rpart() model works for both numerical (dichotomous and discontinuous) and categorical data. We will learn more about rpart models later in this article.\n\nif (!require(rpart)) install.packages('rpart')\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\n\n# Checking headers in the data\nhead(run17)\n\n\n\n  \n\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_lm <- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Building a recursive partitioning model\nmodel_rpart <- rpart(net_sec ~ age + sex, data = run17_marathon)\n\n# Predicting values\nlm_predict <- predict(model_lm, newdata = run17_marathon)\nrpart_predict <- predict(model_rpart, newdata = run17_marathon)\n\n# Calculating error values\nlm_error <- with(run17_marathon, net_sec - lm_predict)\nrpart_error <- with(run17_marathon, net_sec - rpart_predict)\n\n# Printing few error values\nhead(as.data.frame(lm_error))\n\n\n\n  \n\n\nhead(as.data.frame(rpart_error))\n\n\n\n  \n\n\n\nNow we have data frames containing error values calculated between each of the original values in the dataset to the predicted values from the model. You can see that it‚Äôs tedious to compare the error values of the linear model to the logistic model, also some of these error values are negative, which makes the comparison even harder. So how can we know which model is better? Calculating the mean of the square of the prediction errors (m.s.e) would be a great way to start. The m.s.e will reflect the magnitude and not the sign of the errors.\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(lm_error ^ 2, na.rm = T)\n\n[1] 897630\n\nmean(rpart_error ^ 2, na.rm = T)\n\n[1] 905886.9\n\n\nThe linear model has a lower error value compared to the recursive partitioning model. Therefore the fitted values in the linear model are closer to the actual value in the dataset compare to the other model.\nWe can also plot the predicted values and see how they look. Please use the slider to compare the graphs.\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Plotting the linear model\nrun17_marathon %>% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n# Adding rpart predicted values to the dataset\nrpart_predict <- predict(model_rpart, newdata = run17_marathon)\nrun17_marathon$rpart_fitted_values <- rpart_predict\n\n# Plotting the recursive partitioning model\nrun17_marathon %>% ggplot(aes(age, rpart_predict, col = sex)) +\n  geom_line() +\n    labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (recursive partitioning model)\") + \n  theme_bw()\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon <- run17 %>% filter(event == \"10 Mile\")\n\n# Plotting the linear model\nrun17_marathon %>% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n# Adding rpart predicted values to the dataset\nrpart_predict <- predict(model_rpart, newdata = run17_marathon)\nrun17_marathon$rpart_fitted_values <- rpart_predict\n\n# Plotting the recursive partitioning model\nrun17_marathon %>% ggplot(aes(age, rpart_predict, col = sex)) +\n  geom_line() +\n    labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (recursive partitioning model)\") + \n  theme_bw()\n\n\n\n\n\n\n3.3 Choosing the explanatory variables\nIn this exercise, we will use the ToothGrowth dataset from the {datasets} package in R. The dataset is from an experiment that looked at the effect of vitamin C on tooth growth in guinea pigs. In the experiment, three different doses of vitamin C were administered to 60 guinea pigs. Vitamin C was administered either through orange juice or ascorbic acid. The length of odontoblasts (cells responsible for tooth growth) was measured to check for tooth growth.\nWe will be creating a linear model to check if tooth growth is affected by vitamin C dosages. At the same time, there is also a chance that tooth growth is affected by the method of vitamin C administration. Therefore, we have the choice of choosing between ‚Äòvitamin C dosage‚Äô and the ‚Äòmethod of vitamin C administration‚Äô as the explanatory variables. So how do we know which of these variables is better at predicting tooth growth values? To find the best predictor, we can first build a model using ‚Äòvitamin C dosage‚Äô as the only explanatory variable. Then later, we can build yet another model using both ‚Äòvitamin C dosage‚Äô and the ‚Äòmethod of vitamin C administration‚Äô as the explanatory variables. Then similiar to the ealrier case, we can use the mean of the square of the prediction errors can see which model is better.\nIn the ToothGrowth dataset, the variable len contains the length of the odontoblasts, supp contains the method of vitamin C administration (VC - ascorbic acid, OJ - Orange juice) and dose contains the dosage of vitamin C administered in mg/day units.\n\nif (!require(datasets)) install.packages('datasets')\nlibrary(datasets)\n\ndata(\"ToothGrowth\")\n\n# Building a linear model with only dose\nmodel_dose <- lm(len ~ dose, data = ToothGrowth)\n\n# Building a linear model with dose + supp\nmodel_dose_supp <- lm(len ~ dose + supp, data = ToothGrowth)\n\n# Predicting values using the trained dataset\npredict_dose <- predict(model_dose, newdata = ToothGrowth)\npredict_dose_supp <- predict(model_dose_supp, newdata = ToothGrowth)\n\n# Calculating error values\nerror_dose <- with(ToothGrowth, dose - predict_dose)\nerror_dose_supp <- with(ToothGrowth, dose - predict_dose_supp)\n\n# Calculate the mean of the square of the prediction errors\nmean(error_dose ^ 2, na.rm = T)\n\n[1] 341.2716\n\nmean(error_dose_supp ^ 2, na.rm = T)\n\n[1] 344.6941\n\n\nYou can see that the model with both vitamin C dosage and method of vitamin C administration as the explanatory variables has a greater error value compared to the model with only vitamin C dosage. Therefore adding the method of vitamin C administration as the explanatory variable did not improve the model and therefore can be excluded from the analysis.\n\n3.4 Cross validation\nSo far we have been using the training dataset for predicting values. But there is a problem in using it for our analysis as it allows models with the additional explanatory variable to have smaller prediction errors than the base model. Let‚Äôs see what this means in the following code.\nHere we use the chickwts dataset from the {datasets} package in R. In this dataset, there are two variables, weight which tells the weight (g) of the chick measured after 6 weeks and the feed variable, which tells the type of feed that was given to the chicks for those 6 weeks. There were 6 different types of feed used. The experiment was done to check whether the feed type has any effect on the weights of the chick.\nHere we will build a linear model with weight as the response variable and feed type as the explanatory variable. Then we will make a random variable within the dataset which contains values which have no explanatory power. It will be filled with random numbers which have no relationship with the original data. We will build yet another linear model like earlier but this time we shall use both the feed type and the random variable as the explanatory variable. Adding a random variable to the model should cause the model to perform poorly and therefore should lead to an increased m.s.e value when compared to the model with just the feed type variable as the explanatory variable. Let us see if that‚Äôs the case here.\n\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating a random variable\n# The variable contains random numbers\nset.seed(231)\nchickwts$random <- rnorm(nrow(chickwts))\n\n# Building a linear model with only feed\nmodel_feed <- lm(weight ~ feed, data = chickwts)\n\n# Building a linear model with dose + random\nmodel_feed_random <- lm(weight ~ feed + random, data = chickwts)\n\n# Predicting values using the trained dataset\npredict_feed <- predict(model_feed, newdata = chickwts)\npredict_feed_random <- predict(model_feed_random, newdata = chickwts)\n\n# Calculating error values\nerror_feed <- with(chickwts, weight - predict_feed)\nerror_feed_random <- with(chickwts, weight - predict_feed_random)\n\n# Calculate mean of the square of the prediction errors (m.s.e)\nmean(error_feed ^ 2, na.rm = T)\n\n[1] 2754.31\n\nmean(error_feed_random ^ 2, na.rm = T)\n\n[1] 2725.115\n\n\nWell, quite a surprise, right? The model with the random variable is having a lower error value as compared to the model with the correct explanatory variable. We will see later (not in this article though) why the error value was lower. But for now, keep in mind that when you use the same dataset to do both training and prediction in a model, the model with the additional explanatory variable will have smaller prediction errors than the base model, as seen here. Therefore this mistake can throw off our analysis and gives us a false positive that some of the explanatory variables form a relationship with the response variable when in reality there is no effect.\nTo mitigate this problem, we use a technique called ‚Äòcross-validation‚Äô. In this technique, we split the original data into two parts; a training set and a testing set. Both sets will have data points that are chosen at random from the original dataset. We train our model using the training set and then test our model using the testing set and then calculate the m.s.e. value. Thus the explanatory variable values in the testing set will be novel to the model. Let us see how we can do this.\nWe will be reusing our earlier example of chick weight and feed type.\nIn the code given below, the rnorm(nrow(chickwts)) > 0 function will assign TRUE and FALSE values at random to the row values. Looking at this function more closely, the rnorm() function will choose random numbers up to the number of rows in the dataset, and since there is an inequality (greater than zero), it will assign TRUE if the random number is greater than zero and vice versa. The row values with TRUE will go to the training set and rows with FALSE will form the testing set.\n\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating training set\nset.seed(231)\nchickwts$training_set <- rnorm(nrow(chickwts)) > 0\nchickwts$random <- rnorm(nrow(chickwts)) > 0\n\n# Building the linear model using the training set\nmodel_feed <- lm(weight ~ feed, data = subset(chickwts, training_set))\n\n# Building the linear model using the training set but with random variable\nmodel_feed_random <- lm(weight ~ feed + random, data = subset(chickwts, training_set))\n\n# Predicting values using the testing set\n# !training_set means row values with FALSE value\npredict_feed <- predict(model_feed, newdata = subset(chickwts, !training_set))\npredict_feed_random <- predict(model_feed_random, newdata = subset(chickwts, !training_set))\n\n# Calculating error values using the testing data\nerror_feed <- with(subset(chickwts, !training_set), weight - predict_feed)\nerror_feed_random <- with(subset(chickwts, !training_set), weight - predict_feed_random)\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(error_feed ^ 2, na.rm = T)\n\n[1] 2830.79\n\nmean(error_feed_random ^ 2, na.rm = T)\n\n[1] 2732.715\n\n\nFor the seed I have set, using the cross-validation method, we seem to not solve the problem we had earlier. The model with the random variable as the explanatory variable still has a lower error value as compared to the model without the random variable. But this was just an opportunistic case, as the training and testing sets are chosen at random. You might not get the same result as mine if you run this code (provided that the set.seed() is changed). We can deal with this randomness by rerunning the calculation many times to get a more consistent measure of the error value.\nWe will use the cv_pred_error() function from the statisticalModeling package to rerun the calculations many times. The function automatically makes the training and testing sets using the original dataset and also calculates the m.s.e for each trial. In the code given below, we store the results from the cv_pred_error() function into a variable called ‚Äòtrials‚Äô. The variable ‚Äòtrials‚Äô will have two columns in it; mse which denotes the mean of the square of the prediction errors (m.s.e) and model which denotes the name of the model given as input. Then in the final step, we compare the m.s.e values between the model using a simple t-test.\n\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\nlibrary(statisticalModeling)\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating a random variable\n# The variable contains random numbers\nset.seed(231)\nchickwts$random <- rnorm(nrow(chickwts)) > 0\n\n# Building a linear model with only feed\nmodel_feed <- lm(weight ~ feed, data = chickwts)\n\n# Building a linear model with dose + random\nmodel_feed_random <- lm(weight ~ feed + random, data = chickwts)\n\n# Rerunning the models (100 times for each model)\ntrials <- cv_pred_error(model_feed, model_feed_random, ntrials = 100)\n\n# Compare the two sets of cross-validated errors\nt.test(mse ~ model, data = trials)\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = -5.9019, df = 197.95, p-value = 1.537e-08\nalternative hypothesis: true difference in means between group model_feed and group model_feed_random is not equal to 0\n95 percent confidence interval:\n -118.53296  -59.16003\nsample estimates:\n       mean in group model_feed mean in group model_feed_random \n                       3309.232                        3398.079 \n\n\nFor \\alpha = 0.05 level of significance, we have a p-value < 0.05, which means that the mean error values between the models are not the same and are different from each other. From the t-test summary, we can see that the mean error value of the model without the random variable is lower than the model with the random variable (3309.232 < 3398.079). Therefore we can conclude that the addition of the random variable does not improve the model.\nTherefore through the cross-validation technique iterated over many times, in conjunction with the m.s.e values, we can identify which of the variables in our data should be considered as the explanatory variables.\n\n3.5 Prediction error for categorical response variable\nSo far when we were calculating the predictive error values, the response variable we had was numerical. But what if our response variable was a categorical value, then how will we compare models using the predictive error values?\nLet us go back to the run17 dataset from the cherryblossom package in R. In this dataset, we looked at whether the participants‚Äô choice of event was influenced by their age and sex. We hypothesised that the older participants of both sexes will prefer the 5 Km run as compared to the 10 Mile marathon. In the earlier example, we have used the logistic model, but here let us use the recursive partitioning model made using the rpart() function in the rpart package in R. We had learned briefly that the recursive partitioning model is appropriate when the response variable is categorical, which is the case here.\nLike earlier, we will use event as the response variable and sex and age as the explanatory variable. The hypothesis remains the same, irrespective of sex, older participants will prefer the 5 Km run as compared to the 10 Mile marathon. We will build two models, one with only age as the explanatory variable and the other with both age and sex as the explanatory variables.\nWhile predicting for values, we use type = \"class\" so that the model gives prediction values, which are either ‚Äú10 Mile‚Äù or ‚Äú5 Km‚Äù (gives prediction values as categorical values).\nThen we will evaluate these two models by comparing the m.s.e values. This will tell us whether adding the variable sex improves the model or not.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(rpart)\n\n# Creating training set\nset.seed(123)\nrun17$training_set <- rnorm(nrow(run17)) > 0\n\n# Training the model with the training set\n# Building the recursive partitioning model with just age\nmodel_rpart_age <- rpart(event ~ age, data = subset(run17, training_set))\n\n# Building the recursive partitioning model with age + sex\nmodel_rpart_age_sex <- rpart(event ~ age + sex, data = subset(run17, training_set))\n\n# Predicting values using the testing set\npredict_age <- predict(model_rpart_age,\n                       newdata = subset(run17, !training_set), type = \"class\")\npredict_age_sex <- predict(model_rpart_age_sex,\n                           newdata = subset(run17, !training_set), type = \"class\")\n\n# Printing a few row values\nhead(predict_age)\n\n      1       2       3       4       5       6 \n10 Mile 10 Mile 10 Mile 10 Mile 10 Mile 10 Mile \nLevels: 10 Mile 5K\n\nhead(predict_age_sex)\n\n      1       2       3       4       5       6 \n10 Mile 10 Mile 10 Mile 10 Mile 10 Mile 10 Mile \nLevels: 10 Mile 5K\n\nhead(run17$event)\n\n[1] \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\"\n\n\nIn the first 6 row values of the output, both models seem to agree with the values from the original dataset. But our dataset has 199961 rows of data. It would be crazy to even think that one can compare each of the row values between the models instead, we will try to quantify the error. In earlier cases, we could subtract the predicted values from the response variable values in the dataset to get the error value. But that is not possible here as the response variable is categorical.\nOne way to calculate the error values of these models is to see how many mistakes the model made. This can be calculated by checking how many times the predicted value by the model was not equal to the value in the dataset.\n\n# Calculating the sum of errors using the testing data\nwith(data = subset(run17, !training_set), sum(predict_age != event))\n\n[1] 1254\n\nwith(data = subset(run17, !training_set), sum(predict_age_sex != event))\n\n[1] 1254\n\n\nThe number of errors each model made is the same. Before we conclude anything let us try to see some more ways to quantify the error.\nInstead of sum, we can calculate the mean rate of errors also.\n\n# Calculating the mean of errors using the testing data\nwith(data = subset(run17, !training_set), mean(predict_age != event))\n\n[1] 0.1243061\n\nwith(data = subset(run17, !training_set), mean(predict_age_sex != event))\n\n[1] 0.1243061\n\n\nMean error values are the same (no surprises here).\nLet us go one step further. Till now our model predicted a deterministic value to the response variable. It can either be 10 Mile marathon or a 5 km run. Instead of this, we can use the model to predict the probability values to the categorical values present in the response variable. This means, our model will predict how likely a certain participant of a particular age and sex will choose between a 10 Mile marathon and a 5 Km run. To predict probability values, instead of type = \"class\", we will use type = \"prob\" within the predict() function.\n\n# Predicting probability values using the testing set\npredict_age_prob <- predict(model_rpart_age,\n                       newdata = subset(run17, !training_set), type = \"prob\")\npredict_age_sex_prob <- predict(model_rpart_age_sex,\n                           newdata = subset(run17, !training_set), type = \"prob\")\n\n# Comparing the predicted value to the actual value in the dataset\ntesting_data <- run17 %>% select(training_set, event) %>% filter(training_set == F)\ncompare_values <- data.frame(testing_data, predict_age_prob, predict_age_sex_prob)\n\n# Changing the column names for making sense of the column values\ncolnames(compare_values)[c(3:6)] <- c(\"Ten_Mile_age\", \"Five_km_age\", \"Ten_Mile_age_sex\", \"Five_km_age_sex\")\n\n# Printing a few row values\nhead(compare_values)\n\n\n\n  \n\n\n\nThe columns Ten_Mile_age and Five_km_age corresponds to probability values from the model_rpart_age model with just ‚Äòage‚Äô as the explanatory variable. The last two columns Ten_Mile_age_sex and Five_km_age_sex are from the model_rpart_age_sex model with both ‚Äòage‚Äô and ‚Äòsex‚Äô as the explanatory variables. The first-row value in the 10Mile_age column indicates that the model predicts a nearly 88% chance for that particular participant to be choosing the 10 Mile marathon. And with no surprise, you can see that the probability values across the models are the same (because we found that the error values are the same earlier).\nWe can condense these probability values to a single value which is called the ‚Äòlikelihood value‚Äô, which can then be used as a figure of merit to compare the models. This is similar to the mean of the square of the prediction errors (m.s.e) we had when the response variable was numerical. The likelihood values are calculated by multiplying the individual probability values. But since the probability values are in decimal values and are less than 1, multiplying them will lead to a very small value which would be difficult to compare. Therefore we first log transform our probability values and add them, which is mathematically equivalent to multiplying them before the log transformation\nSince the dataset is not ‚Äòtidy‚Äô, some codes are used to tidy it. The ‚Äú10 Mile‚Äù has a space in-between. While doing analysis, R might register ‚Äú10 Mile‚Äù as ‚Äú10‚Äù and ‚ÄúMile‚Äù. So this needs to be reformatted.\n\n# Splitting the data frame into two other data frames\n# Newly made data frames have values corresponding to the respective model\ncompare_values_age <- compare_values[,1:4]\ncompare_values_age_sex <- compare_values[,c(1,2,5,6)]\n\n# Tidying the data\ncompare_values_age$event <- recode(compare_values_age$event,\n                                   \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_km\")\ncompare_values_age_sex$event <- recode(compare_values_age_sex$event,\n                                       \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_km\")\n\n# Calculating the likelihood values\n# Ten_Mile_age and Five_km_age are column names\nlikelihood_age <- with(compare_values_age,\n                       ifelse(event == \"Ten_Mile\", Ten_Mile_age, Five_km_age))\nlikelihood_age_sex <- with(compare_values_age_sex,\n                           ifelse(event == \"Ten_Mile\", Ten_Mile_age_sex, Five_km_age_sex))\n\n# Likelihood value of model with age\nsum(log(likelihood_age)) \n\n[1] -3785.999\n\n# Likelihood value of model with age + sex\nsum(log(likelihood_age_sex)) \n\n[1] -3785.999\n\n\nAs the likelihood values are one and the same we can conclude that the variable ‚Äòsex‚Äô does not improve the model.\n\n3.6 Creating a null model\nThe null model gives the best estimate of our data when there are no explanatory variables modelled to it. The predicted value from the null model will always be a constant, no matter what testing data is provided. Let us see how to make a null model.\nWe will again use the run17 dataset from the cherryblossom package in R. To this dataset we add a new column which contains a constant value. We will make a model where net_sec is the response variable and the constant variable is the explanatory variable. We will use the recursive partitioning architecture to model the data.\n\nlibrary(cherryblossom)\nlibrary(rpart)\n\n# Creating a constant variable\nrun17$constant <- 10\n\n# Creating a null model\nnull_model <- rpart(net_sec ~ constant, data = run17)\n\n# Predicting values\n# Notice how all the values are the same\npredict_null <- predict(null_model, newdata = run17)\n\n# Prinitng a few predicted values\n# Notice how all the predicted values are the same\nhead(predict_null)\n\n[1] 5427.947 5427.947 5427.947 5427.947 5427.947 5427.947\n\n\nWe can calculate the mean of the square of the prediction errors of the null model. The null model essentially acts as a base of our model analysis, where we can compare the errors in the null model to the model of our interest.\n\nlibrary(cherryblossom)\nlibrary(rpart)\n\n# Creating a constant variable\nrun17$constant <- 10\n\n# Creating training set\nset.seed(12)\nrun17$training_set <- rnorm(nrow(run17)) > 0\n\n# Creating a null model using the training set\nnull_model <- rpart(net_sec ~ constant, data = subset(run17, training_set))\n\n# Predicting values using the testing set\npredict_null <- predict(null_model,\n                       newdata = subset(run17, !training_set))\n\n# Prinitng a few predicted values\nhead(predict_null)\n\n[1] 5436.2 5436.2 5436.2 5436.2 5436.2 5436.2\n\n# Calculating error values using testing set\nerror_null <- with(subset(run17, !training_set), net_sec - predict_null)\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(error_null ^ 2, na.rm = T)\n\n[1] 2273954\n\n# Calculating m.s.e by iterating the model 100 times\ntrials <- cv_pred_error(null_model)\n\n# Printing m.s.e values for each iteration\nhead(trials$mse)\n\n[1] 2289640 2289445 2289692 2289697 2289802"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#exploratory-modelling",
    "href": "tutorials/stat_model/intro_stat_model.html#exploratory-modelling",
    "title": "Introduction to statistical modelling in R",
    "section": "\n4 Exploratory modelling",
    "text": "4 Exploratory modelling\nTill now, we had a clear idea of the selection of the explanatory variables while making a model. But sometimes, you might just want to explore your dataset and see which variables affect your response variable without prior knowledge of it. Therefore, we can do exploratory modelling where we take a whole bunch of explanatory variables and see if any of them explain the changes seen in the response variable.\nFor this exercise, we will be using the NHANES dataset from the {NHANES} package in R. The dataset is a survey data collected by the US National Centre for Health Statistics (NCHS). A total of 75 variables concerning health are collected as data from around 7800 individuals in the US.\n\n4.1 Evaluating a recursive partitioning model\nFor our exploratory modelling, we will be using the recursive partitioning architecture. Using the dataset we will see what variables are related to depression. We will use the formula Depressed ~ .. The single period on the right side of the Tilda indicates that we want to model using all the possible variables in the dataset. Finally, we will plot the model as a ‚Äòtree‚Äô using the rpart.plot() function from the rpart.plot package in R.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(rpart.plot)) install.packages('rpart.plot')\nlibrary(NHANES)\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Building the recursive partitioning model\nmodel_rpart <- rpart(Depressed ~ . , data = NHANES)\n\n# Plotting the model with sample sizes\nrpart.plot(model_rpart, extra = 1, type = 4)\n\n\n\n\nWe got this nice-looking tree plot with lots of information. Let‚Äôs see how to interpret them.\nThe response variable that we used was the Depressed variable in the dataset. This variable has three levels.\n\nlevels(NHANES$Depressed)\n\n[1] \"None\"    \"Several\" \"Most\"   \n\n\nThe meaning of each of these levels is; ‚ÄúNone‚Äù = No sign of depression, ‚ÄúSeveral‚Äù = Individual was depressed for less than half of the survey period days, ‚ÄúMost‚Äù = Individual was depressed more than half of the days.\nThe colour code in the plot corresponds to the levels of the response variable. In the beginning node of the tree plot, you can see the label ‚ÄúNone‚Äù with three sets of numbers. The numbers correspond to the respective sample numbers of the levels, i.e.¬†at this node, 5246 individuals belong to ‚ÄúNone‚Äù, 1009 individuals belong to ‚ÄúSeveral‚Äù and 418 individuals belong to ‚ÄúMost‚Äù. Therefore we have data for a total of 6673 individuals (5256 + 1009 + 418 = 6673). Also, in this node, the majority of individuals belong to the level ‚ÄúNone‚Äù. Therefore the node is coloured by the respective colour code for ‚ÄúNone‚Äù, which is orange colour here. You can also see that the node colour changes its brightness to correspond to the difference between the majority value and the other values.\nWorking with numbers can be tricky, so let us represent the sample sizes in percentiles for easy comparisons.\n\n# Plotting the model with percentile values\nrpart.plot(model_rpart, extra = \"auto\", type = 4)\n\n\n\n\nNow instead of the actual sample sizes, we have percentile values. Now let us look at the plot. The beginning node which contains the whole set of depressed individuals in the dataset is further split into two groups and this split is caused by the variable LittleInterest. The variable LittleInterest in the dataset denotes the self-reported number of days where the participant had little interest in doing things. And like the Depressed variable, we have three levels for the variable LittleInterest.\n\nlevels(NHANES$LittleInterest)\n\n[1] \"None\"    \"Several\" \"Most\"   \n\n\nThe meaning of these levels is the same as explained for the Depressed variable.\nThe first group contains 77% of the total depressed individuals and they recorded zero days where they had reduced interest to do things. The rest of the 23% individuals belong to the second group which showed a severe reduction or most severe reduction in interest in doing things. This second group is further split into two by the variable DaysMentHlthBad which denotes the self-reported number of days the participant‚Äôs mental health was not good out of the past 30 days. Here, 13% of people in the second group with 23% of the total set had bad mental days less than 6. The remaining 11% of people (13% + 11% = ~ 23% of the second group) had bad mental days for more than 6 days, which further splits into two by their categories in the LittleInterest variable. There is no splitting of the group after this. This is because the recursive partitioning architecture stops at a point where further subdivisions don‚Äôt lead to a big change in predictive ability.\nSo as a summary, by this exploratory modelling exercise, we were able to determine potential variables that could act as the explanatory variables for a given response variable that we are interested in. Overall this led us to understand the relationships between the variables in the dataset that otherwise would not have been possible with a simple linear model. But also keep in mind that these potential variables are not indicating a cause and effect, but rather a simple relationship."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#covariate",
    "href": "tutorials/stat_model/intro_stat_model.html#covariate",
    "title": "Introduction to statistical modelling in R",
    "section": "\n5 Covariate",
    "text": "5 Covariate\nWe briefly learned what are covariates when we were looking at the run17 dataset from the cherryblossom package in R. In that exercise, we looked at whether the participant‚Äôs choice in choosing between the events is affected by their age and sex. We hypothesized that older participants of both sexes would prefer a 5 km run over the 10 Mile marathon. Therefore we considered age as the main explanatory variable and sex as a covariate.\nCovariates are those variables which are of no immediate interest to the user but hold the potential to explain changes seen in the response variable. There is no special distinction between a covariate and an explanatory variable in the formula syntax of the function R code. It‚Äôs just a mental label given by the user.\nLet us use the Tadpoles dataset from the {mosaicModel} package in R. The dataset contains the swimming speed of tadpoles as a function of the water temperature and the water temperature at which the tadpoles had been raised. Since size is a major determinant of speed, the tadpole‚Äôs length was measured as well. It was hypothesized that tadpoles would swim faster in the temperature of water close to that in which they had been raised.\nAs an exercise, let us see if the maximum swimming speed achieved by the tadpole is affected by the temperature at which they were raised. In addition, let also add the size of the tadpole as a covariate, as size could also affect the swimming speeds. The variable vmax denotes the maximum swimming speed (mm/sec) and therefore will be our response variable. The variable group denotes whether the tadpoles were raised in a cold environment (‚Äúc‚Äù) or warm environment (‚Äúw‚Äù) and length denotes the length of the tadpole (mm). Here both group and length will be our explanatory variables where length is also our covariate.\nWhile predicting values using the model, we can keep the covariate unchanged or constant, and predict different values for the other variable.\n\nif (!require(mosaicModel)) install.packages('mosaicModel')\nlibrary(mosaicModel)\n\ndata(Tadpoles)\n\n# Building a linear model\nmodel_vmax <- lm(vmax ~ group + length, data = Tadpoles)\n\n# Predicting for vmax\npredict_vmax <- predict(model_vmax, newdata = Tadpoles)\nhead(predict_vmax)\n\n       1        2        3        4        5        6 \n27.08018 27.11388 25.32803 27.31605 27.51822 26.57476 \n\n# Keeping length constant, predicting for vmax\npredict(model_vmax, newdata = data.frame(group = \"c\" ,length = 5))\n\n      1 \n26.2378 \n\npredict(model_vmax, newdata = data.frame(group = \"w\" ,length = 5))\n\n       1 \n25.25733 \n\n\nSo while keeping length constant, our model predicts change in swimming speeds between cold and warm environments the tadpoles were raised in."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#effect-sizes",
    "href": "tutorials/stat_model/intro_stat_model.html#effect-sizes",
    "title": "Introduction to statistical modelling in R",
    "section": "\n6 Effect sizes",
    "text": "6 Effect sizes\nEffect sizes are a great way to estimate the change in the output value, given the change in the input. Thus they are a great tool to analyse covariates in the model.\nWhile calculating the effect size, the units of it depend on both the response variable and the explanatory variable for which the effect size is measured.\nIf we have a response variable called ‚Äòsalary‚Äô (Dollars) and we have a numerical explanatory variable called the ‚Äòyears of education‚Äô (years), then the effect size of ‚Äòyears of education on the ‚Äôsalary‚Äô would be in the units of dollars/year. Suppose we get an effect size of 100 dollars/year, this means that for every unit increase in years of education, the salary will increase by 100 dollars.\nHere the effect size is calculated as a rate of change and its unit will be;\n\\begin{align*}\nUnit\\,of\\,effect\\,size = \\frac{Unit\\,of\\,response\\,variable}{Unit\\,of\\,explaratory\\,variable}\n\\end{align*}\nBut if our response variable is categorical, then the effect size is calculated as a difference. As categorical values have no units, the units of effect sizes here would be the same as that of the response variable.\nSuppose we have a response variable called ‚Äòrent‚Äô (Dollars) and we have a categorical explanatory variable called ‚Äòcity‚Äô. Let us say the variable ‚Äòcity‚Äô has two levels; Chicago and New York. The meaning of effect size values in this context would be the numerical difference in the response variable when input is changed from one category to another. If our effect size in moving from Chicago to New York is 500 Dollars, then this means that the rent increases by 500 when we move from Chicago to New York.\nHere the calculated effect size is will have the same unit as the response variable\nLook at the following exercise, we will use the Used_Fords dataset in the {mosaicModel} package in R. The dataset contains details about used Ford cars. We will see if the price of the cars (Price) has any relationship with both, how old the car is (Age) and the colour of the car (Color). We will build two linear models using Price as the response variable. But one model will have Age and the other will have Color as the explanatory variable. The units are; Price = USD and Age = years\nWe will calculate the effect sizes using the effect_size() function from the {statisticalModeling } package. The effect_size() function takes in two arguments; the model and a formula indicating which variable to vary when looking at the model output. The effect size to Age will be represented in dollars/year and the effect size to Colour will be represented as a difference in dollars when changing from one colour to another.\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Used_Fords)\n\n# Building a linear model with only age\nmodel_car_age <- lm(Price ~  Age, data = Used_Fords)\n\n# Calculating the effect sizes by varying age\neffect_size(model_car_age, ~ Age)\n\n\n\n  \n\n\n\nEffect size to Age is represented in the slope column. The value is -1124, which means for every unit increase in the age of the car (1-year increase), the price of the car depreciates by 1124 dollars.\n\n# Building a linear model with only colour\nmodel_car_age <- lm(Price ~  Color, data = Used_Fords)\n\n# Calculating the effect sizes by varying colour\neffect_size(model_car_age, ~ Color, Color = \"blue\", to = \"red\")\n\n\n\n  \n\n\n\nFor categorical explanatory variables, the effect_size() function automatically takes appropriate levels. However, we can manually change this behaviour. From the given code, the effect size to colour is calculated when the colour of the car changes from blue to red. Here the effect size is represented in the change column. The value is -3290. This means that the price of the car reduces by 3290 dollars when the colour of the car changes from blue to red."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#conclusion",
    "href": "tutorials/stat_model/intro_stat_model.html#conclusion",
    "title": "Introduction to statistical modelling in R",
    "section": "\n7 Conclusion",
    "text": "7 Conclusion\nI hope this article provided you with a good learning experience. In a nutshell, we learned;\n\n\nWhat is the meaning of a statistical model\n\nModel functions: lm(), glm() and rpart()\n\nModel formula: response ~ formula\n\nModel architectures: linear, logistic, recursive partitioning\n\n\n\nHow to build a model\n\nBuilding models using functions, formulas and model architectures\nPlotting the model output\n\n\n\nHow to evaluate a model\n\nUsing the model to predict values outside the data\nCalculating the mean of the square of the prediction errors\nUsing error to compare models to aid explanatory variable selection\nCross-validation technique and model iteration by cv_pred_error() function\nPrediction error for a categorical response variable\n\n\nHow to build a null model\n\nHow to do exploratory modelling\n\nEvaluation of a recursive partitioning model and plotting it\n\n\nWhat is a covariate\n\nHow to calculate effect size\n\nFor numerical explanatory variables\nFor categorical explanatory variables\n\n\n\nHappy learning, one day you will make it üëç\nLast updated on\n\n\n[1] \"2022-08-05 11:19:40 IST\""
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorial posts",
    "section": "",
    "text": "ggplot2\n\n\ndata visualization\n\n\n\nLearn how to plot different types of graphs using the ggplot2 package\n\n\n\nJewel Johnson\n\n\nDec 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the aesthetics, labels and axes of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the theme and the colour palette of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggpubr\n\n\ndata visualization\n\n\n\nLearn how to make publication ready plots and visualize results of statistical tests directly on the plot using the ggpubr package\n\n\n\nJewel Johnson\n\n\nJan 6, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMaster the art of data visualization using the {ggplot2} package in R"
  },
  {
    "objectID": "tutorials.html#data-manipulation-using-r",
    "href": "tutorials.html#data-manipulation-using-r",
    "title": "Tutorial posts",
    "section": "2 Data manipulation using R",
    "text": "2 Data manipulation using R\n\n\n\n\n\n\n\n\n\n\nChapter 1: Data tidying using tidyr\n\n\n\ntidyr\n\n\ndata wrangling\n\n\n\nLearn how to make your data tidy with the tidyr package\n\n\n\nJewel Johnson\n\n\nDec 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Data manipulation using dplyr (part 1)\n\n\n\ndplyr\n\n\ndata wrangling\n\n\n\nLearn how to manipulate your data with the dplyr package\n\n\n\nJewel Johnson\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Data manipulation using dplyr (part 2)\n\n\n\ndplyr\n\n\ndata wrangling\n\n\n\nLearn how to manipulate your data with the dplyr package\n\n\n\nJewel Johnson, Jewel Johnson\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n\nTidy your data using powerful cleaning tools provided by the {tidyr} and {dplyr} packages in R"
  },
  {
    "objectID": "tutorials.html#basic-statistics-using-r",
    "href": "tutorials.html#basic-statistics-using-r",
    "title": "Tutorial posts",
    "section": "3 Basic statistics using R",
    "text": "3 Basic statistics using R\n\n\n\n\n\n\n\n\n\n\nChapter 1: Introduction to statistics with R\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn the basics of statistics and how to do them with R\n\n\n\nJewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Distribution and variance\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about normal, bimodal, skewed distributions and the variance\n\n\n\nJewel Johnson, Jewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Standard error and standard deviation\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about standard error and standard deviation, and get to know the standard normal distribution\n\n\n\nJewel Johnson, Jewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4: p-value and level of significance\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about p-value, level of significance, misconceptions behind p-value\n\n\n\nJewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5: Type I and Type II errors\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about type I and type II errors\n\n\n\nJewel Johnson, Jewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6: Confidence interval and effect size\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about confidence interval and effect size\n\n\n\nJewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7: Parametric tests and assumptions\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about parametric tests and assumptions\n\n\n\nJewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8: Non-parametric test and assumptions\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about non-parametric test and assumptions\n\n\n\nJewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 9: Tests for multiple groups of data\n\n\n\nstatistics\n\n\nbasic statistics\n\n\n\nLearn about ANOVA and its non-paramteric derivatives\n\n\n\nJewel Johnson, Jewel Johnson\n\n\nMay 21, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nI am currently remaking the posts, so these posts are subjected to change"
  },
  {
    "objectID": "tutorials.html#statisitcal-modelling-using-r",
    "href": "tutorials.html#statisitcal-modelling-using-r",
    "title": "Tutorial posts",
    "section": "4 Statisitcal modelling using R",
    "text": "4 Statisitcal modelling using R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntroduction to statistical modelling in R\n\n\n\nstat modelling\n\n\n\nLearn how to build a model, predict values using it, evaluating it and plotting the model output in R\n\n\n\nJewel Johnson\n\n\nAug 4, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nGuess t-test wasn‚Äôt good enough for you?"
  }
]