{
  "hash": "0b5e0ebc873faa93694c0dc660ddbbfb",
  "result": {
    "markdown": "---\ntitle: \"Generalized linear models in R\"\ndescription: \"Go beyond linear models and extend them to analyse non-normal datasets\"\ndate: \"08/23/2022\"\nformat:\n  html:\n    css:\n      - https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css\nimage: images/stat_model_3.png\ncategories: [statistical modelling]\n---\n\n```{ojs}\n//| output: false\n//| echo: false\nrequire(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() => null)\n```\n\n\n:::{.callout-note}\n## TL;DR\n\nIn this article you will learn;\n\n1.    Understanding the coefficients in a linear model\n\n2.    Why do linear models fail for some datasets and what are the limitations of a linear model\n\n3.    Two types of GLMs: Poisson and Logistic\n\n4.    Link functions\n\n5.    Understanding the coefficients in a Poisson and logistic models\n\n6.    Plotting Poisson and logistic models\n\n7.    Brief introduction to multiple regression using GLMs\n\n8.    Assumptions of GLMs: When to use GLMs\n\n:::\n\n\n## Prologue\n\nThis is the third tutorial in the series: Statistical modelling using R. In this tutorial we will work with datasets that fail the assumptions of linear models. We will first see what these assumptions are and then we will learn about generalised linear models which is an extension of the linear model architecture. So let's go!\n\n## Making life easier\n\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the following lines of code\n\n# Packages used in this tutorial\ntutorial_packages <- rlang::quos(Stat2Data, modelsummary, devtools, COUNT, broom,\n                                 cherryblossom, datasets)\n\n# Install required packages\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  install.packages,\n  character.only = TRUE\n)\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading the libraries\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  library,\n  character.only = TRUE\n)\n\n# Datasets used in this tutorial\ndata(\"HorsePrices\")\ndata(\"rwm1984\")\ndata(\"run17\")\ndata(\"mtcars\")\n```\n:::\n\n\n## Understanding the coeffecients in a linear model\n\nTo quickly recap, a linear model tries to explain the variability seen in the response variable by estimating the coefficients for explanatory variables. These coefficients are nothing but your effect size or the slope of the explanatory variable calculated from the model. Consider the following example.\n\nWe will use the `HorsePrices` dataset from the `{Stat2Data}` package in R. We will model the price of the horses in this dataset to their height and age. We will use the `summary()` function to output the summary of the linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm <- lm(Price ~ Age + Sex, data = HorsePrices)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"Price ~ Age + Height\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Getting the summary of the linear model\nsummary(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Age + Sex, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24988 -10296  -1494   8286  27653 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18199.7     4129.3   4.407 6.03e-05 ***\nAge           -220.1      393.8  -0.559    0.579    \nSexm         17008.6     3639.6   4.673 2.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12540 on 47 degrees of freedom\nMultiple R-squared:  0.3283,\tAdjusted R-squared:  0.2997 \nF-statistic: 11.48 on 2 and 47 DF,  p-value: 8.695e-05\n```\n:::\n\n```{.r .cell-code}\n# Calculating the effect sizes\neffect_size(model_lm, ~ Age)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"slope\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"to:Age\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sex\"],\"name\":[4],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"-220.0847\",\"2\":\"6\",\"3\":\"10.57398\",\"4\":\"m\",\"_rn_\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nIn the summary of the model, in the coefficients section, the estimate column is nothing but effect sizes. Compare the estimate for 'age' in the model summary and the effect size of 'age' calculated from the `effect_size()` function, both are the same.\n\nRecall that the equation for a straight line is;\n\n\n$$ y = mx+c $$\n\nWhere $m$ is the slope of the line and $c$ is the y-intercept.\n\nWe can see from the graph that there are two straight lines, therefore there will be two equations for the straight lines. Imagine $y_1$ to be the equation of the straight line for females (the red line) and similarly, $y_2$ is for males (the blue line). The equations for the straight lines are;\n\n\n$$ y_1 = m_1x_1 + c_1 \\\\\n y_2 = m_2x_2 + c_2$$\n\nFrom the graph, you can see that the slopes for both the lines are the same. This means that the effect size of age is the same across both the sex. Also, the slope should be a negative value because as age increases, price is decreasing for both sexes. From the model summary, the slope value for age is -220.10. Thus we have;\n\n\n$$m_1 = m_2 = -220.1$$\n\n\nNow recall how I explained that the effect size for a categorical exploratory variable is expressed in terms of [change](https://one-carat-blog.netlify.app/tutorials/stat_model/intro_stat_model.html#effect-sizes) and that change is in terms of their [y-intercepts](https://one-carat-blog.netlify.app/tutorials/stat_model/inter_stat_model.html#interactions-among-explanatory-variables).\n\nNow R calculates intercepts in two ways. The default way or the first way is what is shown in the model summary. In the (Intercept) row, the estimated value is 18199.7 and this value is the y-intercept value for the straight line for 'female'. You can easily verify this from the red line in the graph. Notice how the y-intercepts are the same as shown in the model summary and the plot. Now in the row 'Sexm', the estimated value shows how much difference is there from the estimated value shown in the (Intercept) row or otherwise how much difference in y-intercept is there between females and males. Since the difference is positive this means the y-intercept for males is 17008.6 higher than the y-intercept of females. Thus we have;\n\n\n$$c_1 = 18199.7\\\\c_2 - c_1 = 17008.6$$\n\n\nThe intercept value by default is shown for the first level in the exploratory variable which here is female. \n\nThe second way is to individually show the corresponding y-intercept values for males and females. For that, we can use the expression `-1` in the model formula.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building a linear model with -1 expression\nmodel_lm <- lm(Price ~ Age + Sex - 1, data = HorsePrices)\n\n# Getting the summary of the linear model\nsummary(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Age + Sex - 1, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24988 -10296  -1494   8286  27653 \n\nCoefficients:\n     Estimate Std. Error t value Pr(>|t|)    \nAge    -220.1      393.8  -0.559    0.579    \nSexf  18199.7     4129.3   4.407 6.03e-05 ***\nSexm  35208.2     3497.7  10.066 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12540 on 47 degrees of freedom\nMultiple R-squared:  0.8429,\tAdjusted R-squared:  0.8329 \nF-statistic: 84.05 on 3 and 47 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nNow you get individual y-intercept values for all the levels in the data. There will be slight decimal differences while adding up the values but the concept is the same.\n\n\n$$c_1 = 18199.7\\\\c_2 = 35208.2 = 17008.6 + 18199.7$$\n\n\nSo the first way compares differences between groups by taking the first group as the reference and in a second way, we can see the actual change in the respective group.\n\n## Limitations of linear models\n\nFour assumptions are required to be satisfied to be able to do linear modelling;\n\n1.    The values in the datasets are normally distributed\n2.    The residuals are normally distributed \n3.    The values in the datasets are continuous\n4.    Variables form a linear relationship with each other\n\nMany datasets fail these assumptions. Survival data with binary responses or data with count values are both non-normal and discontinuous. Linear modelling won't do any good in analysing these datasets. So what is the solution? The solution is to extend linear models and generalize them so that they can accept non-normal distributions. In R, the process is simple and we have seen it being used in logistic regression in tutorial 1 using the `glm()` function. Therefore `glm()` function calls for the generalized model in R and the type of model is specified by the distribution of the data. If the data is count type then we can use the `Poisson` family distribution and if the data is binomial then use the `binomial` family distribution. We will see both of these in detail further in the tutorial. The process of generalizing a linear model is through 'non-linear link functions' which link the regression coefficients to the distribution and allow the linear model to be generalised. We will learn more about this in the later sections.\n\nThe syntax for the `glm()` function is as follows;\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(formula, data, family = \"\")\n```\n:::\n\nFor the Poisson regression model we can specify `family = \"poisson\"` and for the binomial regression model we can specify `family = \"binomial\"`. \n\nSince the `lm()` function is only applicable for normal or Gaussian distributions; \n\n`glm(formula, data, family = \"gaussian\")` = `lm(formula, data)` \n\nTherefore a linear model is a generalized linear model with the Gaussian family.\n\n## Poisson regression model\n\nAs mentioned earlier, the Poisson model is used if the dataset contains count data. Count data can be no of things sold in a shop, no people with cancer in a city or no of visitors in a shop. Intuitively, we can say that count data ranges from 0 to infinity. There is no negative or decimal count data. Thus your dataset should only contain 0 or positive integer values for the Poisson model to work. Also, the values in the dataset should have been obtained in the same manner. For example: let's say your dataset contains count data for the no. of people coming to the shop per day, then the same dataset should not contain values for no. of people coming to the shop per week, as they are sampled differently in terms of time. Lastly, if your dataset has a variance greater than the mean or if you have lots of zeros in your data then some modifications have to be done to be able to use the Poisson regression model.\n\nIn short, you should not use the Poisson regression model if you have the following conditions;\n\n1.    Dataset contains negative and/or non-integer values\n2.    Dataset is sampled across in a different manner (no. of people per day vs. no. of people per week)\n3.    Dataset has a variance greater than its mean (Over-dispersed data)\n4.    Dataset has lots of zero values (Zero-inflated data)\n\nLet us build a Poisson model. We will use the `rwm1984` dataset from the `{COUNT}` package in R. The dataset is a German health registry for the year 1984. We are interested in seeing whether the number of visits to the doctor during a year (`docvis`) is associated with age (`age`). Since we have count data we will build a Poisson model to see this association.\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(COUNT)) install.packages('COUNT')\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model output\nsummary(model_poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = docvis ~ age, family = \"poisson\", data = rwm1984)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.229  -2.263  -1.271   0.326  26.383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.0983153  0.0399295  -2.462   0.0138 *  \nage          0.0273395  0.0008204  33.325   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25791  on 3873  degrees of freedom\nResidual deviance: 24655  on 3872  degrees of freedom\nAIC: 31742\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\nApart from the `summary()` function, we also use the `tidy()` function from the `{broom}` package in R to print the outputs of the model as a data frame.\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(broom)) install.packages('broom')\nlibrary(COUNT)\nlibrary(broom)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model output using tidy()\ntidy(model_poisson)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"-0.09831529\",\"3\":\"0.0399294797\",\"4\":\"-2.462223\",\"5\":\"1.380787e-02\"},{\"1\":\"age\",\"2\":\"0.02733953\",\"3\":\"0.0008203971\",\"4\":\"33.324750\",\"5\":\"1.691649e-243\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nSometimes we are interested in extracting the coefficients or finding out the confidence intervals from the model.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Extracting the coefficients\ncoef(model_poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         age \n-0.09831529  0.02733953 \n```\n:::\n\n```{.r .cell-code}\n# Estimating the confidence interval\nconfint(model_poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %      97.5 %\n(Intercept) -0.1767936 -0.02027057\nage          0.0257330  0.02894895\n```\n:::\n:::\n\n## Logistic regression model\n\nIn [tutorial 1](https://one-carat-blog.netlify.app/tutorials/stat_model/intro_stat_model.html#logistic-model), we briefly saw what logistic regression was. We use the logistic model when;\n\n1.    Data is binary (0/1)\n2.    Data has only two mutually exclusive values (Alive/Dead or Yes/No or Win/Lose or Pass/Fail)\n3.    Data has only two values choices or behaviour (Non-veg/Veg, Wet/Dry)\n\nIn short, for a logistic regression model, the model outputs probabilities ($p$) for binary outcomes in the response variable ($y$).\n\n\n$$y = Binomial(p)$$\n\n\nLet us build a logistic model using `run17` dataset from the `{cherryblossom}` package in R. We have seen this dataset in the first tutorial. The dataset contains details for all 19,961 runners in the 2017 Cherry Blossom Run, which is an annual road race that takes place in Washington, DC, USA. The Cherry Blossom Run has two events; a 10 Mile marathon and a 5 Km run. We will be building a logistic model to whether event choice is predicted by the participant's age.   \n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(dplyr)\nlibrary(cherryblossom)\n\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model\nmodel_binomial <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Printing model output\nsummary(model_binomial)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = event ~ age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1471   0.4963   0.5102   0.5285   0.6241  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.250155   0.074991  30.006  < 2e-16 ***\nage         -0.008390   0.001897  -4.423 9.74e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15130  on 19959  degrees of freedom\nResidual deviance: 15111  on 19958  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 15115\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n### Bernouli and binomial distributions\n\nBoth Bernoulli and binomial distributions form the basis for logistic regression. The main difference between these two distributions is that Bernoulli is associated with probabilities of a result in a single event whereas binomial is associated with probabilities of a result in multiple events. For example; the chance of getting a head in a single coin toss follows a Bernoulli distribution and the chance of getting a head in 10 coin tosses follows a binomial distribution.\n\nIn R we have the option to either use binomial or Bernoulli. So how do we know when to use which? The short answer is that it depends on the data structure. For wide format data, the binomial distribution is used and for long format data, the Bernoulli distribution is used. You can learn more about long format and wide format data [here](https://one-carat-blog.netlify.app/tutorials/data_man/project5.html#reshaping-dataset).\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Long format\nhead(run17_tidy)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"event\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"age\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Ten_Mile\",\"2\":\"21\"},{\"1\":\"Ten_Mile\",\"2\":\"22\"},{\"1\":\"Ten_Mile\",\"2\":\"31\"},{\"1\":\"Ten_Mile\",\"2\":\"33\"},{\"1\":\"Ten_Mile\",\"2\":\"35\"},{\"1\":\"Ten_Mile\",\"2\":\"33\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Converting the dataset to wide format\nrun17_wide <- run17_tidy %>% group_by(age, event) %>%\n  summarise(count = n())\n\nrun17_wide <- run17_wide %>% pivot_wider(names_from = \"event\", values_from = \"count\")\nrun17_wide[is.na(run17_wide)] <- 0\n\n# Wide format\nhead(run17_wide)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"age\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Five_Km\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Ten_Mile\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"6\",\"2\":\"4\",\"3\":\"0\"},{\"1\":\"7\",\"2\":\"2\",\"3\":\"0\"},{\"1\":\"8\",\"2\":\"5\",\"3\":\"0\"},{\"1\":\"9\",\"2\":\"8\",\"3\":\"0\"},{\"1\":\"10\",\"2\":\"9\",\"3\":\"0\"},{\"1\":\"11\",\"2\":\"14\",\"3\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nIn the above case, for long format data, we have a single entry in each row corresponding to individual data but in the wide format, we have each row entry corresponding to a single age group. Another way of expressing the data in a wide format is to calculate the percentage of choices, which is by dividing the total number of choices for the first category by the total number of choices for both categories.\n\nSo for wide format data; If our data structure has the absolute value of the no. of choices made then we use `cbind()` in the model formula. If we have percentages, then we use `weights = ()` in the `glm()` function.\n\nSo in summary we choose between Bernoulli and binomial by checking the following questions;\n\n1.    Is the data in long or wide format?\n2.    Do we have individual or group data?\n3.    Are we interested in individuals or groups?\n\nLet us build logistic models for both long format and wide format data.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with long format\nmodel_logistic_long <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Converting the dataset to wide format\nrun17_tidy_1 <- run17_tidy %>% mutate(event_choice = ifelse(event == \"Ten_Mile\", 1, 0))\n\nrun17_wide <- run17_tidy_1 %>% group_by(age, event) %>%\n  summarise(count = n())\n\nrun17_wide <- run17_wide %>% pivot_wider(names_from = \"event\", values_from = \"count\")\nrun17_wide[is.na(run17_wide)] <- 0\n\n# Building a logistic model with wide format\n# We use cbind() to bind the two choices\nmodel_logistic_wide_1 <- glm(cbind(Five_Km, Ten_Mile) ~ age,\n                             data = run17_wide, family = \"binomial\")\n\n# Building a logistic model with wide format\n# We use percentage of choice and the weight or number of observations per group\nrun17_wide$prect_ten_mile <- run17_wide$Ten_Mile / (run17_wide$Ten_Mile + run17_wide$Five_Km)\n\nmodel_logistic_wide_2 <- glm(prect_ten_mile ~ age, \n                             data = run17_wide, family = \"binomial\",\n                             weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\n# Printing model outputs\nsummary(model_logistic_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = event ~ age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1471   0.4963   0.5102   0.5285   0.6241  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.250155   0.074991  30.006  < 2e-16 ***\nage         -0.008390   0.001897  -4.423 9.74e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15130  on 19959  degrees of freedom\nResidual deviance: 15111  on 19958  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 15115\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n\n```{.r .cell-code}\nsummary(model_logistic_wide_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = cbind(Five_Km, Ten_Mile) ~ age, family = \"binomial\", \n    data = run17_wide)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9381  -0.6116   0.4472   1.7556   7.5007  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.245070   0.074961 -29.950  < 2e-16 ***\nage          0.008267   0.001897   4.358 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 411.90  on 79  degrees of freedom\nResidual deviance: 393.17  on 78  degrees of freedom\nAIC: 711.65\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n\n```{.r .cell-code}\nsummary(model_logistic_wide_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = prect_ten_mile ~ age, family = \"binomial\", data = run17_wide, \n    weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-7.5007  -1.7556  -0.4472   0.6116   2.9381  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.245070   0.074961  29.950  < 2e-16 ***\nage         -0.008267   0.001897  -4.358 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 411.90  on 79  degrees of freedom\nResidual deviance: 393.17  on 78  degrees of freedom\nAIC: 711.65\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\nPlease note that all three models give the same coefficients but the model with the long format structure has higher degrees of freedom as compared to the other two models with the wide format.\n\n## Link functions\n\nThe 'link function' is what enables us to generalize a linear model so that we can use it in datasets which do not meet the assumptions of a linear model. In a way, it is a function which transforms the coefficients of the explanatory variables to form a linear combination with each other like in the case of a linear regression model.\n\n*   For `poisson` family models, the link function is `log` and thus the coefficients of the model are in log-scale.\n*   For 'binomial' family models, there are two link functions; `logit` and `probit`\n\nThe logit link function transforms the probabilities to 'log-odds values' which are real numbers ranging from $-\\infty$ to $+\\infty$ and gives rise to a linear equation similar to what is seen in a linear regression model. In logistic regression, by default, the `logit` link function is used.\n\n\n$$logit(p) = m_1x_1 + c_1 = log(\\frac{p}{1-p})$$\n\nThe probit link function is also similar to the logit link but is based on the cumulative standard normal distribution function. By default, the logistic regression in the `glm()` function uses the logit link function. \n\nTo use another link function, we should specify the link function in the family; \n\n`family = \"binomial\"` = `family = binomial(link = \"probit\")`: By default, logit link function is used\n\n`family = binomial(link = \"probit\")`: Using the probit link function\n\nLook at the graphs given below. Use the slider to compare them. You can see that the model with the logit link function has a slightly longer or fatter tail, as both the tail ends reaches the limit slower as compared to the model with the probit link function. Therefore, in general, the logit link function is better at modelling outliers or rare events as compared to probit.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(datasets)\nlibrary(statisticalModeling)\n\n# Building a logistic model with logit link which is the default\nmtcars_logit <- glm(vs ~ mpg, data=mtcars, family= \"binomial\")\n\n# Building a logistic model with probit link which is the default\nmtcars_probit <- glm(vs ~ mpg, data=mtcars, family=binomial(link=\"probit\"))\n\n# Plotting logistic model with logit link\nfmodel(mtcars_logit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with logit link)\")\n\n# Plotting logistic model with probit link\nfmodel(mtcars_probit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with probit link)\")\n```\n:::\n\n::: {.juxtapose data-startingposition=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets)\nlibrary(statisticalModeling)\n\n# Building a logistic model with logit link which is the default\nmtcars_logit <- glm(vs ~ mpg, data=mtcars, family= \"binomial\")\n\n# Building a logistic model with probit link which is the default\nmtcars_probit <- glm(vs ~ mpg, data=mtcars, family=binomial(link=\"probit\"))\n\n# Plotting logistic model with logit link\nfmodel(mtcars_logit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with logit link)\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plotting logistic model with probit link\nfmodel(mtcars_probit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with probit link)\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n:::\n\n## Simulating logit and probit\n\nWe can simulate a logit distribution using the `plogis()` and the `rbinom()` functions together. The `rbinom()` is a random number generator function which has additional input parameters; \n\n`n` = Number of random numbers to generate\n`size` = Number of trails\n`p` = Probability of success\n\n::: {.cell}\n\n```{.r .cell-code}\n# Converting a logit scale value to probability value\np <- plogis(2)\n\n# Simulating a logit distribution\nrbinom(n = 10, size = 1, prob = p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 0 1\n```\n:::\n:::\n\nA probit distribution can be made using the `pnrom()` and the `rbinom()` functions together.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Converting a probit scale value to probability value\np <- pnorm(2)\n\n# Simulating a probit distribution\nrbinom(n = 10, size = 1, prob = p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n## Understanding the coefficients in a Poisson model\n\nThe link function for the Poisson regression model is the log function. Now we will see how to interpret the coefficients/effect sizes of a Poisson regression model. Let us quickly recall how we interpreted the coefficients/effect sizes of a linear regression model. We will use our earlier example but with only 'Sex' as the exploratory variable.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm <- lm(Price ~ Sex, data = HorsePrices)\n\n# Getting the summary of the linear model\nsummary(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Sex, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-23730 -10061  -1255   8495  28495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    16505       2783   5.931 3.20e-07 ***\nSexm           17225       3593   4.794 1.62e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12450 on 48 degrees of freedom\nMultiple R-squared:  0.3238,\tAdjusted R-squared:  0.3097 \nF-statistic: 22.98 on 1 and 48 DF,  p-value: 1.619e-05\n```\n:::\n:::\n\nIn algebraic form, the model formula for a linear model is;\n\n\n$$y \\sim \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon$$\n\nFrom the model summary, we have;\n\n\n$$y = Price\\\\ \\beta_0 = 16505 = Reference\\,intercept\\,(female)\\\\ \\beta_1 = 17225 = Difference\\,from\\,the\\,reference\\,intercept\\,(for\\,male)\\\\ \\epsilon = Error\\,value\\,of\\,the\\,model$$\n\n\nWe also have;\n\n\n$$\\beta_0 + \\beta_1 = Average\\,price\\,of\\,male\\,horses$$\n\nThis shows that the coefficient in a linear model is additive. But when we come to the Poisson model, the link function is a log function which takes in the parameter $\\lambda$ of the Poisson distribution. We have;\n\n\n$$Parameter\\,of\\,poisson\\,distribution:\\lambda = e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon}\\\\ Link\\,function: ln(\\lambda) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon$$\n\n\nHere the coefficients are no longer additive as they are in log scale. Therefore the coefficients are multiplicative. Let us look at the Poisson regression model we created earlier;  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model outputs\nsummary(model_poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = docvis ~ age, family = \"poisson\", data = rwm1984)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.229  -2.263  -1.271   0.326  26.383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.0983153  0.0399295  -2.462   0.0138 *  \nage          0.0273395  0.0008204  33.325   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25791  on 3873  degrees of freedom\nResidual deviance: 24655  on 3872  degrees of freedom\nAIC: 31742\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\nHere we have;\n\n\n$$y = No.\\,of\\,days\\,of\\,doctor\\,visits\\\\ \\beta_0 = -0.0983153 = Reference\\,intercept\\\\ \\beta_1 = 0.0273395 = Difference\\,from\\,the\\,reference\\,intercept\\\\ \\epsilon = Error\\,value\\,of\\,the\\,model$$\n\n\nNow to find the average effect of age on the no. of days of doctor visits, we have to multiply $\\beta_0$ and $\\beta_1$ instead adding the values like in the case of a linear model.\n\n\n$$\\beta_0 * \\beta_1 = ln(average\\,effect\\,of\\,age\\,on\\,the\\,no.\\,of\\,days\\,of\\,doctor\\,visits)\\\\ e^{\\beta_0 + \\beta_1} = average\\,effect\\,of\\,age\\,on\\,the\\,no.\\,of\\,days\\,of\\,doctor\\,visits$$\n\n\nBut if we exponentiate the coefficients, then we get the raw value, which we can add like in the case of a linear model.\n\n## Plotting a Poisson model\n\nYou can either use the `fmodel()` function from the `{statisticalModeling}` package or use the `ggplot()` function from the `{ggplot2}` package.\n\nFor the `ggplot()` function, we specify 'poisson' model in the `method.args = list(family = \" \")` parameter inside `geom_smooth()` argument.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(COUNT)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson <- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Plotting using the fmodel() function\nfmodel(model_poisson) + geom_point(data = rwm1984) +\n  theme_bw() +\n  labs(title = \"docvis ~ age (poisson model)\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plotting using the ggplot() function\nggplot(data = rwm1984, aes(age, docvis)) +\n  geom_jitter(width = 0.05, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = 'poisson')) +\n  theme_bw() +\n  labs(title = \"docvis ~ age (poisson model)\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n:::\n\n## Understanding the coefficients in a logistic model\n\nIn a linear model, the coefficients were straight forward and in the Poisson model, after exponentiating the coefficients, the values were similar to that of the linear model. But for a logistic model transformation won't help and it's not as straightforward as compared to the other models. So instead, the coefficient values are converted to odds ratio. The odds ratio calculates the relative odds of two events occurring. For example, let us imagine a football team that is expected to win 10 games for every game they lose. Then the odds of winning the odds of losing would be 10 to 1. And the odds ratio would be 10/1 which is 10.\n\nAs mentioned earlier, the default link function for a logistic model is logit, so our coefficients start as 'log-odds' which when exponentiated will give us the odds ratio.\n\nIn general, odds-ratio values are interpreted in the following way;\n\n*   OR = 1: Coefficient has no effect\n*   OR < 1: Coefficient decreased the odds\n*   OR > 1: Coefficient increases the odds\n\nImagine a football team is singing two players; A and B. Odds-ratio of winning is 3 if player A joins the team, which means, in that season, if player A joins, the team is bound to win 3 games for every game they lose. Likewise, if player B joins let's say the odds ratio is 0.5, which means they will win 1 game for every 2 games they lose. The odds ratio is often reported along the 95% confidence intervals.\n\nLet us calculate the odds ratio and the confidence interval of the model we previously created. We will use the `tidy()` function from `{broom}` package in R.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(broom)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model\nmodel_binomial <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Finding the odds ratio and confidence interval\ntidy(model_binomial, exponentiate = T, conf.int = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"9.489204\",\"3\":\"0.074990636\",\"4\":\"30.005809\",\"5\":\"8.242298e-198\",\"6\":\"8.1938689\",\"7\":\"10.9941637\"},{\"1\":\"age\",\"2\":\"0.991645\",\"3\":\"0.001897033\",\"4\":\"-4.422763\",\"5\":\"9.744683e-06\",\"6\":\"0.9879755\",\"7\":\"0.9953503\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThe odds ratio of age to the event is ~ 0.9916 which is almost 1. This suggests that when age increases by 1 year, there 50% chance that the participant chooses 5Km over 10 Mile event run (the odds are 1 to 1). So basically age does not seem to have an association with event choice. Also note that for the 95% confidence interval, the upper bound almost includes 1 which further reinforces the notion that age is not associated with the event choice.\n\n## Plotting a logistic model\n\nLike earlier, we can either use `fmodel()` function from the `{statisticalModeling}` package or use the `ggplot()` function from the `{ggplot2}` package.\n\nFor the `ggplot()` function, we specify the 'binomial' model in the `method.args = list(family = \" \")` parameter inside `geom_smooth()` argument. Converting the event to numeric form and subtracting 1 from it will make the choice values stay between zero and one. In the ggplot2 graph, the value 1 corresponds to the 10 Mile run and the value 0 corresponds to the 5 Km run.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with logit link\nmodel_logistic <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the logistic model with link function using fmodel()\nfmodel(model_logistic) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plotting the logistic model with link function using ggplot()\nggplot(run17_tidy, aes(age, as.numeric(event) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = \"binomial\")) +\n  theme_bw() + labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n:::\n\nWe can also plot for different link functions in the logistic model.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17 %>% dplyr::select(event, age)\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with logit link\nmodel_logistic_long <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the logistic model with logit and probit link function using ggplot()\n# Same graph as the earlier one\nggplot(run17_tidy, aes(age, as.numeric(event) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'logit')),\n              colour = \"red\") +\n  geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'probit')),\n              colour = \"green\") + \n  theme_bw() + labs(title = \"event ~ age (logistic model; red: logit, green: probit)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\nAs you can see the lines corresponding to logit and probit link functions are very similar.\n\n## Multiple regression with GLMs\n\nAs seen with linear models, GLMs are also affected by collinearity. Therefore, the order of the explanatory variables in the model formula matters for GLMs, if there exists a correlation between the explanatory variables used. To check the correlation between two variables we can use the `cor()` function.\n\nLet us look at this in action using the logistic model we created earlier, but this time we will also add the `pace_sec` variable which tells the average time (in seconds) to complete a mile. \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Finding the correlation between pace_sec and age\n# Remove any NAs prior to finding the correlation\nrun17_tidy <- drop_na(run17_tidy)\ncor(run17_tidy$pace_sec, run17_tidy$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1620674\n```\n:::\n\n```{.r .cell-code}\n# Building a logistic model wiht pace_sec + age\nmodel_logistic_1 <- glm(event ~ pace_sec + age, data = run17_tidy, family = \"binomial\")\n\n# Building a logistic model wiht age + pace_sec\nmodel_logistic_2 <- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Printing the results\nsummary(model_logistic_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = event ~ pace_sec + age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2235   0.1997   0.3203   0.4770   1.2549  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  8.4756604  0.1606570  52.756  < 2e-16 ***\npace_sec    -0.0107900  0.0002218 -48.650  < 2e-16 ***\nage          0.0154362  0.0022611   6.827 8.69e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15118  on 19956  degrees of freedom\nResidual deviance: 11404  on 19954  degrees of freedom\nAIC: 11410\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n\n```{.r .cell-code}\nsummary(model_logistic_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = event ~ age + pace_sec, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2235   0.1997   0.3203   0.4770   1.2549  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  8.4756604  0.1606570  52.756  < 2e-16 ***\nage          0.0154362  0.0022611   6.827 8.69e-12 ***\npace_sec    -0.0107900  0.0002218 -48.650  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15118  on 19956  degrees of freedom\nResidual deviance: 11404  on 19954  degrees of freedom\nAIC: 11410\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\nYou can see that the model summary is identical, even with different ordering of the exploratory variables. This is because the correlation coefficient is ~ 0.17 which means there is a weak correlation between the used exploratory variables. So before building a model it's a good practice to check if the exploratory variables exhibit collinearity.\n\n## Assumptions of GLMs\n\nGLMs extend the linear model via link function which transforms the coefficient into a linear combination which can be interpreted in a similar way to that seen in linear models. Nevertheless, there are important assumptions which are to be met by the data to apply any GLM.\n\n1.    **Simpson's paradox does not occur**\n\nSimpson's paradox occurs when we fail to include an exploratory variable which significantly changes the model output after its addition. Consider the graphs given below. Please use the slider to compare them.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with age\nmodel_logistic_1 <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_1) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n# Building a logistic model with age + pace_sec\nmodel_logistic_2 <- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_2) + theme_bw() +\n  labs(title = \"event ~ age + pace_sec (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n```\n:::\n\n::: {.juxtapose data-startingposition=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy <- run17\nrun17_tidy$event <- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event <- as.factor(run17_tidy$event)\n\n# Building a logistic model with age\nmodel_logistic_1 <- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_1) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Building a logistic model with age + pace_sec\nmodel_logistic_2 <- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_2) + theme_bw() +\n  labs(title = \"event ~ age + pace_sec (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n```\n\n::: {.cell-output-display}\n![](glm_stat_model_files/figure-html/unnamed-chunk-24-2.png){width=672}\n:::\n:::\n\n:::\n\nIn the first model, predicting event choice with just age suggests that with increasing age, there is less chance that people will participate in the 10 Mile marathon. But when you also add the pace of participants into the model formula then you get completely different results (and absurd too!). The second model indicates that participants with a high pace (shorter y-axis values in the graph) irrespective of age will almost always choose the 10 Mile marathon. So our interpretation is completely changed and this is an example of Simpson's paradox.\n\n2.    **Variables follow linear relationship and are monotonic**\n\nLike in the case of linear models, GLMs also require that the variables follow a linear change. Monotonic means that the change is either always increasing or decreasing.\n\n3.    **Variables should independent**\n\nThe effect sizes of one variable should not influence the effect size of another variable. We have seen this in detail for linear models. If they are independent, then we have to represent them as interaction terms. In this case, the order also matters while inputting them into the model formula.\n\n4.    **Over dispersion**\n\nOverdispersion occurs when the datasets have a lot of zeros or ones in the case of the binomial distribution and a lot of zeros in the case of the Poisson distribution. Also if the variance is greater than the mean of the data, then again the model is over-dispersed.\n\n## Conclusion\n\nThis marks the end of the introduction to generalised linear models. In summary, we learned;\n\n1.    Understanding the coefficients in a linear model\n2.    Why do linear models fail for some datasets and what are the limitations of a linear model\n3.    Two types of GLMs: Poisson and Logistic\n4.    Link functions\n5.    Understanding the coefficients in a Poisson and logistic models\n6.    Plotting Poisson and logistic models\n7.    Brief introduction to multiple regression using GLMs\n8.    Assumptions of GLMs: When to use GLMs\n\nIn the next tutorial, we will take our understanding of GLMs to the next level and learn about 'Hierarchical and Mixed Effects Models in R'. See you then!\n\n#### Last updated on {.unnumbered .unlisted .appendix}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2022-08-23 13:25:19 IST\"\n```\n:::\n:::\n\n<!-- Do not forget to put flag counter -->\n<a hidden href=\"https://info.flagcounter.com/ynrK\"><img src=\"https://s11.flagcounter.com/count2/ynrK/bg_000000/txt_FFFFFF/border_F0F0F0/columns_5/maxflags_25/viewers_0/labels_1/pageviews_1/flags_0/percent_0/\" alt=\"Flag Counter\" border=\"0\"/></a>",
    "supporting": [
      "glm_stat_model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}