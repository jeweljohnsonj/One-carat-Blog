{
  "hash": "c2b52488c4a187c39029ae95efac9057",
  "result": {
    "markdown": "---\ntitle: \"Introductory statistics with R\"\ndescription: \"Learn the basics of statistics using R\"\ndate: \"08/31/2022\"\ndate-modified: last-modified\nformat:\n  html:\n    css:\n      - https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css\nimage: images/intro_stat.png\ncategories: [basic statistics]\nbibliography: intro_stat.bib\nfilters:\n   - social-share\nshare:\n  permalink: \"https://one-carat-blog.netlify.app/tutorials/stat_basic/intro_stat.html\"\n  description: \"Introductory statistics with R\"\n  twitter: true\n  facebook: true\n  reddit: true\n  stumble: true\n  tumblr: true\n  linkedin: true\n  email: true\n---\n\n\n:::{.callout-note}\n## TL;DR\n\nIn this article you will learn;\n\n1.    Descriptive and Inferential statistics\n\n2.    Measures of centre: mean, median and mode\n\n3.    Measures of spread: variance, standard deviation, mean absolute deviation and interquartile range\n\n4.    Distributions: binomial, normal, standard normal, Poisson, exponential, student's t and log-normal\n\n5.    Central limit theorem\n\n6.    Data transformation: Skewness, Q-Q plot and data transformation functions\n\n:::\n\n## Introduction\n\nThis tutorial will cover the basics of statistics and help you to utilize those concepts in R. Most of the topics that I will be discussing here is from what I learned from my undergraduate course: **Biological Data Analysis**, which was offered in 2019 at IISER-TVM. Then by reading online and by taking courses at DataCamp I further continued my journey to master statistics on my own. If you find any mistakes in any of the concepts that I have discussed in this tutorial, kindly raise it as a GitHub issue or comment them down or you can even mail me (the mail address is given in the footnote). Without further ado let us start!\n\n## Why do we need statistics?\n\nIf you are a student and are currently pursuing an undergrad course in college or a university, then chances are that you would come across scientific publications in your respective fields. And while you are reading them, for most of the articles, there will be a separate section highlighting the statistical methods used in the published study. So what was the purpose of it? One of the most important purpose statistical tests fulfil is that it gives evidence to test if the results given in the study occurred due to pure chance alone or due to the experiment that the authors of the paper did.\n\n## Descriptive and Inferential statistics\n\nStatistics is broadly categorized into two: *Descriptive statistics* and *Inferential statistics*\n\n*Descriptive statistics*: Descriptive statistics are used to describe and summarise data. Suppose we have data on the food preferences of a bird in a small area of an island. We found that out of the 100 birds we observed, 20 of them prefer fruits (20%), 30 of them prefer insects (30%) and the remaining 50 prefer reptiles (50%). So with the available data, this is what we know about the birds we have observed.\n\n*Inferential statistics*: Inferential statistics, as suggested by the name is used to make inferences about the population using the sample collected from that population. Using the above data, we can make inferences about the food preferences of all the birds on the island. Thus, we can find what is the percentage of birds on that island that would be preferring reptiles to eat.\n\n## A note on data\n\nData can be broadly categorised into two: *Quantitative* and *Qualitative*. Quantitative data are numerical values and can be continuous or discrete. They can also be arranged in a defined order (as they are numbers) and are thus called ordinal data. Qualitative data are often categorical data which can be either ordinal or nominal.\n\n| Examples of Quantitative data | Examples of Qualitative data |\n|---|---|\n| Speed of train | Survival data: Alive or Dead |\n| Age of a person | Outcomes: Win or Lose |\n| Proportion of visits to a shop | Choices: Kannur or Pune |\n| Change in prices of an item | Marital status: Married or Unmarried |\n| Growth of bacteria | Ordered choices: Agree, Somewhat agree, Disagree |\n\n: Examples of quantitative and qualitative data\n\n## Measures of centre\n\nA good way to summarise data is by looking at their measure of the centre which can be *mean*, *median* and *mode*. I am sure that you all know how to find these measures. \n\n*   Mean is best used to describe data that are normally distributed or don't have any outliers. \n\n*   Median is best for data with non-normal distribution as it is not affected much by outliers in the data. For a median value of 10, what it means is that 50% of our data is above the value of 10 and 50% of the remaining data is below the value of 10. \n\n*   Mode is best used if our data have a lot of repeated values and is best used for categorical data as for calculating mode, the data does not need to be in ordinal scale.\n\nLet us visualize the measure of centres.\n\nWe will use the `penguins` dataset from the `{palmerpenguins}` package in R. Let's plot the distribution curve for the \"body mass\" of the \"Chinstrap\" species of penguins. In R, function for calculating mean is `mean()` and for median is `median()`. There is no base function to calculate the mode, so we will write a function in R which can calculate the mode value.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nif (!require(palmerpenguins)) install.packages('palmerpenguins')\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Creating a function to calculate the mode value\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\n# Calculating the mean, median and mode\npen_avg <- penguins %>% filter(species == \"Chinstrap\") %>% summarise(mean = mean(body_mass_g),\n                                                          median = median(body_mass_g),\n                                                          mode = getmode(body_mass_g))\n\n# Plotting the data\npenguins %>% filter(species == \"Chinstrap\") %>% \n  ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density(fill = \"grey\") + \n  labs(subtitle = paste0(\"N=\",penguins %>% filter(species == \"Chinstrap\") %>% nrow())) +\n  geom_vline(aes(xintercept = pen_avg$mean, colour = \"red\")) +\n  geom_text(aes(x=pen_avg$mean, label=\"mean\", y=4e-04), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = pen_avg$median, colour = \"blue\")) +\n  geom_text(aes(x=pen_avg$median, label=\"median\", y=4e-04), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = pen_avg$mode, colour = \"green\")) +\n  geom_text(aes(x=pen_avg$mode, label=\"mode\", y=4e-04), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nFor a normal distribution, which is what we have in this case, the mean, median and mode are all the same value and are in the middle of the curve. For a skewed dataset or a dataset with outliers, all the measures of the centre will be different and they depend on the skewness of the data.\n\nConsider the plot of the `diamonds` dataset from the `{ggplot2}` package in R. The data is skewed in nature.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\n\n# Create function to get mode\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\n# Plotting the data\ndiamonds %>% filter(cut == \"Fair\") %>%\n  ggplot(aes(x = carat)) + \n  xlab(\"carats\") + ylab(\"Density\") + ggtitle(\"Diamond carat distribution\") + \n  geom_density(fill = \"gold\") + \n  labs(subtitle = paste0(\"N=\",diamonds %>% filter(cut == \"Fair\") %>% nrow())) +\n  geom_vline(aes(xintercept = mean(diamonds$carat), colour = \"red\")) +\n  geom_text(aes(x=mean(diamonds$carat), label=\"mean\", y=1), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = median(diamonds$carat), colour = \"blue\")) +\n  geom_text(aes(x=median(diamonds$carat), label=\"median\", y=1), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = getmode(diamonds$carat), colour = \"green\")) +\n  geom_text(aes(x=getmode(diamonds$carat), label=\"mode\", y=1), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nHere the curve is right-skewed, as the mean is skewed towards the right side of the mode value. If it was the other way around then the curve will be called left-skewed.\n\n![Measures of centre in relation to skeweness](images/centre_measure.png)\n\n## Measures of spread\n\nLike the measures of the centre, we can also summarise the data by measuring the spread of the data. Spread tell us how close or how far each of the data points is distributed in the dataset. There are many methods to measure the spread of the data, let us look at each of them one by one.\n\n### Variance\n\nThe variance of data is defined as the average squared difference from the mean of the data. It tells us how far each of our data points is from the mean value.\n\nThe formula for finding the variance is as follows;\n\n$$s = \\sigma^2 = \\frac{\\sum (x_{i} - \\bar{x})^{2}}{n - 1}$$\n\nWhere $s$ is `sample variance`, $x_{i}$ is your data point, $\\bar{x}$ is mean, $n$ is the sample size and $n-1$ is called as the degrees of freedom. Here $\\sigma$ is the standard deviation which is explained below.\n\nThe function to calculate variance in R is `var()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n\n# Calculating the variance of bill length\nvar(penguins$bill_length_mm, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 29.80705\n```\n:::\n:::\n\n\n\n### Standard deviation\n\nStandard deviation like the variance tells us how much each of our data points is spread around the mean and is easier to understand as they are not being squared like in the case with variance.\n\nThe formula to calculate the standard deviation of the mean is;\n\n$$\\sigma = \\sqrt{\\frac{\\sum (x_{i} - \\bar{x})^{2}}{n - 1}}$$ \n\nWhere $\\sigma$ is the standard deviation, $x_{i}$ is your data point, $\\bar{x}$ is the sample mean, $n$ is the sample size and $n-1$ is called the degrees of freedom. Here $\\sigma^2$ gives the variance of the data.\n\nThe function to calculate variance in R is `sd()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n\n# Calculating the variance of bill length\nsd(penguins$bill_length_mm, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.459584\n```\n:::\n\n```{.r .cell-code}\n# Calculating the variance\n(sd(penguins$bill_length_mm, na.rm = T))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 29.80705\n```\n:::\n:::\n\n\n### Mean absolute deviation\n\nMean absolute deviation takes the absolute value of the distances to the mean and then takes the mean of those differences. While this is similar to standard deviation, it's not the same. Standard deviation squares distances, so longer distances are penalized more than shorter ones, while mean absolute deviation penalizes each distance equally. \n\nThe formula to calculate mean absolute deviation is;\n\n$$MAD = (\\frac{1}{n})\\sum_{i=1}^{n}\\left | x_{i} - \\bar{x} \\right |$$\n\nWhere $n$ is the sample size, $x_{i}$ is your data point, $\\bar{x}$ is the sample mean, $n$ is the sample size and $n-1$ is called the degrees of freedom. Here $\\sigma^2$ gives the variance of the data.\n\nTo calculate absolute deviation in R, there is no base function to do so, so we have to manually calculate it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n\n# Calculating the distances\ndistances <- penguins$bill_length_mm - mean(penguins$bill_length_mm, na.rm = T)\n\n# Calculating the mean absolute deviation\nmean(abs(distances), na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.706797\n```\n:::\n:::\n\n\n### Interquartile range (IQR)\n\nQuantiles of data can be calculated using the `quantile()` function in R. By default, the data is split into four equal parts which is why it's called a 'quartile'.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n\n# Calculating the quartile\nquantile(penguins$bill_length_mm, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    0%    25%    50%    75%   100% \n32.100 39.225 44.450 48.500 59.600 \n```\n:::\n:::\n\n\nHere the data is split into four equal parts which are called the quartiles of the data. In the output, we can see that 25% of the data points are between 32.1 and 39.225, and another 25% of the data points are between 39.225 and 44.450 and so on. Here the 50% quartile is 44.450 which is the median value.\n\nWe can manually specify the splitting. Below given code splits the data into five parts and hence is called a quantile. The splitting is specified by the argument `probs` inside the `quantile()` function. You can either manually put the proportions of the split or use the `seq()` function to provide the proportions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n\n# Calculating the quantile\nquantile(penguins$bill_length_mm, probs =  seq(0, 1, 0.2), na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   0%   20%   40%   60%   80%  100% \n32.10 38.34 42.00 46.00 49.38 59.60 \n```\n:::\n:::\n\n\n> IQR is the distance between the second quartile and the third quartile of the data or the height of the box plot.\n\nThe interquartile range can be calculated using the `IQR()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Finding the IQR\nIQR(penguins$bill_length_mm, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.275\n```\n:::\n:::\n\n\nThe interquartile range is overall the best way to summarise the spread of the data and forms the crux of a boxplot design.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n\n# Plotting a boxplot\nggplot(penguins, aes(species, body_mass_g, fill = species)) + \n  geom_boxplot() +\n  labs(title = \"Body masses of three different species of penguins\",\n       x = \"Penguin species\",\n       y = \"Body mass (g)\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nIn the box plot, the middle dark line is the median or the 50% quartile or the second quartile, the bottom of the box is the first quartile (25%) and the top of the box is the third quartile (75%). You can see two data points outside the box for 'Chinstrap' species, these are outliers. What makes a data point an outlier? That's where the whiskers of the box come in. Typically the outliers are calculated in the following format;\n\n*\tOutliers: $x_i < Q_1 - 1.5 * IQR$ or $x_i > Q_3 + 1.5 * IQR$\n \nHere $Q_1$ is the first quartile, $Q_3$ is the third quartile and $IQR$ is the interquartile distance.\n\n![Characteristics of a boxplot](images/boxplot.png)\n\n## Distributions\n\nWe will come across different types of distributions while analysing data. Let us look at each of them.\n\n### Binomial distribution\n\nThe binomial distribution describes the probability of the number of successes in a sequence of independent trials. You might have seen this type of distribution in your introductory probability classes. It is easy to visualize this using a coin toss event. Imagine we have a fair coin, we are tossing it to see the number of times we get heads. So here getting a head is a success and getting a tail is a failure.\n\nIn R we can simulate this using the `rbinom()` function.  \n\nWe want to see the results when we are tossing a coin once. Since the functions take random values, to be concise I will set a seed so that you can repeat the codes that I have given to get the same results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(123)\n\n# Tossing a coin one time\nrbinom(1,1,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nWe got a value of zero, which corresponds to the event of getting a tail. Here, in the `rbinom()` function, we have the following syntax;\n\n`rbinom(no. of flips, no. of coins, the probability of getting a head)`\n\nTo see the number of heads we get by flipping a single coin three times would be with equal chances of getting a head and a tail is;\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(123)\n\n# Tossing a one coins three times\nrbinom(3,1,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 1 0\n```\n:::\n:::\n\n\nIn the first flip we got zero heads, in the second we got one head and in the third, we got zero head.\n\nChecking to see how total number of heads we get by flipping three coins one time;\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(258)\n\n# Tossing a three coins one time\nrbinom(1,3,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nSo we get a total of 2 heads when we flipped three coins one time.\n\nChecking to see the total number of heads we get by flipping four coins three times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(258)\n\n# Tossing a four coins three times\nrbinom(3,4,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3 3 2\n```\n:::\n:::\n\n\nIn the first flip of three coins, we got three heads, in the second we got again three heads and lastly, we got 2 heads.\n\nWe can also calculate the results if our coin is biased.\n\nChecking to see the total number of heads we get by flipping four coins three times, the coin only has a 25% probability of falling on heads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(258)\n\n# Tossing a four coins three times, but coins is unfair\nrbinom(3,4,0.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 2 1\n```\n:::\n:::\n\n\nHence the `rbinom()` function is used to sample from a binomial distribution.\n\nWhat about finding discrete probabilities like are chance of getting 7 heads if we flipped a coin 10 times? To find that we use the function `dbinom()`.\n\nChecking the probability of getting exactly 7 heads while tossing a coin 10 times;\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(258)\n\n# Probability of getting 7 heads in 10 coin flips\n# dbinom(no. of heads, no. of flips, chance of getting a head)\ndbinom(7,10,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1171875\n```\n:::\n:::\n\n\nLikewise, we can also find the probability of getting 7 or fewer heads while tossing the 10 times using the `pbinom()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(258)\n\n# Probability of getting 7 heads in 10 coin flips\n# pbinom(no. of heads, no. of flips, chance of getting a head)\npbinom(7,10,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9453125\n```\n:::\n:::\n\n\nTo find the probability of getting more than 7 heads in 10 trials would be;\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed\nset.seed(258)\n\n# Probability of getting more than 7 heads in 10 coin flips\n# Set lower.tail = FALSE\npbinom(7,10,0.5, lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0546875\n```\n:::\n\n```{.r .cell-code}\n# Alternatively you find it in the following also\n1 - pbinom(7,10,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0546875\n```\n:::\n:::\n\n\nThe expected value of an event would be equal to; $E = n*p$\n\nWhere $n$ is the number of trails and $p$ is the probability of successes.\n\nNow let us visualize the binomial distribution using the function we have covered till now;\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a binomial distribution\ndata.frame(heads = 0:10, \n           pdf = dbinom(0:10, 10, prob = 0.5)) %>%\nggplot(aes(x = factor(heads), y = pdf)) +\n  geom_col() +\n  geom_text(\n    aes(label = round(pdf,2), y = pdf + 0.01),\n    position = position_dodge(0.9),\n    size = 3,\n    vjust = 0\n  ) +\n  labs(title = \"Probability of X = x successes.\",\n       subtitle = \"Binomial distribution (n = 10, p = 0.5)\",\n       x = \"No of heads (successes)\",\n       y = \"Probability\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nThis is a binomial distribution showing the probability of getting heads when a single coin is flipped 10 times. For a fair coin, we get the expected probability that most times we will get 5 heads when the coin is flipped 10 times.\n\n### Normal distribution\n\nWe saw an example of the normal distribution when we were discussing the measures of the centre. The normal distribution or also known as the Gaussian distribution is one of the most important distributions in statistics as it is one of the requirements the data has to fulfil for numerous statistical analyses.\n\nLet us look at the `penguins` dataset from the `{palmerpenguins}` package in R. We will plot the distribution curve for the \"body mass\" of the \"Chinstrap\" species of penguins.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Plotting a normal distribution\npenguins %>% filter(species == \"Chinstrap\") %>% ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Chinstrap penguins\") + \n  geom_density(fill = \"darkred\") + \n  labs(subtitle = paste0(\"N=\", penguins %>%\n                           filter(species == \"Chinstrap\") %>% nrow())) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nAs you can see, the data distribution closely resembles a \"bell-shaped\" curve. On closer look, you can also see that the area under the curve is almost symmetrical to both sides and there is only a single peak present. We can also visualize the *variance* of the data by looking at the width of the curve. A wider curve means variance is higher and a narrower curve means variance in the data is smaller. Also for a normal distribution, there are no outliers present. We saw earlier that the mean, median and mode for a normal distribution are the same.\n\nAs seen earlier, we can use the `rnorm()` function to sample from the normal distribution. The syntax for the function is as follows;\n\n`rnorm(number of samples, mean, sd)`\n\nMany real-life datasets closely resemble a normal distribution, like the one which is plotted above. Since they approximate a normal distribution, we can use different base functions in R to answer some interesting questions.\n\n*   What percentage of Chinstrap penguins are below 3500g? To get this answer we approximate the body mass distribution of Chinstrap penguins to a normal distribution and use the `pnrom()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins below 3500g\npnorm(3500, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2721009\n```\n:::\n:::\n\n\nSo there are about 27% of penguins who have their body mass below 3500g\n\n*   What percentage of Chinstrap penguins are above 4000g? To get this answer we again use the `pnrom()` function but use the `lower.tail = F` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins above 4000g\npnorm(4000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass,\n      lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2436916\n```\n:::\n:::\n\n\nThere are about 24% of penguins in the dataset have a body mass above 4000g.\n\n*   What percentage of Chinstrap penguins have their body masses between 3000g and 4000g? To find this answer, we first find the percentage of penguins who have masses below 3000g and then below 4000g. Then we subtract them to get our answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins between 3000g and 4000g\nbelow_3000 <- pnorm(3000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\nbelow_4000 <- pnorm(4000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n\nbelow_4000 - below_3000\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7280752\n```\n:::\n:::\n\n\nAbout 73% of penguins have body masses between 3000g and 4000g.\n\n*   At what body mass is 60% of the penguins weigh lower than? We can get the answer using the `qnorm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Body mass at which 60% of the penguins weigh lower than\nqnorm(0.6, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3830.458\n```\n:::\n:::\n\n\nWe find that 60% of the penguins weigh lower than 3830g.\n\n*   At what body mass is 30% of the penguins weigh greater than? We can get the answer using the `qnorm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary <- penguins %>% filter(species == \"Chinstrap\") %>%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Body mass at which 30% of the penguins weigh greater than\nqnorm(0.3, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass, lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3934.634\n```\n:::\n:::\n\n\nWe find that 30% of the penguins weigh greater than 3935g.\n\n### Standard normal distribution\n\nA normal distribution with mean ($\\bar{x}$) = 0 and variance ($\\sigma^{2}$) = 1 is called *standard normal distribution* or a *z-distribution*. With the help of standard deviation, we can split the area under the curve of standard normal distribution into three parts. This partitioning of the area is known as the *68–95–99.7 rule*. \n\nWhat it means is that;\n\n-   68% of the data will lie within $±\\sigma$ from $\\bar{x}$ or they lie within 1 standard deviation from the mean\n-   95% of the data will lie within $±2\\sigma$ from $\\bar{x}$ or they lie within 2 standard deviations from the mean\n-   99.7% of the data will lie within $±3\\sigma$ from $\\bar{x}$ or they lie within 3 standard deviations from the mean\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Plotting a standard normal distribution\nggplot(data.frame(x = c(-4,4)), aes(x)) +\n  # Plot the pdf\n  stat_function(\n    fun = dnorm,\n    n = 101, args = list(mean = 0, sd = 1),\n    geom = \"area\", color = \"grey75\", fill = \"grey75\", alpha = 0.4) +\n  # Shade below -2\n  stat_function(\n    fun = function(x) ifelse(x <= -2, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade below -1\n  stat_function(\n    fun = function(x) ifelse(x <= -1, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade above 2\n  stat_function(\n    fun = function(x) ifelse(x >= 2, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade above 1\n  stat_function(\n    fun = function(x) ifelse(x >= 1, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  ggtitle(\"The standard normal distribution\") +\n  xlab(expression(italic(z))) +\n  ylab(expression(paste(\"Density, \", italic(f(z))))) +\n  geom_text(x = 0.5, y = 0.25, size = 3.5, fontface = \"bold\",\n            label = \"34.1%\") +\n  geom_text(x = -0.5, y = 0.25, size = 3.5, fontface = \"bold\",\n            label = \"34.1%\") +\n  geom_text(x = 1.5, y = 0.05, size = 3.5, fontface = \"bold\",\n            label = \"13.6%\") +\n  geom_text(x = -1.5, y = 0.05, size = 3.5, fontface = \"bold\",\n            label = \"13.6%\") +\n  geom_text(x = 2.3, y = 0.01, size = 3.5, fontface = \"bold\",\n            label = \"2.1%\") +\n  geom_text(x = -2.3, y = 0.01, size = 3.5, fontface = \"bold\",\n            label = \"2.1%\") +\n  geom_vline(xintercept=0, col = \"red\") +\n  annotate(\"text\", x=-0.25, y=0.15, label=\"Mean(x̅) = 0\", angle=90) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n1.    The lightest grey area contains 68% (~ 34.1 + 34.1) of the data and it corresponds to $\\bar{x}±\\sigma$.\n2.    The second lightest grey area contains 95% (~ 34.1 + 34.1 + 13.6 + 13.6) and it corresponds to $\\bar{x}±2\\sigma$.\n3.    The darkest grey area contains 99.7% (~ 34.1 + 34.1 + 13.6 + 13.6 + 2.1 + 2.1) and it corresponds to $\\bar{x}±3\\sigma$.\n4.    For a standard normal distribution; the area under the curve is 1, the mean is 0 and the variance is 1.\n\n### Poisson distribution\n\nPoisson distribution describe poisson processes. A Poisson process is when events happen at a certain rate but are completely random. For example; the number of people visiting the hospital, the number of candies sold in a shop, and the number of meteors falling on earth in a year. Thus the Poisson distribution describes the probability of some number of events happening over a fixed time. Thus the Poisson distribution is only applicable for datasets containing 0 and positive integers. It won't work with negative or decimal values.\n\nThe Poisson distribution has the same value for its mean and variance and is denoted by $\\lambda$.\n\nAs seen in earlier distributions, the function to sample from a Poisson distribution is `rpois()`. Given below is an example of a Poisson distribution for $\\lambda = 3$, where the number of events ranges from 0 to 10.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a poisson distribution\ndata.frame(events = 0:10, \n           pdf = dpois(x = 0:10, lambda = 3)) %>%\nggplot(aes(x = factor(events), y = pdf)) +\n  geom_col() +\n  geom_text(\n    aes(label = round(pdf,2), y = pdf + 0.01),\n    position = position_dodge(0.9),\n    size = 3,\n    vjust = 0\n  ) +\n  labs(title = \"Probability of X = x Events\",\n       subtitle = \"Poisson distribution (λ = 3)\",\n       x = \"Events (x)\",\n       y = \"Probability\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nLet us take a scenario where the number of people visiting the hospital per week is 25. This is a Poisson process and we can model this scenario using a Poisson distribution. Here $\\lambda = 25$. As seen earlier, we can use different base functions in R to answer different questions concerning the Poisson distribution.\n\n*   What is the probability that 20 people will visit the hospital in a week given that 25 people on average visit the hospital in a week?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculating the probability that 20 people will visit the hospital in a week\ndpois(20, lambda = 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05191747\n```\n:::\n:::\n\n\nWe get a 5.2% chance that 20 people will visit the hospital in a week.\n\n*   What is the probability that 15 or fewer people will visit the hospital given that 25 people on average visit the hospital in a week? To get the answer to this question, we use the `ppois()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculating the probability that 15 or fewer people will visit the hospital in a week\nppois(15, lambda = 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02229302\n```\n:::\n:::\n\n\nThere is a 2.2% chance that 15 or fewer people will visit the hospital.\n\n*   What is the probability that more than 5 people will visit the hospital given that 25 people on average visit the hospital in a week? To get the answer to this question, we use the `ppois()` function but with the `lower.tail = F` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculating the probability that more than 5 people will visit the hospital in a week\nppois(5, lambda = 25, lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9999986\n```\n:::\n:::\n\n\nWe get a 100% chance that more than 5 people will visit the hospital.\n\n### Exponential distribution\n\nThe exponential distribution describes the probability distribution of time between events in a Poisson process. Exponential distribution can be used to predict the probability of waiting 10 minutes between two visitors in a hospital, the probability of elapsing 5 minutes between the sale of two candies, probability of having 6 months between two meteor showers on earth.\n\nSome real-life examples which exhibit exponential distribution are bacterial growth rate, oil production, call duration, and my parent's patience (quickly decaying curve).\n\nThe exponential distribution is also described by the same parameter $\\lambda$ as that of the Poisson distribution but it's measured as a 'rate'. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a exponential distribution\nx <- seq(0, 20, length.out=1000)\ndat <- data.frame(time =x, px=dexp(x, rate=0.6))\n\nggplot(dat, aes(x=x, y=px)) +\n  geom_line() +\n  labs(title = \"Probability of X = x Time\",\n       subtitle = \"Exponential distribution (λ = 0.6 or rate = 0.6)\",\n       x = \"Time\",\n       y = \"Probability\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nYou can use the function `rexp()` to sample across an exponential distribution, similarly use `dexp()` and `pexp()` to find the probability of events like shown for other distributions.\n\n### Student's t distribution\n\nThe student's t distribution or simply called the t distribution is a probability distribution similar to a normal distribution but is estimated with a low sample size collected from a population whose standard deviation is unknown. The parameter in estimating the t-distribution is called the degrees of freedom.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plotting the t distribution and comparing to the standard normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = function(x) dt(x, df = 2),\n                aes(color = \"t\")) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1),\n                aes(color = \"normal\")) +\n  labs(title = \"Student's t-distribution vs Standard normal distribution\",\n       subtitle = \"t-distribution (degrees of freedom = 2)\",\n       x = \"t or z\",\n       y = \"Probability\") +\n  scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\")) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nIn the graph shown above, the t-distribution with degrees of freedom = 2 is plotted alongside the standard normal distribution. You can see that both the tail ends of the t-distribution are thicker as compared to the normal distribution and hence for the t-distribution the values are more away from the mean. But as the degrees of freedom increase, the t-distribution tends to become similar to that of a normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plotting the t distribution and comparing to the standard normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = function(x) dt(x, df = 2),\n                aes(color = \"t_2\")) +\n  stat_function(fun = function(x) dt(x, df = 25),\n                aes(color = \"t_25\")) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1),\n                aes(color = \"normal\")) +\n  labs(title = \"Student's t-distributions vs Standard normal distribution\",\n       subtitle = \"t-distribution (df = 2 and df = 25)\",\n       x = \"t or z\",\n       y = \"Probability\") +\n  scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\", \"darkgreen\")) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nIn the above graph, you can see that the t-distribution with degrees of freedom = 25 (green) is very similar to the standard normal distribution.\n\n### Log-normal distribution\n\nThe log-normal distribution is a probability distribution of variable 'x' whose log-transformed values follow a normal distribution. Like the normal distribution, log-normal distribution has a mean value and a standard deviation which are estimated from log-transformed values. Some real-life examples which follow the log-normal distribution are; the length of chess games, blood pressure in adults, and the number of hospitalizations in the 2003 SARS outbreak.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a log normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = dlnorm, args = list(meanlog = 2.2, sdlog = 0.44), \n                colour = \"red\") +\n  labs(title = \"Log normal distribution\",\n       subtitle = \"Log normal distribution [mean_log = 2.2 and sd_log = 0.44]\",\n       x = \"x\",\n       y = \"Probability\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n## The central limit theorem\n\nImagine we are rolling a die 5 times and we are calculating the mean of the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times\nsample_of_5 <- sample(die, 5, replace = T)\nsample_of_5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 1 5 6 2\n```\n:::\n\n```{.r .cell-code}\n# Calculating the mean fo the results\nmean(sample_of_5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n\nNow imagine we are repeating the experiment of rolling a die 5 times for 10 trials and then we are calculating the mean for each trial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repreating it 10 times\nsample_of_5 <- replicate(10, sample(die, 5, replace = T) %>% mean())\n\n# Mean values for the 10 trials\nsample_of_5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 3.2 4.0 2.4 4.8 4.8 3.2 3.0 4.8 3.2 3.6\n```\n:::\n:::\n\n\nLet us go further and repeat this experiment for 100 and 1000 trials and visualize the means.\n\n::: {.panel-tabset}\n# For 10 trials\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(10, sample(die, 5, replace = T) %>% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n# For 100 trials\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(100, sample(die, 5, replace = T) %>% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n# For 1000 trials\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(1000, sample(die, 5, replace = T) %>% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n:::\n\nYou can see that as the number of trials increases, the distribution of means reaches a normal distribution. This is a result of the central limit theorem.\n\n> The central limit theorem states that a sampling distribution of a statistic will approach a normal distribution as the number of trials increases, provided that the samples are randomly sampled and are independent.\n\nIn the above case, we can see that the mean of the samples approaches the central value or the 'expected value' of 3.5. \n\nThe central limit theorem also applies to other statistics such as the standard deviation.\n\n::: {.panel-tabset}\n# For 10 trials\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(10, sample(die, 5, replace = T) %>% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n# For 100 trials\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(100, sample(die, 5, replace = T) %>% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n# For 1000 trials\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie <- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 <- replicate(1000, sample(die, 5, replace = T) %>% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n:::\n\nThus as a result of the central limit theorem, we can take multiple samples from a large population and estimate different statistics which would be an accurate estimate of the population statistic. Thus we can circumvent the difficulty of sampling the whole population and still be able to measure population statistics and make useful inferences.\n\n## Data transformation\n\nSometimes it is easier to visualise data after transforming it rather than trying to see it in its raw format, especially if we are working with skewed data. Given below are different datasets which have different skewness to them. For each data, the quantile-quantile plot is also plotted. The quantile-quantile plot or simply called the Q-Q plot plots the normal quantiles of the data distribution on the x-axis and the quantiles of the dataset on the y-axis. If our data is normally distributed, then the normal quantile and the data quantile would be the same and will be in a straight line. Deviation from this linear nature can be a result of the skewness of the dataset and can be visualized easily using a Q-Q plot.\n\n### Normal distribution\n\nWe have seen what a normal distribution is, now let us look at its Q-Q plot.\n\n::: {.panel-tabset}\n# Normal distriubtion\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Plotting a normal distribution\ndata.frame(events = 0:100000, \n           pdf = rnorm(n = 0:100000, 1)) %>%\n  ggplot(aes(pdf)) +\n  geom_density() +\n  labs(title = \"Normal distribution\",\n       x = \"x\",\n       y = \"Probability\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n# Q-Q plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Plotting a q-q plot\ndata <- data.frame(events = 0:100000, \n           pdf = rnorm(n = 0:100000, 1))\nggplot(data, aes(sample = pdf)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot\",\n       subtitle = \"For a normal distribution, Q-Q plot is a straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n:::\n\nIf our data is normally distributed then it is easier to do statistical analyses with it. Finding a correlation between two normally distributed variables also becomes straightforward. In R, we can use the `cor()` function to find the correlation estimate between two variables. The correlation coefficient (r) lies between -1 and 1. The magnitude of the 'r' denotes the strength of the relationship and the sign denotes the type of relationship.\n\n1.    Correlation coefficient (r) = 0: No relationship between the two variables\n2.    Correlation coefficient (r) = -1: Strong negative relationship between the two variables\n2.    Correlation coefficient (r) = 1: Strong positive relationship between the two variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data.frame(x = 0:100, \n           y = 100:200)\n\n# Plotting x and y\nggplot(data, aes(x,y)) + geom_point() +\n  labs(title = \"Relationship between x and y\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Finding correlation between x and y\ncor(data$x, data$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nWe have a linear relationship between x and y. And we got a correlation coefficient value of 1 (in real life scenario this is next to impossible to obtain). But there are instances where we would not get a straightforward linear relationship between two variables and we would have to transform our data to make it easier to find the relationship. Before getting into data transformation, let us see different types of skewed distribution and plot their Q-Q plots.\n\n### Negative or left-skewed distribution\n\nGiven below are graphs showing negative or left-skewed distribution, and its Q-Q plot.\n\n::: {.panel-tabset}\n# Negative or left-skewed distribution\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Plotting a negative or left skewed distribution\ndata.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 5, 2)) %>%\n  ggplot(aes(y)) +\n  geom_density() +\n  labs(title = \"Negative or left skewed distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n# Q-Q plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Plotting a q-q plot\ndata <- data.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 5, 2))\nggplot(data, aes(sample = y)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot for negative or left skewed distribution\",\n       subtitle = \"The tail ends of Q-Q plot is bend towards the right side of the straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n\n:::\n\nYou can see that for negative or left-skewed distribution, the tail ends of the Q-Q plot are bent towards the right side of the straight line.\n\n### Positive or right-skewed distribution\n\n::: {.panel-tabset}\n# Positive or right-skewed distribution\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Plotting a positive or right skewed distribution\ndata.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 2, 5)) %>%\n  ggplot(aes(y)) +\n  geom_density() +\n  labs(title = \"Positive or right skewed distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n# Q-Q plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Plotting a q-q plot\ndata <- data.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 2, 5))\nggplot(data, aes(sample = y)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot for positive or right skewed distribution\",\n       subtitle = \"The tail ends of Q-Q plot is bend towards the left side of the straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_stat_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\n:::\n\nYou can see that for positive or right-skewed distribution, the tail ends of the Q-Q plot bend towards the left side of the straight line.\n\n### Different types data transformations\n\nDepending on the nature of the dataset, different transformations can be applied to better visualize the relationship between two variables. Given below are different data transformation functions which are used depending on the skewness of the data [@s_data_2010].\n\n1.    For right-skewed data, the standard deviation is proportional to the mean. In this case, logarithmic transformation ($log(x)$) works best. The square root transformation ($\\sqrt{x}$) can also be used. \n\n2.    If the variance is proportional to the mean (in the case of Poisson distributions), square root transformation ($\\sqrt{x}$) is preferred. This happens more in the case of variables which are measured as counts e.g., number of malignant cells in a microscopic field, number of deaths from swine flu, etc.\n\n3.    If the standard deviation is proportional to the mean squared, a reciprocal transformation ($\\frac{1}{x}$) can be performed. Reciprocal transformation is carried out for highly variable quantities such as serum creatinine.\n\n4.    Other transformations include the square transformation ($x^2$) and exponential transformation ($e^x$) which can be used for left skewed data.\n\n| Transformation | When is it used? | When it cannot be used? |\n|---|---|---|\n| Logarithmic transformation<br>($log(x)$) | Scaling large values to small values and making them fit a normal distribution | Cannot be used when there are 0 and negative values.<br>Can be circumvented by subtracting the whole dataset<br>with the min value in the database |\n| Square root transformation<br>($\\sqrt{x}$) | To inflate small values and to stabilize large values | Not applicable to negative values |\n| Reciprocal transformation<br>($\\frac{1}{x}$) | Highly varying data | Not applicable when there are zero values |\n\n: Different types of data transformations and their usage criteria\n\n## Conlusion\n\nWe have completed the basics of statistics and also learned how to implement them in R. In summary we learned about;\n\n1.    Descriptive and Inferential statistics.\n\n2.    Measures of centre: mean, median and mode\n\n3.    Measures of spread: variance, standard deviation, mean absolute deviation and interquartile range\n\n4.    Distributions: binomial, normal, standard normal, Poisson, exponential, student's t and log-normal\n\n5.    Central limit theorem\n\n6.    Data transformation: Skewness, Q-Q plot and data transformation functions\n\nIn the next chapter will we see how to use R for hypothesis testing.\n\n<!-- Do not forget to put flag counter -->\n<a hidden href=\"https://info.flagcounter.com/ynrK\"><img src=\"https://s11.flagcounter.com/count2/ynrK/bg_000000/txt_FFFFFF/border_F0F0F0/columns_5/maxflags_25/viewers_0/labels_1/pageviews_1/flags_0/percent_0/\" alt=\"Flag Counter\" border=\"0\"/></a>",
    "supporting": [
      "intro_stat_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}