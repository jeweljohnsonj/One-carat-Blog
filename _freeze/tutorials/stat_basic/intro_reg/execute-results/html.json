{
  "hash": "965f13df1fb20df94c6a53050252a3c3",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Regression in R\"\ndescription: \"Learn the basics of regression in R\"\ndate: \"09/02/2022\"\nformat:\n  html:\n    css:\n      - https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css\nimage: images/intro_reg.png\ncategories: [regression]\nfilters:\n   - social-share\nshare:\n  permalink: \"https://one-carat-blog.netlify.app/tutorials/stat_model/intro_reg.html\"\n  description: \"Introduction to Regression in R\"\n  twitter: true\n  facebook: true\n  reddit: true\n  stumble: true\n  tumblr: true\n  linkedin: true\n  email: true\n---\n\n\n:::{.callout-note}\n## TL;DR\n\nIn this article you will learn;\n\n1.    What is linear regression / linear model\n\n2.    How to build a linear model and use it to predict\n\n3.    Understanding the linear model summary output\n\n4.    How to view model outputs as data frames using the `{broom}` package\n\n5.    Assessing linear model performance using; R-squared, Residual standard error and Root mean square error\n\n6.    Visualizing model fit using; Residual vs. Fitted plot, Q-Q plot and Scale-location plot\n\n7.    Analysing outliers using; Leverage (hat values) and Influence (cook's distance)\n\n8.    What is logistic regression / logistic model\n\n9.    How to build a logistic model\n\n10.   Different types of predictions using the logistic model: Probability values, Most like outcomes, Odds ratio and Log odds ratio\n\n11.   Understanding the logistic model summary output\n\n12.   Assessing logistic model performance using; Confusion matrix, Accuracy, Sensitivity and Specificity\n\n:::\n\n## Introduction\n\nIn this tutorial, we will learn the basics of linear regression and logistic regression using R. We will learn how interpret the model summaries of both of them, how to predict values using models, how to assess and visualize model performance and so on.\n\n## Linear regression\n\nRegression models are a part of statistical models which are used to find relationships between variables in a dataset. Linear regression or linear models are the simplest of the models which describe a linear relationship between two variables. Throughout this tutorial (and also in other tutorials) I might use regression and model interchangeably, please don't get confused and just have it in your mind that they generally mean the same.\n\nWe will use the `HorsePrices` dataset from the `{Stat2Data}` package in R. The dataset contains the price and related characteristics of horses listed for sale on the internet. Using the dataset, we will see how the price of a horse (`Price`) changes according to its age (`Age`) using a linear model. We will plot this association using the `geom_smooth()` function in the `{ggplot2}` package in R. But first let us visualize the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting the data\nHorsePrices %>% ggplot(aes(Age, Price)) + geom_point() +\n  labs(title = \"Does the price of the horse decrease as they age?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNow let us fit a model to the graph using the `geom_smooth()` function. To fit a linear model, we will use the `method = \"lm\"` argument. By default, the linear trend line comes with confidence intervals, we can disable this by specifying `se = F`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting a linear model\nHorsePrices %>% ggplot(aes(Age, Price)) + geom_point() +\n  geom_smooth(method = \"lm\", se  = F) +\n  labs(title = \"Does the price of the horse decrease as they age?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nFrom the linear trend line, we can see that there is some evidence suggesting that the price of the horse decreases as they age.\n\nIn the above linear regression plot, the relationship between the price of the horse and the age of the horse is described as a straight line which is the feature of linear regression. Thus, this trend line can be described by the equation of the straight line;\n\n$$y=mx+c$$\n\nHere $m$ is the slope and $c$ is the y-intercept.\n\nNow let's try to get the slope and y-intercept values for the model we just plotted. We will use the `lm()` function to model the regression of price and age of the horses. The syntax for the `lm()` function is as follows;\n\n*   `lm(dependent variable ~ independent variable, dataset_name)`\n\nHere, we want to see how the price of the horse, which is the dependent variable, depends on the age of the horse, which is the independent variable. As mentioned before, linear regression is a type of statistical model. So in other words, we can also say that we are building a linear model to predict the price of the horse using the age of the horse. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Printing the model results\nprint(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Age, data = HorsePrices)\n\nCoefficients:\n(Intercept)          Age  \n    29796.8       -415.9  \n```\n:::\n:::\n\n\nWe get two values, an 'intercept' value and a value under the column 'Age'. \n\n1.    Here the slope of the line is denoted under the 'Age' column and is -415.9\n2.    The y-intercept is given under the column intercept and is 29796.8.\n\nNow, what do these values mean according to our model? Recall that the slope is the rate of change of y for x. Therefore, the value of -415.9 means that, as the horse ages one year, the price of the horse decreases by 415.9 dollars. Similarly, the y-intercept value is got when the x value is zero. So the intercept value of 29796.8 means that if the horse has an age of 0 years, or a newborn horse has an average price of 29796.8 dollars. The y-intercept and the slopes got from the model output are called the coefficients.\n\nWe can also predict the values of the response variable using a categorical explanatory variable. In the earlier case, the age of the horse was a numerical value, and we calculated the slope of it. Then what values do we get for a categorical explanatory variable?\n\nTo test this we will use the `Hawks` dataset from the `{Stat2Data}` package in R. The dataset contains measurements of three hawk species. We will see if body weight (`Weight`) is different between the three hawk species (`Species`). The three different hawk species in this dataset are; CH=Cooperâ€™s, RT=Red-tailed, and SS=Sharp-Shinned.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Building a linear model \nmodel_lm <- lm(Weight ~ Species, data = Hawks)\n\n# Printing the model results\nprint(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Weight ~ Species, data = Hawks)\n\nCoefficients:\n(Intercept)    SpeciesRT    SpeciesSS  \n      420.5        673.9       -272.5  \n```\n:::\n:::\n\n\nWe got the coefficient values where we have an intercept value and the values for two of the hawk species (RT and SS), so what happened to the third species (CH)? And what do the values for each species mean?\n\nThe intercept value shows the mean body weight for the third species (CH) and the rest of the values by their magnitude and sign tell us how greater or less their average body weight is. For example, the average body weight of RT is 420.5 + 673.9 = 1094.4g. We can change the model formula syntax by adding `-1`, to calculate individual averages for each level in our categorical explanatory variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Building a linear model \nmodel_lm <- lm(Weight ~ Species - 1, data = Hawks)\n\n# Printing the model results\nprint(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Weight ~ Species - 1, data = Hawks)\n\nCoefficients:\nSpeciesCH  SpeciesRT  SpeciesSS  \n    420.5     1094.4      148.0  \n```\n:::\n:::\n\n\nThus we essentially get the back the average body masses of all the hawk species in our data. We can cross check this by checking the means individually as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Calculating the mean\nHawks %>% select(Species, Weight) %>%\n  group_by(Species) %>%\n  summarise(mean_weight = mean(Weight, na.rm = T))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Species\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"mean_weight\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"CH\",\"2\":\"420.4857\"},{\"1\":\"RT\",\"2\":\"1094.4301\"},{\"1\":\"SS\",\"2\":\"147.9688\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Making predicition using a linear model\n\nOne cool use of models is that we can use them to predict values for which data is not available. Let us go back to the first model we created where we have the price of horses predicted by their age. Let us look at the plot again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting a linear model\nHorsePrices %>% ggplot(aes(Age, Price)) + geom_point() +\n  geom_smooth(method = \"lm\", se  = F) +\n  labs(title = \"Does the price of the horse decrease as they age?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe have age values from 1 year to 20 years. What if we want to know what the price of the course would be for a 30-year-old horse given the trend seen without collected data? We can know this price with the use of the model we created. Using the `predict()` function, we can input our model and the value for which we want the prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\nage_30 <- data.frame(Age=c(30))\n\n# Predicting price for a 30 years old horse\npredict(model_lm, newdata = age_30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n17320.89 \n```\n:::\n:::\n\n\nOur model predicts that the price of a 30-year-old horse would be 17320.89 dollars. Therefore, even though we did not have data for a 30-year-old horse, with help of our model, we were able to extrapolate to find the price for the 30-year-old horse.\n\n## Understanding the summary of a linear model\n\nWe can use the `summary()` function to get the results of the linear model. Let us look at what each section in summary means;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Printing model outputs\nsummary(model_lm)\n```\n:::\n\n\n![](images/lm_summary_1.png)\n\n1.    In section 1, we have the model formula used for modelling\n\n2.    In section 2, we have the 1st quartile, median and the 3rd quartile of the residuals. The residual is the distance or difference between the predicted value and the actual value in the dataset. One of the important requirements for a linear model to be applied to a dataset is that the residuals should be normally distributed. We can check if the residuals are normally distributed by looking at this section. For normally distributed residuals, the median should be close to zero and the 1st and 3rd quartiles should have similar absolute values.\n\n3.    In section 3, we have the y intercept value and the slope for the Age variable. We also have the standard error value and the t-statistics coupled with a corresponding p-value. The level of significance is noted by the number of asterisks.\n\n4.    In section 4, we can evaluate the model performance. There are mainly two metrics that assess the model fit; the *residual standard error* and the *R-squared value*. We will see more about these values later.\n\n## Getting the model output\n\nUsing various functions in the `{broom}` package in R, we can extract or view different model outputs.\n\n1.    We can use the `tidy()` function to view or save the coefficients of the model as a data frame which is easier to manipulate using different functions in the `{dplyr}` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Visualising coefficients and other metrics as a data frame\ntidy(model_lm)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"29796.7868\",\"3\":\"3952.4881\",\"4\":\"7.5387416\",\"5\":\"1.103623e-09\"},{\"1\":\"Age\",\"2\":\"-415.8631\",\"3\":\"468.8925\",\"4\":\"-0.8869051\",\"5\":\"3.795523e-01\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n2.    We can use the `augment()` function to return the dataset values with their corresponding fitted value and various other values. The `.fitted` column contains the fitted values and the `.resid` column contains the residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Visualising dataset and model output values\naugment(model_lm)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Price\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".fitted\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".resid\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".hat\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".sigma\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".cooksd\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".std.resid\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"38000\",\"2\":\"3.0\",\"3\":\"28549.20\",\"4\":\"9450.8026\",\"5\":\"0.03647777\",\"6\":\"15106.68\",\"7\":\"7.785389e-03\",\"8\":\"0.64131577\"},{\"1\":\"40000\",\"2\":\"5.0\",\"3\":\"27717.47\",\"4\":\"12282.5288\",\"5\":\"0.02434290\",\"6\":\"15063.01\",\"7\":\"8.558381e-03\",\"8\":\"0.82827258\"},{\"1\":\"10000\",\"2\":\"1.0\",\"3\":\"29380.92\",\"4\":\"-19380.9237\",\"5\":\"0.05641641\",\"6\":\"14890.07\",\"7\":\"5.279975e-02\",\"8\":\"-1.32897980\"},{\"1\":\"12000\",\"2\":\"8.0\",\"3\":\"26469.88\",\"4\":\"-14469.8818\",\"5\":\"0.02077267\",\"6\":\"15021.14\",\"7\":\"1.006221e-02\",\"8\":\"-0.97399633\"},{\"1\":\"25000\",\"2\":\"4.0\",\"3\":\"28133.33\",\"4\":\"-3133.3343\",\"5\":\"0.02943486\",\"6\":\"15164.72\",\"7\":\"6.805566e-04\",\"8\":\"-0.21185001\"},{\"1\":\"35000\",\"2\":\"8.0\",\"3\":\"26469.88\",\"4\":\"8530.1182\",\"5\":\"0.02077267\",\"6\":\"15119.62\",\"7\":\"3.496823e-03\",\"8\":\"0.57417910\"},{\"1\":\"35000\",\"2\":\"5.0\",\"3\":\"27717.47\",\"4\":\"7282.5288\",\"5\":\"0.02434290\",\"6\":\"15133.65\",\"7\":\"3.008711e-03\",\"8\":\"0.49109748\"},{\"1\":\"12000\",\"2\":\"17.0\",\"3\":\"22727.11\",\"4\":\"-10727.1137\",\"5\":\"0.11541294\",\"6\":\"15080.33\",\"7\":\"3.765107e-02\",\"8\":\"-0.75970799\"},{\"1\":\"22000\",\"2\":\"4.0\",\"3\":\"28133.33\",\"4\":\"-6133.3343\",\"5\":\"0.02943486\",\"6\":\"15144.62\",\"7\":\"2.607619e-03\",\"8\":\"-0.41468506\"},{\"1\":\"25000\",\"2\":\"6.0\",\"3\":\"27301.61\",\"4\":\"-2301.6081\",\"5\":\"0.02120188\",\"6\":\"15168.02\",\"7\":\"2.600693e-04\",\"8\":\"-0.15495975\"},{\"1\":\"40000\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"13114.2551\",\"5\":\"0.02001180\",\"6\":\"15048.26\",\"7\":\"7.950052e-03\",\"8\":\"0.88240366\"},{\"1\":\"25000\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"-1885.7449\",\"5\":\"0.02001180\",\"6\":\"15169.27\",\"7\":\"1.643799e-04\",\"8\":\"-0.12688393\"},{\"1\":\"4500\",\"2\":\"14.0\",\"3\":\"23974.70\",\"4\":\"-19474.7031\",\"5\":\"0.06630769\",\"6\":\"14884.27\",\"7\":\"6.399356e-02\",\"8\":\"-1.34246525\"},{\"1\":\"19900\",\"2\":\"6.0\",\"3\":\"27301.61\",\"4\":\"-7401.6081\",\"5\":\"0.02120188\",\"6\":\"15132.52\",\"7\":\"2.689543e-03\",\"8\":\"-0.49832608\"},{\"1\":\"45000\",\"2\":\"3.0\",\"3\":\"28549.20\",\"4\":\"16450.8026\",\"5\":\"0.03647777\",\"6\":\"14973.58\",\"7\":\"2.358941e-02\",\"8\":\"1.11632415\"},{\"1\":\"45000\",\"2\":\"6.0\",\"3\":\"27301.61\",\"4\":\"17698.3919\",\"5\":\"0.02120188\",\"6\":\"14945.74\",\"7\":\"1.537779e-02\",\"8\":\"1.19157489\"},{\"1\":\"48000\",\"2\":\"6.0\",\"3\":\"27301.61\",\"4\":\"20698.3919\",\"5\":\"0.02120188\",\"6\":\"14861.74\",\"7\":\"2.103292e-02\",\"8\":\"1.39355509\"},{\"1\":\"15500\",\"2\":\"12.0\",\"3\":\"24806.43\",\"4\":\"-9306.4293\",\"5\":\"0.04332558\",\"6\":\"15108.20\",\"7\":\"9.095371e-03\",\"8\":\"-0.63377500\"},{\"1\":\"8500\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"-18385.7449\",\"5\":\"0.02001180\",\"6\":\"14927.99\",\"7\":\"1.562591e-02\",\"8\":\"-1.23710029\"},{\"1\":\"22000\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"-4885.7449\",\"5\":\"0.02001180\",\"6\":\"15154.73\",\"7\":\"1.103429e-03\",\"8\":\"-0.32874145\"},{\"1\":\"35000\",\"2\":\"5.0\",\"3\":\"27717.47\",\"4\":\"7282.5288\",\"5\":\"0.02434290\",\"6\":\"15133.65\",\"7\":\"3.008711e-03\",\"8\":\"0.49109748\"},{\"1\":\"16000\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"-10885.7449\",\"5\":\"0.02001180\",\"6\":\"15086.79\",\"7\":\"5.477710e-03\",\"8\":\"-0.73245649\"},{\"1\":\"16000\",\"2\":\"3.0\",\"3\":\"28549.20\",\"4\":\"-12549.1974\",\"5\":\"0.03647777\",\"6\":\"15056.78\",\"7\":\"1.372698e-02\",\"8\":\"-0.85156770\"},{\"1\":\"15000\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"-11885.7449\",\"5\":\"0.02001180\",\"6\":\"15070.40\",\"7\":\"6.530336e-03\",\"8\":\"-0.79974233\"},{\"1\":\"33000\",\"2\":\"4.0\",\"3\":\"28133.33\",\"4\":\"4866.6657\",\"5\":\"0.02943486\",\"6\":\"15154.70\",\"7\":\"1.641776e-03\",\"8\":\"0.32904346\"},{\"1\":\"20000\",\"2\":\"14.0\",\"3\":\"23974.70\",\"4\":\"-3974.7031\",\"5\":\"0.06630769\",\"6\":\"15159.95\",\"7\":\"2.665655e-03\",\"8\":\"-0.27399138\"},{\"1\":\"25000\",\"2\":\"6.0\",\"3\":\"27301.61\",\"4\":\"-2301.6081\",\"5\":\"0.02120188\",\"6\":\"15168.02\",\"7\":\"2.600693e-04\",\"8\":\"-0.15495975\"},{\"1\":\"30000\",\"2\":\"8.0\",\"3\":\"26469.88\",\"4\":\"3530.1182\",\"5\":\"0.02077267\",\"6\":\"15162.89\",\"7\":\"5.988830e-04\",\"8\":\"0.23761923\"},{\"1\":\"50000\",\"2\":\"6.0\",\"3\":\"27301.61\",\"4\":\"22698.3919\",\"5\":\"0.02120188\",\"6\":\"14798.13\",\"7\":\"2.529394e-02\",\"8\":\"1.52820855\"},{\"1\":\"1100\",\"2\":\"19.0\",\"3\":\"21895.39\",\"4\":\"-20795.3875\",\"5\":\"0.15790449\",\"6\":\"14807.35\",\"7\":\"2.136210e-01\",\"8\":\"-1.50945594\"},{\"1\":\"15000\",\"2\":\"0.5\",\"3\":\"29588.86\",\"4\":\"-14588.8552\",\"5\":\"0.06262041\",\"6\":\"15011.77\",\"7\":\"3.364849e-02\",\"8\":\"-1.00368531\"},{\"1\":\"45000\",\"2\":\"14.0\",\"3\":\"23974.70\",\"4\":\"21025.2969\",\"5\":\"0.06630769\",\"6\":\"14836.12\",\"7\":\"7.458970e-02\",\"8\":\"1.44935357\"},{\"1\":\"2000\",\"2\":\"20.0\",\"3\":\"21479.52\",\"4\":\"-19479.5244\",\"5\":\"0.18207668\",\"6\":\"14842.96\",\"7\":\"2.290995e-01\",\"8\":\"-1.43468366\"},{\"1\":\"20000\",\"2\":\"3.0\",\"3\":\"28549.20\",\"4\":\"-8549.1974\",\"5\":\"0.03647777\",\"6\":\"15118.53\",\"7\":\"6.370795e-03\",\"8\":\"-0.58013434\"},{\"1\":\"45000\",\"2\":\"5.0\",\"3\":\"27717.47\",\"4\":\"17282.5288\",\"5\":\"0.02434290\",\"6\":\"14955.62\",\"7\":\"1.694457e-02\",\"8\":\"1.16544768\"},{\"1\":\"20000\",\"2\":\"12.0\",\"3\":\"24806.43\",\"4\":\"-4806.4293\",\"5\":\"0.04332558\",\"6\":\"15154.88\",\"7\":\"2.426048e-03\",\"8\":\"-0.32732153\"},{\"1\":\"50000\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"23114.2551\",\"5\":\"0.02001180\",\"6\":\"14784.60\",\"7\":\"2.469691e-02\",\"8\":\"1.55526206\"},{\"1\":\"50000\",\"2\":\"8.0\",\"3\":\"26469.88\",\"4\":\"23530.1182\",\"5\":\"0.02077267\",\"6\":\"14770.04\",\"7\":\"2.660797e-02\",\"8\":\"1.58385874\"},{\"1\":\"39000\",\"2\":\"11.0\",\"3\":\"25222.29\",\"4\":\"13777.7075\",\"5\":\"0.03476094\",\"6\":\"15033.29\",\"7\":\"1.571136e-02\",\"8\":\"0.93410052\"},{\"1\":\"20000\",\"2\":\"11.0\",\"3\":\"25222.29\",\"4\":\"-5222.2925\",\"5\":\"0.03476094\",\"6\":\"15151.99\",\"7\":\"2.257264e-03\",\"8\":\"-0.35406080\"},{\"1\":\"12000\",\"2\":\"6.0\",\"3\":\"27301.61\",\"4\":\"-15301.6081\",\"5\":\"0.02120188\",\"6\":\"15003.15\",\"7\":\"1.149477e-02\",\"8\":\"-1.03020727\"},{\"1\":\"15000\",\"2\":\"2.0\",\"3\":\"28965.06\",\"4\":\"-13965.0606\",\"5\":\"0.04547162\",\"6\":\"15027.87\",\"7\":\"2.159169e-02\",\"8\":\"-0.95209985\"},{\"1\":\"27500\",\"2\":\"5.0\",\"3\":\"27717.47\",\"4\":\"-217.4712\",\"5\":\"0.02434290\",\"6\":\"15171.78\",\"7\":\"2.682993e-06\",\"8\":\"-0.01466517\"},{\"1\":\"12000\",\"2\":\"2.0\",\"3\":\"28965.06\",\"4\":\"-16965.0606\",\"5\":\"0.04547162\",\"6\":\"14958.90\",\"7\":\"3.186484e-02\",\"8\":\"-1.15663170\"},{\"1\":\"6000\",\"2\":\"0.5\",\"3\":\"29588.86\",\"4\":\"-23588.8552\",\"5\":\"0.06262041\",\"6\":\"14749.72\",\"7\":\"8.797046e-02\",\"8\":\"-1.62286808\"},{\"1\":\"15000\",\"2\":\"0.5\",\"3\":\"29588.86\",\"4\":\"-14588.8552\",\"5\":\"0.06262041\",\"6\":\"15011.77\",\"7\":\"3.364849e-02\",\"8\":\"-1.00368531\"},{\"1\":\"60000\",\"2\":\"13.0\",\"3\":\"24390.57\",\"4\":\"35609.4338\",\"5\":\"0.05384116\",\"6\":\"14201.03\",\"7\":\"1.691822e-01\",\"8\":\"2.43846846\"},{\"1\":\"50000\",\"2\":\"4.0\",\"3\":\"28133.33\",\"4\":\"21866.6657\",\"5\":\"0.02943486\",\"6\":\"14822.35\",\"7\":\"3.314484e-02\",\"8\":\"1.47844210\"},{\"1\":\"30000\",\"2\":\"9.0\",\"3\":\"26054.02\",\"4\":\"3945.9813\",\"5\":\"0.02348448\",\"6\":\"15160.63\",\"7\":\"8.506893e-04\",\"8\":\"0.26598035\"},{\"1\":\"40000\",\"2\":\"7.0\",\"3\":\"26885.74\",\"4\":\"13114.2551\",\"5\":\"0.02001180\",\"6\":\"15048.26\",\"7\":\"7.950052e-03\",\"8\":\"0.88240366\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n3.    The `glance()` function return section 4 part of the model or the model performance metrics as a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Visualising model performance metrics\nglance(model_lm)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"r.squared\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"adj.r.squared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"logLik\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"AIC\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BIC\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"deviance\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df.residual\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"nobs\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.01612329\",\"2\":\"-0.004374138\",\"3\":\"15012.95\",\"4\":\"0.7866007\",\"5\":\"0.3795523\",\"6\":\"1\",\"7\":\"-550.7598\",\"8\":\"1107.52\",\"9\":\"1113.256\",\"10\":\"10818649234\",\"11\":\"48\",\"12\":\"50\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Assessing linear model fit\n\nOnce we fit a model, we can predict values outside the dataset. A good model will be able to predict values more accurately, so how do we know how good a model is? To assess model fit, we mainly use three metrics and they are described below.\n\n### R-squared value\n\n![](images/lm_summary_1.png)\n\nIn section 4 of the model summary, we can see an R-squared value of 0.01612. The R-squared value explains how much of the variance seen in the response variable is explained by the explanatory variables. The value of R-squared is between 0 and 1. The value 0 corresponds to the worst possible fit and 1 corresponds to the best fit. Here we have a value close to 0, which means our model is poorly fit. The R-squared value is typically just the squared value of the correlation between the response and the explanatory variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Extracting the R-squared value\nmodel_lm %>% glance() %>%\n  pull(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01612329\n```\n:::\n\n```{.r .cell-code}\n# R-squared is nothing but squared of correlation value\nHorsePrices %>% summarise(R_squared = cor(Price, Age)^2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"R_squared\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.01612329\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n### Residual standard error (RSE)\n\nResidual is the difference between the predicted and actual value in the model. The residual standard error estimated on average how much difference is there between the predicted and actual value in the model. A lower value suggests that the predicted value closely aligns with the values in the dataset suggesting a good fit.\n\nIn the previous model the residual standard error is;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Extracting the RSE value\nmodel_lm %>% glance() %>%\n  pull(sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15012.95\n```\n:::\n:::\n\n\nWe got a value of 15012, this means that there is, on average a difference of 15012 dollars between the predicted and actual values in the dataset.\n\nThe residual standard error is calculated by taking the square root of the sum of the squared residuals divided by the number of degrees of freedom. The degrees of freedom are calculated as the difference between the total observations and the number of variables used in the model (we used Price and Age variables, so we used 2 variables in total).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Calculating the RSE value\nHorsePrices %>%\n  mutate(residual_sq = residuals(model_lm)^2) %>%\n  summarise(sum_residual_sq = sum(residual_sq),\n            degrees_of_freedom = n() - 2,\n            rse = sqrt(sum_residual_sq/degrees_of_freedom))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"sum_residual_sq\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"degrees_of_freedom\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"rse\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"10818649234\",\"2\":\"48\",\"3\":\"15012.95\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n### Root mean square error (RMSE)\n\nThe root mean square error is another metric which is similar to that of the residual standard error value and functions the same way in quantifying how inaccurate model predictions are. It is typically not preferred but it is another metric that is sometimes used. The only difference in the calculation between RSE and RMSE is that instead of divining by the degrees of freedom, RMSE is calculated by dividing the total number of observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Calculating the RMSE value\nHorsePrices %>%\n  mutate(residual_sq = residuals(model_lm)^2) %>%\n  summarise(sum_residual_sq = sum(residual_sq),\n            total_observations = n(),\n            rse = sqrt(sum_residual_sq/total_observations))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"sum_residual_sq\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"total_observations\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"rse\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"10818649234\",\"2\":\"50\",\"3\":\"14709.62\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Visualizing linear model fit\n\nIn addition, to assess model fit using different metrics, we can also visualize the model fit using different methods. These methods include *Residual vs. Fitted plot*, *Q-Q plot* and *Scale-location plot*. We will be using the `autoplot()` function in the `{ggfortify}` package in R to visualize all three of them. We will use the `which = ` argument inside the `autoplot()` function to specify which plot to visualize. The argument takes in the following values;\n\n*   `1` : Residual vs. Fitted plot\n*   `2` : Q-Q plot\n*   `3` : Scale-location plot\n\nNow let us see what each of these plots corresponds to.\n\n### Residual vs. Fitted plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(ggfortify)) install.packages('ggfortify')\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting residual vs. fitted plot\nautoplot(model_lm, which = 1, ncol = 1) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nRecall that the residual is the difference between the predicted value and the data value. In the plot shown above, the x-axis has the fitted or predicted values of the model and its corresponding residual value on the y-axis. If the predicted values closely resemble the actual data values, then the blue line will be very similar to the dotted horizontal line at y = 0. In our case, however, the model is a mess! as you can see the blue line very much deviates from the y = 0 line.\n\n### Q-Q plot\n\nThe Q-Q plot tells us whether the residuals are normally distributed. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting Q-Q plot\nautoplot(model_lm, which = 2, ncol = 1) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nYou can see that the tail ends of the Q-Q plot deviate away from the straight line, suggesting that at lower and higher age values, the model does a poor fit to predict the price values.\n\n### Scale-location plot\n\nThe scale-location plot is plotted with the square root values of the standardized residual values on the y-axis and fitted or predicted values on the x-axis. The plot tells us whether, along the predicted value, the residuals get bigger or smaller.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting scale-location plot\nautoplot(model_lm, which = 3, ncol = 1) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nIn the plot shown above, starting from lower price values, the residuals decrease in size and for higher price values, the residuals increase in the size. Overall this suggests that for lower and higher age values in our dataset, the model struggles to output a good fit.\n\nWe can plot all three by inputting all the number arguments;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting all visualizations concerning the model fit \nautoplot(model_lm, which = 1:3) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n## Outliers, leverage, and influence\n\nOutliers are data values which are either considerably large or small when compared to the rest of the data values. They rarely occur and can be due to pure chance or measurement errors. But they can have a strong influence on the linear relationship between two variables and can mess up our inferences while doing linear modelling. So there are two ways to assess the presence and effect of outliers in the dataset; they are called *leverage* and *influence*.\n\n### Leverage\n\nLeverage value denotes extreme values in our dataset. Higher the leverage, the more extreme the data point is. We can find the leverage values from a model object using the `hatvalues()` function. Alternatively, using the `augment()` function from the `{broom}` package, we can extract the leverage value from the `.hat` column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Finding the leverage values using hatvalues()\nleverage <- hatvalues(model_lm)\n\n# Finding the leverage values using augment()\nmodel_lm %>% augment() %>%\n  select(Price, Age, .hat) %>%\n  arrange(desc(.hat)) %>% # Finding the extreme values\n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Price\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".hat\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2000\",\"2\":\"20\",\"3\":\"0.18207668\"},{\"1\":\"1100\",\"2\":\"19\",\"3\":\"0.15790449\"},{\"1\":\"12000\",\"2\":\"17\",\"3\":\"0.11541294\"},{\"1\":\"4500\",\"2\":\"14\",\"3\":\"0.06630769\"},{\"1\":\"20000\",\"2\":\"14\",\"3\":\"0.06630769\"},{\"1\":\"45000\",\"2\":\"14\",\"3\":\"0.06630769\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nLooks like we have three outliers in this dataset which have significantly large leverage values. \n\n### Influence\n\nThe influence of a data point measures how much of a difference will the model have if that value is excluded while building the model. It is represented by a metric called *Cook's distance* where bigger values denoted higher influence and thus the presence of an outlier. We can use the `cooks.distance()` function to calculate the influence of data points in a model object. Alternatively, we can, like before, use the `augment()` function and extract the `.cooksd` column to get the Cook's distance values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Finding the influence values using cooks.distance()\ncooks_distance <- cooks.distance(model_lm)\n\n# Finding the influence values using augment()\nmodel_lm %>% augment() %>%\n  select(Price, Age, .cooksd) %>%\n  arrange(desc(.cooksd)) %>% # Finding the extreme values\n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Price\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".cooksd\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2000\",\"2\":\"20.0\",\"3\":\"0.22909945\"},{\"1\":\"1100\",\"2\":\"19.0\",\"3\":\"0.21362104\"},{\"1\":\"60000\",\"2\":\"13.0\",\"3\":\"0.16918220\"},{\"1\":\"6000\",\"2\":\"0.5\",\"3\":\"0.08797046\"},{\"1\":\"45000\",\"2\":\"14.0\",\"3\":\"0.07458970\"},{\"1\":\"4500\",\"2\":\"14.0\",\"3\":\"0.06399356\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n### Plotting leverage and influence\n\nLike before, we can use the `autoplot()` function in the `{ggfortify}` package in R to visualize leverage and influence. The input values for `which = ` corresponding leverage and influence are;\n\n*   `4` : Cook's distance\n*   `5` : Residuals vs. Leverage\n*   `6` : Cook's distance vs. Leverage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm <- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting all visualizations concerning leverage and influence\nautoplot(model_lm, which = 4:6) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nFrom the plots, it seems like we have three outliers which can significantly affect the model.\n\n## Logistic regression\n\nLinear models are best used for the continuous response variable. But sometimes we can have a binary response variable like; survival data: dead or alive, choice data: yes or no, survey data: disagree or agree. In these cases, linear models will not work and we should rely on logistic models, which model using a binomial distribution. The logistic model is built in R using the `glm()` function with `family = ` argument set to `binomial`. This is shown below;\n\n*   Building a logistic model: `glm(response ~ predictor, data, family = \"binomial\")`\n\nLet us look at the `CreditCard` dataset in the `{AER}` package in R. The dataset contains the credit history of a sample of applicants for a type of credit card. We will predict to see the success of the acceptance of the credit card application (`card`) using yearly income (`income`). Let us look at the dataset first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(AER)) install.packages('AER')\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Viewing the data\nhead(CreditCard)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"card\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"reports\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"income\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"share\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"expenditure\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"owner\"],\"name\":[7],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"selfemp\"],\"name\":[8],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"dependents\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"months\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"majorcards\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"active\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"yes\",\"2\":\"0\",\"3\":\"37.66667\",\"4\":\"4.5200\",\"5\":\"0.033269910\",\"6\":\"124.983300\",\"7\":\"yes\",\"8\":\"no\",\"9\":\"3\",\"10\":\"54\",\"11\":\"1\",\"12\":\"12\",\"_rn_\":\"1\"},{\"1\":\"yes\",\"2\":\"0\",\"3\":\"33.25000\",\"4\":\"2.4200\",\"5\":\"0.005216942\",\"6\":\"9.854167\",\"7\":\"no\",\"8\":\"no\",\"9\":\"3\",\"10\":\"34\",\"11\":\"1\",\"12\":\"13\",\"_rn_\":\"2\"},{\"1\":\"yes\",\"2\":\"0\",\"3\":\"33.66667\",\"4\":\"4.5000\",\"5\":\"0.004155556\",\"6\":\"15.000000\",\"7\":\"yes\",\"8\":\"no\",\"9\":\"4\",\"10\":\"58\",\"11\":\"1\",\"12\":\"5\",\"_rn_\":\"3\"},{\"1\":\"yes\",\"2\":\"0\",\"3\":\"30.50000\",\"4\":\"2.5400\",\"5\":\"0.065213780\",\"6\":\"137.869200\",\"7\":\"no\",\"8\":\"no\",\"9\":\"0\",\"10\":\"25\",\"11\":\"1\",\"12\":\"7\",\"_rn_\":\"4\"},{\"1\":\"yes\",\"2\":\"0\",\"3\":\"32.16667\",\"4\":\"9.7867\",\"5\":\"0.067050590\",\"6\":\"546.503300\",\"7\":\"yes\",\"8\":\"no\",\"9\":\"2\",\"10\":\"64\",\"11\":\"1\",\"12\":\"5\",\"_rn_\":\"5\"},{\"1\":\"yes\",\"2\":\"0\",\"3\":\"23.25000\",\"4\":\"2.5000\",\"5\":\"0.044438400\",\"6\":\"91.996670\",\"7\":\"no\",\"8\":\"no\",\"9\":\"0\",\"10\":\"54\",\"11\":\"1\",\"12\":\"1\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nHere, the response variable `card` has only two values and thus is a binary response variable. Let us plot the data and see how the values are distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CreditCard\")\n\n# Plotting the data\nggplot(CreditCard, aes(income, card)) + geom_jitter(width = 0, height = 0.05) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nWe get some interesting results, even at a low yearly income, we have a high number of successful credit applicants but at the same time, we also have many applicants getting rejected from acquiring a credit card. Very few people have their credit card requests rejected when they have a high yearly income. Overall, the plot seems to suggest that yearly income is not a good predictor for looking at the success of credit card applications. Let us see if that is the case by modelling the data using a logistic model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log <- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Printing model outputs\nsummary(model_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = card ~ income, family = \"binomial\", data = CreditCard)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2501   0.5301   0.7005   0.7502   0.8733  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.73452    0.15842   4.636 3.55e-06 ***\nincome       0.15582    0.04597   3.390 0.000699 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1404.6  on 1318  degrees of freedom\nResidual deviance: 1391.6  on 1317  degrees of freedom\nAIC: 1395.6\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nEven though from the model it was not apparent that yearly income had an association with the success of a credit application, the model suggests that yearly income does significantly explain the variance seen in the success of the credit applications. We will come back to interpreting the model summary soon.\n\nAs seen earlier for linear models, we can also plot logistic models. We have to specify `method = \"glm\"` in the and then specify the family by inputting `method.args = list(family = binomial)`. Now to the plot, we have to change the y-axis variable to boolean values (0 or 1). We can either use the `recode()` function before plotting and modifying the dataset or straight away input `as.numeric()` to the y-axis variable and subtract 1 to rescale the values to boolean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CreditCard\")\n\n# Plotting the logistic model\nggplot(CreditCard, aes(income, as.numeric(card) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial)) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw() \n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nIn the y-axis, you can see values ranging from 0 to 1 and the blue trend line shifting towards 1 for high values of yearly income. This suggests that, as the yearly income increase, the probability of success in credit card application increases.\n\nNow one question that might come to your mind (I had this question when I was learning regression) is why can't we apply linear regression if it is just 0 and 1 values for the response variable. So let us plot both linear and logistic regression for the above data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CreditCard\")\n\n# Plotting the logistic model\nggplot(CreditCard, aes(income, as.numeric(card) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = \"lm\", se = F, col = \"red\") +\n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial)) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw() \n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nHA! If we got a similar trend line as that of the logistic model, then why bother with a logistic model? Well, hold your horses! If we take a closer look at the trend line, we can see that the red line overshoots the y-axis value of 1. This means we have a probability value greater than 1 which is absurd. Perhaps the best way to see why linear models don't work for binary response variables is by predicting values using them with values outside the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log <- glm(card ~ income, data = CreditCard, family = \"binomial\")\nmodel_lm <- lm((as.numeric(card) - 1) ~ income, data = CreditCard)\n\n# # Generating random income values\nexplorartory_values <- tibble(income = seq(-50,50,2))\n\n# Predicting the values for linear and logistics\n# We use the 'type = response' to calculate probabilities\npredict_values <- explorartory_values %>%\n  mutate(predict_lm = predict(model_lm, explorartory_values),\n         predict_log = predict(model_log, explorartory_values, type = \"response\"))\n\n# Plotting the predicted values as a line graph\nggplot(predict_values, aes(income)) +\n  geom_line(data = predict_values, aes(y = predict_lm, col = \"linear\")) +\n  geom_line(data = predict_values, aes(y = predict_log, col = \"logistic\")) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n   scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\")) +\n  theme_bw() \n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\nYou can see that the predicted values using the linear model are going beyond 0 and 1 whereas the logistic model predicts values between 0 and 1. Also, notice the sigmoid curve or the S-shaped curve which is a characteristic of the logistic model. Thus for a binary response variable, logistic modelling is the best.\n\n## Making predictions using a logistic model\n\nLike in the linear model, we use the `predict()` function to predict values using a logistics model. We can change the type of values we get from the `predict()` function using the `type = ` argument. By default, the predict function outputs log odds ratio (which is `type = \"link\"`), another option is `type = \"response\"` which outputs probability values. Other ways for expressing predicted values from the model include 'odds ratio' and 'most likely outcomes.\n\n### Probability values\n\nOne of the easier ways of expressing the predicted values from a logistic model. Simply it just calculates the probability of the levels in the response variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log <- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values <- tibble(income = seq(-50,50,2))\n\n# Calculating the probability values\npredicted_Values <- explorartory_values %>%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"))\n\n# Printing probability values\nhead(predicted_Values)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"income\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"probability\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-50\",\"2\":\"0.000861106\"},{\"1\":\"-48\",\"2\":\"0.001175608\"},{\"1\":\"-46\",\"2\":\"0.001604792\"},{\"1\":\"-44\",\"2\":\"0.002190316\"},{\"1\":\"-42\",\"2\":\"0.002988834\"},{\"1\":\"-40\",\"2\":\"0.004077276\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Plotting the probability values\nggplot(predicted_Values, aes(income, probability)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Probability values\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Probability of application being accepted\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n### Most likely outcome\n\nThis is the easiest way to express logistic model predicted values. It rescales the probability values into stating whether an event happens or not. Simply put, it rounds the probability values to give either 0 or 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log <- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values <- tibble(income = seq(-50,50,2))\n\n# Calculating the most likely values\npredicted_Values <- explorartory_values %>%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"),\n         most_likely = round(probability))\n\n# Printing most likely values\nhead(predicted_Values)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"income\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"probability\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"most_likely\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-50\",\"2\":\"0.000861106\",\"3\":\"0\"},{\"1\":\"-48\",\"2\":\"0.001175608\",\"3\":\"0\"},{\"1\":\"-46\",\"2\":\"0.001604792\",\"3\":\"0\"},{\"1\":\"-44\",\"2\":\"0.002190316\",\"3\":\"0\"},{\"1\":\"-42\",\"2\":\"0.002988834\",\"3\":\"0\"},{\"1\":\"-40\",\"2\":\"0.004077276\",\"3\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Plotting the most likely values\nggplot(predicted_Values, aes(income, most_likely)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Most likely values\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Number of predicted applications being accepted\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n### Odds ratio\n\nThe odds ratio is calculated by taking the probability of success divided by the probability of losing. In simple terms, it tells the odds of one event happening compared to the other.\n\n$$Odds\\,ratio = \\frac{p(success)}{1-p(success)} = \\frac{p(success)}{p(fail)}$$\n\n\n\nWe can visualize this from the predicted values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log <- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values <- tibble(income = seq(-50,50,2))\n\n# Calculating the odds ratio values\npredicted_Values <- explorartory_values %>%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"),\n         odds_ratio = probability / (1 - probability))\n\n# Printing odds ratio values\nhead(predicted_Values)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"income\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"probability\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"odds_ratio\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-50\",\"2\":\"0.000861106\",\"3\":\"0.0008618481\"},{\"1\":\"-48\",\"2\":\"0.001175608\",\"3\":\"0.0011769920\"},{\"1\":\"-46\",\"2\":\"0.001604792\",\"3\":\"0.0016073714\"},{\"1\":\"-44\",\"2\":\"0.002190316\",\"3\":\"0.0021951236\"},{\"1\":\"-42\",\"2\":\"0.002988834\",\"3\":\"0.0029977936\"},{\"1\":\"-40\",\"2\":\"0.004077276\",\"3\":\"0.0040939683\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Plotting the odds ratio values\nggplot(predicted_Values, aes(income, odds_ratio)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Odds ratio\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Odds ratio\") +\n  geom_hline(yintercept = 1, col = \"red\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nThe plot has a horizontal line plotted with y = 0. The points on this line correspond to equal odds of being rejected and accepted for a credit card application (equal odds of winning and losing). The y values for example 1000 correspond to 1000 successes for every one failure in a credit card application. The plot suggests that after the 18,000 dollars yearly income mark, the odds of success to failure rise exponentially.\n\n### Log odds ratio\n\nThe log odds ratio is the log-transformed value of the odds ratio. We will see how this value is superior to all the other values when we plot the values. Since we are transforming the y-axis values to log, we convert the scale of the y-axis to log using the `scale_y_log10()` function. By default, the `predict()` function outputs log odds ratio.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log <- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values <- tibble(income = seq(-50,50,2))\n\n# Calculating the log odds ratio values\npredicted_Values <- explorartory_values %>%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"),\n         odds_ratio = probability / (1 - probability),\n         log_odds_ratio = log(odds_ratio),\n         log_odds_ratio_1 = predict(model_log, explorartory_values))\n\n# Printing log odds ratio values\nhead(predicted_Values)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"income\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"probability\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"odds_ratio\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"log_odds_ratio\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"log_odds_ratio_1\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-50\",\"2\":\"0.000861106\",\"3\":\"0.0008618481\",\"4\":\"-7.056431\",\"5\":\"-7.056431\"},{\"1\":\"-48\",\"2\":\"0.001175608\",\"3\":\"0.0011769920\",\"4\":\"-6.744793\",\"5\":\"-6.744793\"},{\"1\":\"-46\",\"2\":\"0.001604792\",\"3\":\"0.0016073714\",\"4\":\"-6.433155\",\"5\":\"-6.433155\"},{\"1\":\"-44\",\"2\":\"0.002190316\",\"3\":\"0.0021951236\",\"4\":\"-6.121517\",\"5\":\"-6.121517\"},{\"1\":\"-42\",\"2\":\"0.002988834\",\"3\":\"0.0029977936\",\"4\":\"-5.809879\",\"5\":\"-5.809879\"},{\"1\":\"-40\",\"2\":\"0.004077276\",\"3\":\"0.0040939683\",\"4\":\"-5.498241\",\"5\":\"-5.498241\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Plotting the log odds ratio values\nggplot(predicted_Values, aes(income, odds_ratio)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Log Odds ratio\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Log Odds ratio\") +\n  geom_hline(yintercept = 1, col = \"red\") +\n  scale_y_log10() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nLog transforming the odds ratio values resulted in a linear trend line which is easier to understand the overall relationship. We can see that as yearly income increases, the probability of success in credit card application will also rise.\n\nGiven below is the summary of the predicted value types and their usage concerning the audience in question. Some values like most likely outcomes might be best in explaining the trends in the data for non-experts or common people, whereas log odds might be best for experts.\n\n| Scale | Easier to explain values? | Easier to explain changes in the response variable? | Is it precise? |\n|---|---|---|---|\n| Probability | Yes | Not easy for x values in the middle | Yes |\n| Most likely outcomes | Easiest out of all | Quickly compare high and low x values | No |\n| Odds ratio | Kind of easy to explain.<br>Odds of one event against another | Not really | Yes |\n| Log odds ratio | Log transformed values are harder to explain | Linear relation after log transformation<br>makes it easy to interpret changes | Yes |\n\n: Different scales of predicted values from a logistic model\n\n## Understanding the summary of a logistic model\n\nNow let us look at the summary of the logistic model we built earlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log <- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Printing model outputs\nsummary(model_log)\n```\n:::\n\n\n![](images/log_summary_1.png)\n\n1.    Section 1 is the model formula we used to build the model.\n\n2.    Section 2 is the quantile values of the residuals in the model, which is to check if the residuals are normally distributed.\n\n3.    Section 3 outlines the model coefficients which are in log odds values which we discussed in detail above.\n\n4.    Section 4. I don't know what these values are, so once I learn I will update this text.\n\n5.    Section 5. I don't know what these values are, so once I learn I will update this text.\n\n## Assessing logistic model fit\n\nTo assess the performance of a logistic model we have several different metrics we can use that are different from the linear model metrics. Let us look at each of them.\n\n### Confusion matrix\n\nThe confusion matrix is a 2 by 2 matrix showcasing the number of correct predictions and no of wrong predictions.\n\n| Scale | Actually false | Actually true |\n|---|---|---|\n| Predicted false | True negative | False negative |\n| Predicted true | False positive | True positive |\n\n: The framework of a confusion matrix\n\nLet us make a confusion matrix of a logistic model where credit card success (`card`) is predicted by the number of major derogatory reports against the applicant (`reports`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log <- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values <- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values <-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix <- table(predicted_values, actual_values)\nconfusion_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                actual_values\npredicted_values    0    1\n               0  104   18\n               1  192 1005\n```\n:::\n:::\n\n\nWe have 1109 (104 + 1005) correct predictions, 18 false negatives and 192 false positives. We can plot the confusion matrix with the help of `{yardstick}` and `{ggfortify}` packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(yardstick)) install.packages('yardstick')\nlibrary(AER)\nlibrary(ggplot2)\nlibrary(yardstick)\nlibrary(ggfortify)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log <- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values <- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values <-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix <- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix <- conf_mat(confusion_matrix)\n\n# Plotting the confusion matrix\nautoplot(conf_matrix) +\n  labs(title = \"Mosaic plot of confusion matrix\",\n       x = \"Actual value\",\n       y = \"Predicted value\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](intro_reg_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\nNow let us see how to interpret this plot. In the x-axis, you can see the actual values, 0 corresponds to failure in getting the credit card application accepted and 1 corresponds to the success of the same. The width of the bars corresponds to the proportion of the values of 0 and 1 in the dataset. There are more success values than failure values. Now, for the corresponding actual value, we have the bar split into respective predicted values. For actual failure values, we have a lot of predicted values falsely marking it as success. On the other hand, for the actual success value, most of the predicted values were correct. Thus the first bar under 0 tells us about the false positive cases and the second bar under 1 tells us about the false negative cases. We can see that our model has more false positive cases than false positive cases which is what we saw in the confusion matrix.\n\n### Accuracy\n\nAccuracy is simply the proportion of correct predictions compared to all predictions.\n\n$$Accuracy = \\frac{true\\,positives + true\\,negatives}{total\\,predictions}$$\n\nWe can find the accuracy from the summary of the confusion matrix we made using the `summary()` and `slice()` functions. The `event_level = \"second\"` is to denote the second column of the confusion matrix as the success value which is 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(yardstick)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log <- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values <- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values <-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix <- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix <- conf_mat(confusion_matrix)\n\n# Finding accuracy of the model\nsummary(conf_matrix, event_level = \"second\") %>% slice(1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.8407885\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWe got an accuracy value of 84% for the model we built.\n\n### Sensitivity\n\nSensitivity tells us the proportion of true positives. Higher values suggest that there are fewer false negatives.\n\n$$Sensitivity = \\frac{true\\,positives}{false\\,negatives + true\\,positives}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(yardstick)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log <- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values <- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values <-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix <- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix <- conf_mat(confusion_matrix)\n\n# Finding sensitivity of the model\nsummary(conf_matrix, event_level = \"second\") %>% slice(3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"sens\",\"2\":\"binary\",\"3\":\"0.9824047\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWe have around very 2% false negatives (100 - 98).\n\n### Specificity\n\nSensitivity tells us the proportion of true negatives. Higher values suggest that there are fewer false positives.\n\n$$Sensitivity = \\frac{true\\,negatives}{false\\,positives + true\\,negatives}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\nlibrary(yardstick)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log <- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values <- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values <-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix <- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix <- conf_mat(confusion_matrix)\n\n# Finding specificity of the model\nsummary(conf_matrix, event_level = \"second\") %>% slice(4)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"spec\",\"2\":\"binary\",\"3\":\"0.3513514\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWe have around 64% false positives (100 - 35).\n\n## Conclusion\n\nHurray! We completed the basics of regression using R. This tutorial will lay the foundations for learning statistical modelling which I have covered in the tutorial section. Let us recap what we studied in this tutorial;\n\n1.    What is linear regression / linear model\n\n2.    How to build a linear model and use it to predict\n\n3.    Understanding the linear model summary output\n\n4.    How to view model outputs as data frames using the `{broom}` package\n\n5.    Assessing linear model performance using; R-squared, Residual standard error and Root mean square error\n\n6.    Visualizing model fit using; Residual vs. Fitted plot, Q-Q plot and Scale-location plot\n\n7.    Analysing outliers using; Leverage (hat values) and Influence (cook's distance)\n\n8.    What is logistic regression / logistic model\n\n9.    How to build a logistic model\n\n10.   Different types of predictions using the logistic model: Probability values, Most like outcomes, Odds ratio and Log odds ratio\n\n11.   Understanding the logistic model summary output\n\n12.   Assessing logistic model performance using; Confusion matrix, Accuracy, Sensitivity and Specificity\n\n#### Last updated on {.unnumbered .unlisted .appendix}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2022-09-06 22:00:35 IST\"\n```\n:::\n:::\n\n\n<!-- Do not forget to put flag counter -->\n<a hidden href=\"https://info.flagcounter.com/ynrK\"><img src=\"https://s11.flagcounter.com/count2/ynrK/bg_000000/txt_FFFFFF/border_F0F0F0/columns_5/maxflags_25/viewers_0/labels_1/pageviews_1/flags_0/percent_0/\" alt=\"Flag Counter\" border=\"0\"/></a>",
    "supporting": [
      "intro_reg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}