---
title: "Chapter 4: p-value and level of significance"
subtitle: |
  Learn about p-value, level of significance, misconceptions behind p-value
author: "Jewel Johnson"
date: "05/21/2022"
format:
  html:
    toc: true
    toc-title: On this page
    html-math-method: katex
    css: styles.css
    number-sections: true
    smooth-scroll: true
    highlight-style: github
    code-link: true
    code-fold: true
    code-tools: true
citation: true
comments:
  hypothesis: true
execute: 
  warning: false
image: "images/stat.jpg"
---

```{r}
#| include: false
paged_print <- function(x, options) {
  knitr::asis_output(
    rmarkdown:::paged_table_html(x, options = attr(
      x,
      "options"
    )),
    meta = list(dependencies = rmarkdown::html_dependency_pagedtable())
  )
}

registerS3method(
  "knit_print", "data.frame", paged_print,
  envir = asNamespace("knitr")
)
```

## Definiton of p-value

"What is p-value?" This was the first question that came from my instructor while I was taking the course on statistics. The question was very simple, we just have to give the definition of p-value. Yet, I was completely dumbfounded. I have seen it in almost all the scientific papers that I have read and I know that it is a result obtained after hypothesis testing. And if the value is less 0.05 then the we accept the alternate hypothesis and reject the null hypothesis. I thought that was it, until I found out about the real deal about p-value.

A study published on 2007 surveyed medical researchers and asked them "what is interpretation on p \> 0.05 ?" and thankfully they had the choice to select the answer from four choices. The choices were;

1.  The chances are greater than 1 in 20 that a difference would be found again if the study were repeated.
2.  The probability is less than 1 in 20 that a difference this large could occur by chance alone.
3.  The probability is greater than 1 in 20 that a difference this large could occur by chance alone.
4.  The chance is 95% that the study is correct.

What do you think is the answer if you could answer now?

Anyway, around 60% of the participants of the survey selected choice 3 and 100% of them were wrong.

Yes, all of them answered wrong! which means there was no right answer among the choices to begin, yes betrayal indeed! The closest correct answer would be choice 3, but the correct definition for p-value is as follows.

::: callout-important
## Definition of p-value

**The probability to get the observed results, plus more extreme results, if the null hypothesis were true**
:::

## Misconceptions about p-value

[Goodman S. A dirty dozen: twelve p-value misconceptions. Semin Hematol. 2008 Jul;45(3):135-40. doi: 10.1053/j.seminhematol.2008.04.003. Erratum in: Semin Hematol. 2011 Oct;48(4):302. PMID: 18582619. [Link](https://www.sciencedirect.com/science/article/abs/pii/S0037196308000620?via%3Dihub)]{.aside}

Now even after knowing this definition, sometimes it might not be enough. So let us try to see some of the common misconceptions regarding p-value which is beautifully written in the paper 'A dirty dozen: twelve p-value misconceptions.' by Goodman S. I will be summarising the content of the paper.

::: callout-note
## Misconception 1

> If p = 0.05, the null hypothesis has only a 5% chance of being true.

If p = 0.05, by definition it means that there is a 5% chance that we get the observed result plus more extreme results, given that the null hypothesis is true. Therefore by definition, we never say anything about the chance of the null hypothesis being true.
:::

::: callout-note
## Misconception 2

> A non significant difference (eg, p \> 0.05) means there is no difference between groups.

p-value as we now know is a probability value. Therefore we cannot conclude anything with absolute certainty like mentioned above saying that 'there is no difference between the groups'. The takeaway message is that the result that, "there is no difference between the groups" is statistically significant within the level of significance we have chosen and falls within the range of results we should expect.
:::

::: callout-note
## Misconception 3

> Statistical significance means biological significance.

This does not need to be always the case. A drug effect experimented with within a large sample size can yield significant p-values but still be not good enough to be used for a cure. This is because the p-value does not tell you anything about the magnitude of the effect. Effect size, which is independent of sample size is a better measure to determine biological significance. We will see effect sizes in detail later.
:::

::: callout-note
## Misconception 4

> Studies with p values on opposite sides of 0.05 are conflicting.

A better way to see if two results are conflicting is by comparing the confidence intervals of the data. If they do not overlap and have little overlap, then they can be considered conflicting with each other. Two studies looking at the same result can have conflicting p-values because of the differences in sample sizes.
:::

::: callout-note
## Misconception 5

> Studies with the same p value provide the same evidence against the null hypothesis.

Two different results can be statistically significant and have same the p-value. But it is not necessary that they yield the same evidence against the null hypothesis. Consider two studies, where each of them test a novel drug against a disease. The null hypothesis for these studies would be that the cure rate of the drugs would be zero. Let us imagine that the first study had a cure rate of 5% and the second study have 15% cure rate and both studies have the same p-value of 0.05. Now here, both studies are able to reject the null hypothesis but, they don't show the same evidence against the null hypothesis, as the second study shows higher cure rate compared to first. As mentioned before the p-value does not convey any information about the magnitude of effect in question.
:::

::: callout-note
## Misconception 6

> p = 0.05 means that we have observed data that would occur only 5% of the time under the null hypothesis.

From the definition of the p-value, we know that this statement is incomplete. If $\alpha$ = 0.05 and p = 0.05, then what it means is that this is the highest probability value at which the observed results or more extreme results occur, given that the null hypothesis is true. If the above statement also mentioned about the occurrence of more extreme results than the observed data then it would be a correct statement.
:::

::: callout-note
## Misconception 7

> p = 0.05 and p ≤ 0.05 mean the same thing.

Now, this would be the most confusing one out of all the misconceptions that are described in this post. In the paper that I referenced for this article, they don't give any explanation other than saying to explain this difference one would need to use the "Bayesian evidence metric". So after reading about it this is my understanding of it.

We can calculate the "likelihood" for a hypothesis(H) for a given data(x) which is expressed as L(H\|x). The expression is the same as that of a conditional probability, but since we are dealing with hypotheses we use the word likelihood and if it was results, we associate it with probability. Also, L(H\|x) is directly proportional to P(x\|H), where P(x\|H) is the probability of obtaining data(x) given hypothesis(H) is true. Now for null(Ho) and alternate hypothesis (Ha), we have;

-   L(Ho\|x) $\propto$ P(x\|Ho)
-   L(Ha\|x) $\propto$ P(x\|Ha)

Now likelihood ratio ($LR$) is calculated as;

$LR = L(Ha|x) / L(Ho|x) = P(x|Ha) / P(x|Ho)$

Here $P(x|H)$ can be probabilities or probability densities.

Now let us try to understand the difference between p = 0.05 and p ≤ 0.05.

[![Fig1: Probability distribution curves for Ha and Ho. Credits: Goodman, S.N. (1993)](https://d3i71xaburhd42.cloudfront.net/7a46007578dcfe0da282f0a6b22a47ba8b4a629a/8-Figure1-1.png)](https://www.semanticscholar.org/paper/p-values%2C-hypothesis-tests%2C-and-likelihood%3A-for-of-Goodman/7a46007578dcfe0da282f0a6b22a47ba8b4a629a)

Consider the figure given above.

For p = 0.05, we will have;

$LR = P(x|Ha) / P(x|Ho) = P(A)/P(o)$

where $P(A)$ is the probability at B for curve Ha and $P(o)$ is the probability at A for curve Ho.

and for p ≤ 0.05, we will have;

$LR = P(x|Ha) / P(x|Ho) = Shaded\;area/Striped\;area$

where the shaded area is the probability density for p ≤ 0.05 in curve Ha and the striped area is the probability density for p ≤ 0.05 in curve Ho.

Now for p values between 0.05 to 0.001;

$$P(A)/P(o) < Shaded\;area/Striped\;area$$

which means, for p = 0.05 we have less evidence for the alternate hypothesis as opposed to when p ≤ 0.05. Therefore p = 0.05 and p ≤ 0.05 are different from each other.
:::

::: callout-note
## Misconception 8

> p values are properly written as inequalities (eg, "P ≤ 0.02" when P = 0.015)

While showing p-values in results, it is always a thumb rule to denote the actual value rather than denoting them as an inequality. For example, for $\alpha$=0.05, a result displaying p = 0.04 is far more useful, when compared to displaying it as p ≤ 0.05 as we know that the p-value is barely on the edge of the level of significance.
:::

::: callout-note
## Misconception 9

> p = 0.05 means that if you reject the null hypothesis, the probability of a type I error is only 5%.

Type I error also called false-positive is when we accept that there is some effect when in reality there is none. Now the definition of p-value is that it is the probability of obtaining the observed effect plus more extreme effects, given that the null hypothesis is true. Therefore for type, I error to happen, the null hypothesis has to be true. So if we reject the null hypothesis, then the chance of type I error is zero.
:::

::: callout-note
## Misconception 10

> With p = 0.05 threshold for significance($\alpha$), the chance of a type I error will be 5%.

If our p = $\alpha$ = 0.05, then we reject the null hypothesis. In doing so, we are allowing ourselves the chance of error that there is a 5% chance that the null hypothesis is true. Do not get confused by definitions of p-value and $\alpha$. Having p = 0.05 and setting $\alpha$ = 0.05 are two different things. Therefore $\alpha$ can also be termed as the probability that a type I error occurs. So when does this statement becomes a misconception?

For multiple pairwise comparisons, this concept is not true. For $\alpha$ = 0.05, the new $\alpha$ for $n$ tests is equal to;

$$\alpha' = 1 - (1-\alpha)^n = 1 - (1-0.05)^n$$

Therefore even if we had set $\alpha$ = 0.05 prior, after multiple comparisons the resultant $\alpha'$ will be a greater value. This is also called a multiple comparison problem and it is corrected through methods called multiple comparison corrections. This is explained in detail in Chapter 9.
:::

## References

1.  Goodman S. A dirty dozen: twelve p-value misconceptions. Semin Hematol. 2008 Jul;45(3):135-40. doi: 10.1053/j.seminhematol.2008.04.003. Erratum in: Semin Hematol. 2011 Oct;48(4):302. PMID: 18582619. [Link](https://www.sciencedirect.com/science/article/abs/pii/S0037196308000620?via%3Dihub)

2.  Gao, J. P-values -- a chronic conundrum. BMC Med Res Methodol 20, 167 (2020). https://doi.org/10.1186/s12874-020-01051-6 [Link](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01051-6#citeas)

3.  Goodman SN. p values, hypothesis tests, and likelihood: implications for epidemiology of a neglected historical debate. Am J Epidemiol. 1993 Mar 1;137(5):485-96; discussion 497-501. doi: 10.1093/oxfordjournals.aje.a116700. PMID: 8465801. [Link](https://academic.oup.com/aje/article-abstract/137/5/485/50007?redirectedFrom=fulltext&login=false)

4.  Lang JM, Rothman KJ, Cann CI. That confounded P-value. Epidemiology. 1998 Jan;9(1):7-8. doi: 10.1097/00001648-199801000-00004. PMID: 9430261. [Link](https://journals.lww.com/epidem/Citation/1998/01000/That_Confounded_P_Value.4.aspx)

5.  

#### Last updated on {.unnumbered .unlisted}

```{r}
#| echo: false
Sys.time()
```

<a hidden href="https://info.flagcounter.com/ynrK"><img src="https://s11.flagcounter.com/count2/ynrK/bg_000000/txt_FFFFFF/border_F0F0F0/columns_5/maxflags_25/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"/></a>
